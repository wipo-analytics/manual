<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>The WIPO Manual on Open Source Patent Analytics</title>
  <meta name="description" content="A practical introduction to free and open source tools for patent analytics.">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="The WIPO Manual on Open Source Patent Analytics" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="A practical introduction to free and open source tools for patent analytics." />
  <meta name="github-repo" content="wipo-analytics/manual_bookdown" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="The WIPO Manual on Open Source Patent Analytics" />
  
  <meta name="twitter:description" content="A practical introduction to free and open source tools for patent analytics." />
  



<meta name="date" content="2016-01-01">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="datasets.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; position: absolute; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; }
pre.numberSource a.sourceLine:empty
  { position: absolute; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: absolute; left: -5em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">The WIPO Manual on Open Source Patent Analytics</a></li>

<li class="divider"></li>
<li class="part"><span><b>I Approaching Patent Data</b></span></li>
<li class="chapter" data-level="1" data-path="datafields.html"><a href="datafields.html"><i class="fa fa-check"></i><b>1</b> Datafields</a></li>
<li class="chapter" data-level="2" data-path="datasets.html"><a href="datasets.html"><i class="fa fa-check"></i><b>2</b> Datasets</a></li>
<li class="part"><span><b>II Obtaining Patent Data</b></span></li>
<li class="chapter" data-level="3" data-path="databases.html"><a href="databases.html"><i class="fa fa-check"></i><b>3</b> Databases</a></li>
<li class="part"><span><b>III Cleaning Patent Data</b></span><ul>
<li class="chapter" data-level="3.1" data-path="databases.html"><a href="databases.html#install-open-refine"><i class="fa fa-check"></i><b>3.1</b> Install Open Refine</a></li>
<li class="chapter" data-level="3.2" data-path="databases.html"><a href="databases.html#create-a-project"><i class="fa fa-check"></i><b>3.2</b> Create a Project</a></li>
<li class="chapter" data-level="3.3" data-path="databases.html"><a href="databases.html#open-refine-basics"><i class="fa fa-check"></i><b>3.3</b> Open Refine Basics</a></li>
</ul></li>
<li class="part"><span><b>IV Visualizing Patent Data</b></span><ul>
<li class="chapter" data-level="3.3.1" data-path="databases.html"><a href="databases.html#filtering-the-data"><i class="fa fa-check"></i><b>3.3.1</b> Filtering the data</a></li>
<li class="chapter" data-level="3.3.2" data-path="databases.html"><a href="databases.html#setting-node-size"><i class="fa fa-check"></i><b>3.3.2</b> Setting Node Size</a></li>
<li class="chapter" data-level="3.3.3" data-path="databases.html"><a href="databases.html#colouring-the-nodes"><i class="fa fa-check"></i><b>3.3.3</b> Colouring the Nodes</a></li>
</ul></li>
<li class="part"><span><b>V Using APIs</b></span></li>
<li class="divider"></li>
<li><a href="https://github.com/wipo-analytics/wipo-analytics.github.io" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">The WIPO Manual on Open Source Patent Analytics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="databases" class="section level1">
<h1><span class="header-section-number">Chapter 3</span> Databases</h1>
<p>##Introduction</p>
<p>This chapter provides a quick overview of some of the main sources of free patent data. It is intended for quick reference and points to some free tools for accessing patent databases that you may not be familiar with.</p>
<p>It goes without saying that getting access to patent data in the first place is fundamental to patent analysis. There are quite a few free services out there and we will highlight some of the important ones. Most free sources have particular strengths or weaknesses such as the number of records that can be downloaded, the data fields that can be queried, the format the data comes back in or how <code>clean</code> data is in terms of the hours required to prepare for analysis. We won’t go into all of the details that but will provide some basic pointers.</p>
<p>##The Databases</p>
<p>###<a href="https://www.lens.org/lens/">The Lens</a></p>
<p>Previously known as the Patent Lens this is a well designed site with quite a few visualisation options and access to sequence data. It is possible to search the title, abstract, description and claims of patent documents and create and share data in collections. In 2015 the ability to download up to 10,000 records at a time was added. When combined with interactive charts that allow the user to drill down into results set, this has transformed the Lens into a very useful and innovative database and visualization tool.</p>
<p><img src="images/tools/Lens_2015-0517_14-19-26.png" /></p>
<p>###<a href="https://patentscope.wipo.int/search/en/search.jsf">Patentscope</a></p>
<p>The WIPO Patentscope database provides access to Patent Cooperation Treaty data including downloads of a selection of fields (up to 10,000 records), a very useful <a href="https://patentscope.wipo.int/search/en/clir/clir.jsf?new=true">search expansion translation tool</a>, and <a href="https://www3.wipo.int/patentscope/translate/translate.jsf?interfaceLanguage=en">translation</a>.</p>
<p><img src="images/tools/simplesearchresultspizza.png" /></p>
<p>Obtaining <a href="https://patentscope.wipo.int/search/en/sequences.jsf">sequence data from Patentscope</a>. Note that this rapidly becomes gigabytes of data.</p>
<p><img src="images/tools/pctseq.png" /></p>
<p>###<a href="http://worldwide.espacenet.com/?locale=en_EP">espacenet</a></p>
<p>Probably the best known free patent database from the European Patent Office.</p>
<p><img src="images/tools/Espacenet.png" /></p>
<p>###<a href="http://lp.espacenet.com">LATIPAT</a></p>
<p>For readers in Latin America (or Spain &amp; Portugal) LATIPAT is a very useful resource.</p>
<p><img src="images/tools/Espacenet_Latipat_2015-0517_15-11-21.png" /></p>
<p>###<a href="http://www.epo.org/searching/free/ops.html">EPO Open Patent Services</a></p>
<p>Access patent data through the EPO Application Programming Interface (API) free of charge. Requires programming knowledge.</p>
<p><img src="images/tools/OPS.png" /></p>
<p>The developer portal allows you to test your API queries and is recommended.</p>
<p><img src="images/tools/OPS_Developer_Portal.png" /></p>
<p>###<a href="http://www.patentsview.org/web/">USPTO Patents View</a></p>
<p>The <a href="http://patft.uspto.gov">USPTO main database search page</a> can reasonably be described as well… old. In 2016 the USPTO team initiated an <a href="http://www.uspto.gov/learning-and-resources/open-data-and-mobility">Open Data and Mobility initiative</a> that opens up USPTO patent and trademark data. The new <a href="https://developer.uspto.gov">Open Date Portal</a> is still in Beta but provides an insight into things to come.</p>
<p>As part of the shift to open data the USPTO has established an external <a href="http://www.patentsview.org/web/">Patents View</a> for free searches and <a href="http://www.patentsview.org/download/">bulk downloads</a>. If simple searching does not meet your needs, or the bulk options are too overwhelming, then <a href="http://www.patentsview.org/api/doc.html">the new JSON API service</a> is likely to meet your needs. The services are still in beta but this is a very exciting development for those who need greater levels of access to patent data or access to specific data fields.</p>
<p>###<a href="http://www.google.com/patents">Google Patents</a></p>
<p><img src="images/tools/googlepatents_2015-0517_14-09-22.png" /></p>
<p>The <a href="https://developers.google.com/patent-search/terms">Google Patent Search API</a> has been deprecated. Access through the Google Custom Search API with the API flag for patents <a href="http://stackoverflow.com/questions/15028166/python-module-for-searching-patent-databases-ie-uspto-or-epo">reported</a> to be <code>&amp;tbm=pts</code> with example code for using the API in Python.</p>
<p>In the free version of the Google Custom Search API data retrieval is limited and the patent field headings are unclear (that is they use non-standard names). For free patent analytics, Google Custom Search is presently of very limited use.</p>
<p>###<a href="https://www.google.com/patents/related">Google Prior Art Finder</a></p>
<p>The Google Prior Art Finder is a relatively recent development that allows you to enter search terms or patent numbers and to view and export results.</p>
<p><img src="images/tools/google_priorart1.png" /></p>
<p>The results include a Top Ten and are broken down into sections including Google Scholar, Patents etc.</p>
<p><img src="images/tools/google_priorart2.png" /></p>
<p>The Export button will export the top ten results for each section in a .csv file.</p>
<p><img src="images/tools/google_priorart3.png" /></p>
<p>It is possible to load more results for a section (e.g. see More Patent Results at the bottom of the results) and then export them (e.g. 20 patent documents rather than 10). In a test we managed to export 140 patent results but this could rapidly become laborious. An additional issue is that the data will need transposing. At the time of writing we had not identified an API route to Prior Art Finder.</p>
<p>###<a href="https://www.google.com/googlebooks/uspto.html">Google USPTO Bulk download</a></p>
<p>The <a href="http://patft.uspto.gov">USPTO patent databases</a> may be archaic but you can download the entire US collection from the <a href="https://www.google.com/googlebooks/uspto-patents.html">Google USPTO Bulk download service</a>.</p>
<p><img src="images/tools/USPTObulk.png" /></p>
<p>It is a fantastic service, and an example to patent offices everywhere on freeing up patent data. If you have a good broadband connection and the hard drive space, it is quite good fun to suddenly have access to millions of patent records. The authors used the service to text mine the collection for millions of biological species names as reported <a href="http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0078737">here</a>.</p>
<p><img src="images/tools/USPTOGrant.png" /></p>
<p>However, one important issue to note is that the XML delimiting individual documents is not always well demarcated. This means that any code that will work for one bulk set of files may fail on another set. While it is possible to address this, be prepared to spend time working on this and/or seek assistance from a professional programmer. For an insight into these issues see this <a href="http://stackoverflow.com/questions/25107557/parseing-xml-by-r-always-return-xml-declaration-error">Stackoverflow discussion</a> on parsing the data in R.</p>
<p>###<a href="http://www.freepatentsonline.com">Free Patents Online</a></p>
<p>Sign up for a free account for enhanced access and to save and download data. It has been around quite a while now and while the download options are limited we rather like it.</p>
<p><img src="images/tools/Freepatentsonline2015-03-26%2016-26-13.png" /></p>
<p>###<a href="http://www.dpma.de/english/service/e-services/depatisnet/">DEPATISnet</a></p>
<p>We are not covering national databases. However, the patent database of the German Patent and Trademark Office struck us as potentially very useful. It allows for searches in English and German and has extensive coverage of international patent data, including the China, EP, US and PCT collections. The coverage details are <a href="https://depatisnet.dpma.de/DepatisNet/depatisnet?action=datenbestand">here</a>. Worth experimenting with.</p>
<p><img src="images/tools/DEPATISnet_13-53-19.png" /></p>
<p>###<a href="http://www.oecd.org/sti/inno/oecdpatentdatabases.htm">OECD Patent Databases</a></p>
<p>One that is more for patent statisticians. The OECD has invested a lot of effort into developing patent indicators and resources including citations, the Harmonised Applicants names database <a href="http://www.oecd.org/sti/inno/43846611.pdf">HAN database</a>, mapping through the <a href="http://www.oecd.org/sti/inno/40794372.pdf">REGPAT database</a> among other resources that are available free of charge.</p>
<p><img src="images/tools/OECD_patent_databases.png" /></p>
<p>Along the same lines the US National Bureau of Economic Research <a href="http://www.nber.org/patents/">NBER US Patent Citations Data File</a> is an important resource.</p>
<p>###<a href="https://www.epo.org/searching-for-patents/business/patstat.html">EPO World Patent Statistical Database</a></p>
<p>The most important database for statistical use is the EPO World Patent Statistical Database (PATSTAT) and contains around 90 million records. PATSTAT is not free and costs 1250 Euro for a year (two editions) or 630 Euro for a single edition. The main barrier to using PATSTAT is the need to run and maintain a +200 Gigabyte database. However, there is also an online version of PATSTAT that is free for the first two months if you wish to try it by signing up for the trial (knowledge of SQL required).</p>
<p><img src="images/tools/patstat.png" /></p>
<p>For users seeking to load PATSTAT into a MySQL database Simone Mainardi provides the following <a href="https://github.com/simonemainardi/load_patstat">code on Github</a>.</p>
<p>###Other data sources</p>
<p>A number of companies provide access to patent data, typically with tiered access depending on your needs and budget. Examples include <a href="https://www.thomsoninnovation.com/login">Thomson Innovation</a>, <a href="https://www.questel.com/index.php/en/">Questel Orbit</a>, <a href="http://www.stn-international.de/index.php?id=123">STN</a>, and <a href="https://www.patbase.com/login.asp">PatBase</a>. We will not be focusing on these services but we will look at the use of data tools to work with data from services such as Thomson Innovation.</p>
<p>For more information on free and commercial data providers try the excellent <a href="http://www.piug.org">Patent Information User Group</a> and its list of <a href="http://wiki.piug.org/display/PIUG/Patent+Databases">Patent Databases</a> from Tom Wolff and Robert Austin.</p>
<p><img src="images/tools/PIUG_Wiki_2015-0517_15-45-05.png" /></p>
<p>Also worth mentioning is the Landon IP <a href="http://www.intellogist.com/wiki/Main_Page">Intellogist</a> blog which maintains <a href="http://www.intellogist.com/wiki/Category:Intellogist_Reports">Search System Reports</a></p>
<p><img src="images/tools/Intellogist_2015-0517_16-03-52.png" /></p>
<p>##Tools for Accessing Patent Data</p>
<p>In closing this chapter we will highlight a couple of tools for accessing patent data, typically using APIs and Python. We will come back to this later and are working to try this approach in R.</p>
<p>###<a href="https://github.com/Patent2net/Patent2Net">Patent2Net</a> in Python</p>
<p>A Python tool to access and process the data from the European Patent Office OPS service.</p>
<p><img src="images/tools/Patent2Net_GitHub_2015-0517_15-49-58.png" /></p>
<p>###<a href="https://github.com/55minutes/python-epo-ops-client">Python EPO OPS Client</a> by Gsong</p>
<p>A Python client for OPS access developed by Gsong and freely available on GitHub. Used in Patent2Net above.</p>
<p><img src="images/tools/python-epo-ops-client-GitHub_2015-0517_15-53-34.png" /></p>
<p>###<a href="https://github.com/funginstitute/patentserver">Fung Institute Patent Server</a> for USPTO data in JSON</p>
<p>Researchers at the Fung Institute have also been active in developing open source resources for accessing and working with patent data. We highlight <code>patentserver</code> but it is worth checking out other resources in the repository such as <a href="https://github.com/funginstitute">patentprocessor</a>, a set of Python scripts for processing USPTO bulk download data. Note that development of these tools no longer appears to be active.</p>
<p><img src="images/tools/funginstitutepatentserver.png" /></p>
<p>##Round Up</p>
<p>One problem confronting patent analysts is access to data in a form that is suitable for more detailed analysis. Typically this involves hundreds or many thousands of records. Recent years have increasingly opened up patent data through the ability to download 1,000 or 10,000 records at a time. However, access to downloads of titles, abstracts and claims or descriptions and full text remains limited when this is what is needed. Patent offices such as the USPTO have taken a leading role in making bulk patent data available and this is very much to be welcomed for those working on patent analytics. However, it is reasonable to say that the present situation is one of improvements in access (through Patentscope, the Lens and the EPO OPS service) but not quite in the quantitities or with the data fields patent analysts would like.</p>

<p>#The Lens</p>
<p>##Introduction</p>
<p>In this chapter we provide a brief introduction to <a href="https://www.lens.org/lens/">The Lens</a> patent database as a free source of data for patent analytics.</p>
<p>The Lens is a patent database based in Australia that describes itself as “an open global cyberinfrastructure to make the innovation system more efficient and fair, more transparent and inclusive.” The main way it seeks to do this is by providing access to patent information with a particular focus on sequence information as well as analysis of issues such as DNA related patent activity. An important feature of The Lens for those working on biotechnology related subjects is <a href="https://www.lens.org/lens/bio">PatSeq</a>.</p>
<p>##Getting Started</p>
<p>To get the most out of the Lens the first step is to sign up for an account from the front page.</p>
<p><img src="images/lens/fig1_front.png" /></p>
<p>It is possible to begin searching directly from the front page. However, selecting the small button next to the search box takes you to the search controls.</p>
<p><img src="images/lens/fig2_controls.png" /></p>
<p>As we can see we can use boolean queries, for searching a range of fields including the full text, title, abstract or claims (a major plus). We can also select one or multiple jurisdictions. In addition the results can be refined to patent applications or grants, and there are options for full text or one doc per family (which greatly reduces the number of results).</p>
<p>We used our standard query “pizza”, all jurisdictions, and one document per family. We turned stemming off.</p>
<p>Our search for pizza returned 13,714 families from a total of 29,617 publications containing the term in the full text. This approach assists with refining searches by reducing duplication.</p>
<p>The Lens allows users to create collections of up to 10,000 results from a search. To create a collection use the <code>Create Collection</code> button and name the collection. How you add records to a collection is not obvious and involves 2 steps.</p>
<ol style="list-style-type: decimal">
<li>Check the arrow next to Document as in the image below. When the mouse hovers over the arrow a menu will pop up. Choose <code>Top 10k Results</code>.</li>
<li>In the box displaying the name of the collection above the results press the + arrow to add the 10,000 documents to the Collection.</li>
</ol>
<p><img src="images/lens/fig3_addtocollection.png" /></p>
<p>Once you understand this process it is easy to add documents to collections. One very nice feature of the Lens is that when a collection has been created we can share it with others using the <code>Share</code> button. Users have the option of maintaining a private collection or publicly sharing. The URL for the collection we just generated is <a href="https://www.lens.org/lens/collection/9606" class="uri">https://www.lens.org/lens/collection/9606</a>.</p>
<p>We could imagine that for more restricted searches, and taking confidentiality issues into account, this could be a useful way of sharing patent data with colleagues. One useful addition would be the ability to share with groups based on email addresses or something similar (although that may be possible by choosing a private link and sharing it).</p>
<p>Using the small icons above <code>Document</code> on the left we can save our query for later use, limit the data to simple families or expand to publications, and download the data.</p>
<p>There are two main options for downloading data. The first is to download 1000 records by selecting the export button above <code>Document</code>.</p>
<p>When we select the export button we will be presented with a choice on the number of records to export and whether to export in JSON (for programmatic use), RIS for bibliographic software or .csv for use in tools such as Excel or other programmes.</p>
<p><img src="images/lens/fig4_exportoptions.png" /></p>
<p>The outputs of the export are clean and clear about what they represent when compared with some patent databases. A <code>url</code> link to the relevant file on the Lens is also provided which can assist in reviewing documents.</p>
<p><img src="images/lens/fig5_export.png" /></p>
<p>The JSON output (in the lower right of the image above) is also nice and clean.</p>
<p>The second route to exporting data is to download up to 10,000 results using the collections. When we select the <code>Work Area</code> icon at the top of the screen and select <code>Collections</code> we will see a new screen with a range of icons next to an individual collection.</p>
<p><img src="images/lens/fig5a_export_collection.png" /></p>
<p>When we select the download icon we can now download the 10,000 records in the collection in either .csv, ris or JSON formats. This is very easy to use once you understand how to navigate the interface.</p>
<p>We also have an option to upload documents into a collection using the upload button and then enter comma separated identifiers. However, at the time of writing we were not able to make this very useful function work.</p>
<p>##Additional features</p>
<p>In addition to these features, it is also important to note that data exports include a cited count that counts the number of patent/non-patent records <code>cited</code> by the applicant.</p>
<p>The online data also shows the citing documents. For example <a href="https://www.lens.org/lens/patent/US_3982033_A/citations#c/out">US 3982033 A Process for Coating Pizza Shells With Sauce</a> cites three patent documents but has <a href="https://www.lens.org/lens/patent/US_3982033_A/citations#c/in">11 forward citations by later applicants</a>.</p>
<p>While the citing documents are not included with the downloaded data it is possible to visit a record of interest online and then create a new set with the citing documents. Where a number of documents of interest have been identified this could be the basis for creating a new collection of cited or citing literature on a topic of interest linked to a core query.</p>
<p>As such, one possible workflow using the Lens would involve initial exploratory queries and refinement, downloading the results of a refined query for closer inspection and then selecting documents of interest to explore the backward (cited) and forward (citing) citations and generate a new dataset.</p>
<p>##Visualisation</p>
<p>The Lens makes good use of online visualisation options using <a href="http://www.highcharts.com">Highcharts</a> and HTML5. To access the visualisations choose the small icon on the right above the <code>Sort by</code> pull down menu.</p>
<p><img src="images/lens/fig6_visual.png" /></p>
<p>We now see a set of charts for our results. Using the up icon in the top right of each image we can get an expanded view and work with the charts. The Lens uses the Highcharts Javascript library and a very nice feature of this approach is that it the visuals are interactive and can be used to refine search results. In the image below we have opened the applicants image. As an aside, note that each image can be copied as an iframe to embed in your own web page.</p>
<p><img src="images/lens/fig7_applicants.png" /></p>
<p>This suggests that Google is the top user of the word pizza in the patent system with <a href="https://www.lens.org/lens/collection/9608">880 documents in 353 families</a>. We can then select the top result and the charts will regenerate focusing on our selection (in this case Google). To view the results we need to select the results button (the first on the right above the charts) to see the following.</p>
<p><img src="images/lens/fig8_google.png" /></p>
<p>What is very useful about is that it is easy to create a <a href="https://www.lens.org/lens/collection/9608">new collection</a> for an applicant of interest, to download the results or select areas of a portfolio based on a jurisdiction or technology area or to explore highly cited patents. In short, we can easily dig into the data.</p>
<p>Other interesting features of the chart area are references to authors, DOIs, and PubMed Ids for exploration of data extracted from the documents. This reflects the interest at the Lens in researching the relationship between basic scientific research and innovation. Accessing the literature related information requires opening a chart (for example authors) and selecting the top result and the moving into the results view. We then select one of the results such as <a href="https://www.lens.org/lens/patent/US_8200847_B2/citations#c/publications">Voice Actions On Computing Devices</a> and the Citations tab. This reveals a publication from a workshop on Wireless Geographical Information Systems from 2003 as we can see below.</p>
<p><img src="images/lens/fig9_crossref.png" /></p>
<p>An impressive feature of this approach is the effort that has been made to link the citation data to the publication using <a href="http://www.crossref.org">crossref</a>. According to the documentation around 15 million non-patent literature citations have been linked so far. Note that one additional feature of the Lens download data is that it includes a non-patent literature citation field. For example, downloading the <a href="https://www.lens.org/lens/collection/9608">google pizza portfolio</a> and a search for the citation above will reveal the citation but without the added value of the DOI. As such, the download provided the raw NPL data.</p>
<p>##Working with texts</p>
<p>In common with other free databases, the Lens is not designed to allow downloads of multiple full texts. However, you can access the full text of documents, including .pdf files, and you can make notes that will be stored with a collection in your account. The image below provides an example of our ongoing efforts to understand why Google is so dominant in the results of searches for pizza in patent documents.</p>
<p><img src="images/lens/fig10_notes.png" /></p>
<p>##PatSeq</p>
<p>One important focus of the development of the Lens has been DNA sequence data including an <a href="https://www.lens.org/about/">ongoing series of articles</a> on the interpretation and significance of sequence data in patent activity.</p>
<p><img src="images/lens/fig11_patseq.png" /></p>
<p>Patseq includes a number of tools.</p>
<ol style="list-style-type: decimal">
<li>PatSeq data permits access to patent documents disclosing sequences available for bulk download from a growing number of countries. This is a very useful site for obtaining sequence data. Note that you will need to request access to download sequence data in your account area.</li>
<li>Species finder and keyword search focuses on searching documents that contain a sequence for a species name or key term.</li>
</ol>
<p><img src="images/lens/fig12_patseq_species.png" /></p>
<p>A series of patent portfolios have been generated for some major plant and animal species, e.g. rice, maize, humans, chickens etc. That can be downloaded as collections.</p>
<ol start="3" style="list-style-type: decimal">
<li>The PatSeq Explorer allows the exploration of sequence data for four genomes (at present), notably the human and mouse genome for animals and the soybean, maize and rice genome for plants.</li>
</ol>
<p><img src="images/lens/fig13_patseq_explorer.png" /></p>
<p>This is an area where researchers with <a href="http://www.cambia.org/daisy/cambia/home.html">Cambia</a>, the non-profit organisation behind the Lens, have invested considerable effort and it is well worth reading the research articles listed on the Cambia and Lens websites on this topic. PatSeq Analyzer is closely related to the Explorer and presently provides details on the genomes mentioned above with a detailed summary of sequences by document including the region, sequence, transcript, single nucleotide polymorphisms (SNPs) and grants with sequences in the patent claims.</p>
<ol start="4" style="list-style-type: decimal">
<li>PatSeq Finder</li>
</ol>
<p>The PatSeq Finder allows a user to enter a DNA or amino acid sequence into the search box and find applications and grants with identical or similar sequences. We selected a sequence at random from the WIPO Patentscope sequence listings browser <a href="http://www.wipo.int/patentscope/search/en/detail.jsf?LANGUAGE=ENG&amp;KEY=16/026850&amp;ELEMENT_SET=F">W016/026850</a>.</p>
<p><img src="images/lens/fig14_seq_explorer.png" /></p>
<p>After processing we will see a list of results that can be downloaded in a variety of formats. The results indicate that our random sequence does not appear in the claims of a granted patent or a patent application but does appear in a number of applications and grants. Further details are provided by hovering over the individual entries and additional controls are available for similarity and other scores to refine the results.</p>
<p><img src="images/lens/fig15_seq_results.png" /></p>
<p>As far as we could tell, while the data can be downloaded, it is not presently possible to generate a collection of documents from the results of the PatSeq Finder.</p>
<p>##Round Up</p>
<p>The Lens is a very useful patent database that, when you have worked out the meaning of icons, is easy to use. The ease with which collections can be shared and up to 10,000 records downloaded is a real plus for the Lens. In addition, the use of HTML5 and Highcharts makes this a highly interactive experience. The ability to use charts to drill down into the data is very welcome. The link to the <code>crossref</code> service for non-patent literature is very useful but it would be good to see this data included in some way as a field in the data downloads.</p>
<p>With the addition of data downloads (in 2015) the Lens is becoming a very useful platform for searching, refining, visualizing and downloading patent data. What would perhaps be useful would be a set of demonstration walkthroughs or use cases that explain the way in which the Lens can be used in common work flows. For example, developing and refining a search, testing results, then retrieving backward and forward citations for refinement and visualization are quite common tasks in patent landscape analysis. Use cases would help users make the most of what the Lens has to offer.</p>
<p>The Lens also stands out for its distinctive long term work on sequence data in patents and this will be of particular interest to researchers working on biotech particularly in exploring the analytical tools.</p>

<p>#Patentscope</p>
<p>##Introduction</p>
<p><a href="https://patentscope.wipo.int/search/en/search.jsf">Patentscope</a> is the WIPO public access database. It includes coverage of the Patent Cooperation Treaty applications (administered by WIPO) and a <a href="https://patentscope.wipo.int/search/en/help/data_coverage.jsf">wide range of other countries</a> including the European Patent Office, USPTO and Japan totalling 51 million patent documents including 2.8 million PCT applications.</p>
<p>In this article we cover the basics of using Patentscope to search for and download up to 10,000 records. A detailed <a href="http://www.wipo.int/edocs/pubdocs/en/patents/434/wipo_pub_l434_08.pdf">User’s Guide</a> provides more details on specific features. A set of <a href="https://patentscope.wipo.int/search/en/tutorial.jsf">video tutorials</a> are also available. When compared with other free services Patentscope has the following main strengths.</p>
<ol style="list-style-type: decimal">
<li>Full text search in the description and claims of PCT applications on the day of publication and patent applications from a wide range of other countries including the United States, Japan, China and the European Patent Office among others.</li>
<li>Download up to 10,000 records</li>
<li>Expand search terms into multiple other languages using <code>Cross Lingual Expansion</code> or <a href="https://patentscope.wipo.int/search/en/clir/clir.jsf?new=true">CLIR</a></li>
<li>Simple, Advanced and Combined Field searching</li>
<li>Accessible in multiple languages and a <a href="https://www3.wipo.int/patentscope/translate/translate.jsf?interfaceLanguage=en">WIPO Translate</a> text function</li>
<li><a href="https://patentscope.wipo.int/search/mobile/index.jsf">Mobile version</a> and <a href="http://www.wipo.int/patentscope/en/news/pctdb/2015/news_0002.html">https:</a> access</li>
<li><a href="https://patentscope.wipo.int/search/en/sequences.jsf">Sequence listing downloads</a></li>
<li>Green technologies through the <a href="http://www.wipo.int/classifications/ipc/en/est/">IPC Green Inventory</a></li>
<li>Different types of graphical analysis of results lists on the fly using the Options menu.</li>
</ol>
<p>To get the most out of Patentscope it is a good idea to consult the two detailed guides and the video tutorials:</p>
<ol style="list-style-type: decimal">
<li><a href="http://www.wipo.int/edocs/pubdocs/en/patents/434/wipo_pub_l434_08.pdf">Patentscope Search: The User’s Guide</a>.</li>
<li>Patentscope CLIR for the Cross-Lingual Information Retrieval Tool <a href="https://patentscope.wipo.int/search/help/en/CLIR_DOC.pdf">here</a>.</li>
<li><a href="https://patentscope.wipo.int/search/en/tutorial.jsf">Patentscope video tutorials</a></li>
</ol>
<p>If you would like to download patent or sequence data you will need to register for a free account. To register for a free account go <a href="https://patentscope.wipo.int/search/en/reg/registration.jsf">here</a>.</p>
<p>##Collections to Search</p>
<p>Perhaps the best place to start is with the collections we will be searching. Those can be accessed under the Options menu on the main menu and then the tab reading <a href="https://patentscope.wipo.int/search/en/reg/registration.jsf">office</a>.</p>
<p><img src="images/patentscope/collections.png" /></p>
<p>We can see here that Patentscope provides access to the Patent Cooperation Treaty collection, regional collections such as the ARIPO and the European Patent Office and national collections such as the United States, Japan and Others. The ability to search and retrieve data from the LATIPAT collection will be particularly useful for researchers in Latin America and could be linked to analysis using the espacenet version of <a href="http://lp.espacenet.com">LATIPAT</a>. If you are only interested in particular collections, this is the place to change the settings.</p>
<p>##Simple Search</p>
<p>We can select a range of different fields for search. In this case we have selected full text from the drop down menu for a simple search on the term pizza.</p>
<p><img src="images/patentscope/simplesearch.png" /></p>
<p>Note that Patentscope groups documents for the same application into a record or dossier and that we are seeing the document that is the key for the record. The other documents in the dossier for the record can be accessed by clicking the document number and selecting the Documents menu as in this <a href="https://patentscope.wipo.int/search/en/detail.jsf?docId=WO2014047700&amp;recNum=2&amp;tab=PCTDocuments&amp;maxRec=25177&amp;office=&amp;prevFilter=&amp;sortOption=Relevance&amp;queryString=ALLTXT%3A%28pizza%29">example</a>.</p>
<p>For more details on using simple search see the <code>Simple Search</code> <a href="https://patentscope.wipo.int/search/en/tutorial.jsf">video tutorial</a>. Videos are also available on the use of Field Combinations for constructing searches and Advanced Search.</p>
<p>##Results</p>
<p>When we arrive at the results page we can see that we have 24,614 results with our query displaying as searching <code>AllTXT</code> and all languages. We then have an RSS button to copy the feed over to an RSS feeder for updates.</p>
<p><img src="images/patentscope/patentscopsesimple_pizza.png" /></p>
<p>There is also a query tree button that displays results by language and terms in the relevant sections of the document. We can see an example of this for a more complex query below.</p>
<p><img src="images/patentscope/query_tree.png" /></p>
<p>A video tutorial is also available for the <a href="https://patentscope.wipo.int/search/en/tutorial.jsf">Search result list</a></p>
<p>##Downloading Results</p>
<p>The two excel icons at the end of the menu allow a user to download either the short list (first icon) or the second list as a <code>.xls</code> file. To see these icons you must be logged in with a user account or they will not display.</p>
<p><img src="images/patentscope/pizzaexporting.png" /></p>
<p>When we download these results we will receive an <code>.xls</code> sheet with up to 10,000 entries with a couple of header rows that show the query. Note that each record in the Excel sheet is hyperlinked to the corresponding record in Patentscope.</p>
<p><img src="images/patentscope/results.png" /></p>
<p>We will go into the use of this data, including with Tableau Public and other tools, in some depth and there are a few things to note here. The first is that the hyperlinked publication number does not possess a kind code (A1, B1 etc.). This only matters in the sense that the number will retrieve multiple documents in other databases linked to the Patentscope number. A second point to note is that Patentscope data is <code>raw</code> in the sense that it is data as it comes from the data providers and is not processed. That means that there can be encoding issues that we will come back to later on in the discussions on data cleaning.</p>
<p>What is very useful about Patentscope is that we can actually obtain quite a significant volume of data on a topic of interest. While this article has simply downloaded the first 10,000 results, to obtain the full result set it would be easy enough to limit the data by year and download the data as a series of sets that can be combined later (e.g. three sets).</p>
<p>To do this we need to visit the Field Combination Page. Here we will start by putting our query in English All to gain the total number of results. Then we will restrict the data by the publication data field using <code>[]</code> and period between dates (as DD.MM.YYYY). An example is shown below.</p>
<p><img src="images/tools/wipo_settings.png" /></p>
<p>This will helpfully show the total results for the query (although it can take some time) and we can run and then download results for each year limited segment.</p>
<p><img src="images/tools/wipo_results.png" /></p>
<p>When working with multiple downloads it is a good idea to write down the total number of results and then the results for each date limited segment to ensure that the data adds up to what you would expect. Some experimentation may also be needed with the field settings using the Boolean AND/OR operators.</p>
<p>##Cross Lingual Searching</p>
<p>One challenge in patent searching is the use of different expressions in different languages for the same query. Patentscope presents a very useful solution to this through cross-lingual searching. From the pull down menu select <code>Cross Lingual Expansion</code>, then enter the search terms and press go. The tool will now generate search terms in multiple languages.</p>
<p><img src="images/patentscope/pizzacrosslingual.png" /></p>
<p>To go further with this tool use either the slider settings (precision vs. recall). For example, if we were to insert the search term “synthetic biology” and move recall to the top level (4), we would generate the following query.</p>
<p><code>&quot;FP:((EN_TI:(&quot;synthetic biology&quot; OR &quot;biologic synthetic&quot;) OR EN_AB:(&quot;synthetic biology&quot; OR &quot;biologic synthetic&quot;)) OR (DE_TI:(&quot;synthetische Biologie&quot; OR &quot;synthetischen biologischen&quot; OR &quot;biologische synthetische&quot; OR &quot;Biologische synthetische&quot;) OR DE_AB:(&quot;synthetische Biologie&quot; OR &quot;synthetischen biologischen&quot; OR &quot;biologische synthetische&quot; OR &quot;Biologische synthetische&quot;)) OR (ES_TI:(&quot;biológicas sintéticas&quot;) OR ES_AB:(&quot;biológicas sintéticas&quot;)) OR (FR_TI:(&quot;biologie synthétique&quot; OR &quot;biologie synthéthique&quot;) OR FR_AB:(&quot;biologie synthétique&quot; OR &quot;biologie synthéthique&quot;)) OR (JA_TI:(&quot;生物合成&quot; OR &quot;合成生体&quot; OR &quot;の生物学的合成&quot;) OR JA_AB:(&quot;生物合成&quot; OR &quot;合成生体&quot; OR &quot;の生物学的合成&quot;)) OR (ZH_TI:(&quot;合成生物&quot;) OR ZH_AB:(&quot;合成生物&quot;)))&quot;</code></p>
<p>If supervised mode is selected from the <code>Expansion mode</code> drop down, it becomes possible to select technology areas for the generation of terminology. While we haven’t worked through this in detail that could be very helpful for domain specific query generation. All in all, this is one of the most original and powerful tools that Patentscope has to offer. A detailed <code>.pdf</code> guide to using CLIR is available <a href="https://patentscope.wipo.int/search/help/en/CLIR_DOC.pdf">here</a>.</p>
<p>##Sequence Data</p>
<p>A third major feature of Patentscope is access to DNA and amino acid sequence listings filed with PCT Applications. This data can be accessed and downloaded for individual records <a href="https://patentscope.wipo.int/search/en/sequences.jsf">here</a>.</p>
<p><img src="images/patentscope/sequencesearching.png" /></p>
<p>A sample record from the lists can be seen below as a plain text file. Note that some issues may arise with reconciling the plain text file with the WIPO publication number (WO etc.) and this merits careful attention if using this data.</p>
<p><img src="images/patentscope/pctseq.png" /></p>
<p>Registered account holders can also use the <code>ftp anonymous download</code> service from the same page. This provides access to the sequence data by year as can be seen below.</p>
<p><img src="images/patentscope/sequenceftp.png" /></p>
<p>If using the anonymous ftp service note that the recent data is measured in gigabytes, so do not try to download this data over a weak WIFI connection, a gated connection or to your phone(!). Nevertheless, the open accessibility of this data is important. For other sequence data sources you may be interested in the European Bioinformatics Institute resources <a href="http://www.ebi.ac.uk/patentdata">here</a> and for the US by document number <a href="http://seqdata.uspto.gov/">here</a> and until March 2015 at the DNA Patent Database <a href="https://dnapatents.georgetown.edu/">here</a>. Also important is the Lens <code>Patseq</code> tool <a href="https://www.lens.org/lens/bio/sequence">here</a>.</p>
<p>##Round Up</p>
<p>WIPO Patentscope is a powerful tool for gaining access to a significant amount of patent data on a topic of interest. The ability to download 10,000 or more records at a time cannot be beaten by other free tools. The <code>Cross Lingual Searching</code> tool appears to be unique and valuable. Free access to bulk download of sequence data is likely to keep bioinformaticians happy for quite a long time.</p>
<p>One way of thinking about the role of Patentscope in patent analytics is as a resource that can be combined with other data tools. For example, if we wanted to obtain the abstracts, descriptions or claims of PCT documents in Patentscope then we might use the Patenscope numbers to retrieve data from EPO Open Patent Services or Google Patents using R or Python or other tools. That is, in this case Patentscope overcomes the limitations of search results from other tools but allows for the targeted use of other tools to retrieve more information. The <code>Cross Lingual Searching</code> tool could also be particularly useful for trying to identify and later acquire patent documents from other jurisdictions where a company or organisation may be seeking to operate or to expand patent landscape analysis into jurisdictions with non-Roman alphabets.</p>
<p>The main difficulties that arise from using Patentscope can stem from occasional noise in the data. Patenscope does not clean the data provided from the individual collections with the exception of checking for typological errors in priority numbers and IPC codes. In addition, all text is transformed into UTF-8. However, as is common when dealing with diverse data sources, the results are not always perfect. In addition, because Patentscope data is drawn from a wide range of languages users may need to update their font libraries if large numbers of unusual characters appear in the data (such as installing the Asian language pack for Windows). In practice, as is common with most patent data sources, this can mean significant time is required to clean up the data. Having said this, no other free database tool allows us to download as much data in table form for analysis. As we will see, it is possible to do a lot with Patentscope data.</p>

</div>



<p>Cleaning patent data is one of the most challenging and time consuming tasks involved in patent analysis. In this chapter we will cover.</p>
<ol style="list-style-type: decimal">
<li>Basic data cleaning using Open Refine</li>
<li>Separating a patent dataset on applicant names and cleaning the names.</li>
<li>Exporting a dataset from Open Refine at different stages in the cleaning process.</li>
</ol>
<p><a href="http://openrefine.org">Open Refine</a> is an open source tool for working with all types of messy data. It started life as Google Refine but has since migrated to Open Refine. It is a programme that runs in a browser on your computer but does not require an internet connection. It is a key tool in the open source patent analysis tool kit and includes extensions and the use of custom code for particular custom tasks. In this article we will cover some of the basics that are most relevant to patent analysis and then move on to more detailed work to clean patent applicant names.</p>
<p>The reason that patent analysts should use Open Refine is that it is the easiest to use and most efficient free tool for cleaning patent data without programming knowledge. It is far superior to attempting the same cleaning tasks in Excel or Open Office. The terminology that is used can take some getting used to, but it is possible to develop efficient workflows for cleaning and reshaping data using Open Refine and to create and reuse custom codes needed for specific tasks.</p>
<p>In this chapter we use Open Refine to clean up a raw dataset from WIPO Patentscope containing nearly 10,000 raw records that make some kind of reference to the word <code>pizza</code> in the whole text. To follow this chapter using one of our training sets, download it from the Github repository <a href="https://github.com/poldham/opensource-patent-analytics/blob/master/2_datasets/pizza_medium/pizza_medium.csv?raw=true">here</a> or use your own dataset.</p>
<div id="install-open-refine" class="section level2">
<h2><span class="header-section-number">3.1</span> Install Open Refine</h2>
<p>To install Open Refine visit the Open Refine <a href="http://openrefine.org">website</a> and <a href="http://openrefine.org/download.html">download</a> the software for your operating system:</p>
<p><img src="images/openrefine/OpenRefine-2015-06-01%2012-18-34.png" /></p>
<p>From the download page select your operating system. Note the extensions towards the bottom of the page for future reference.</p>
<p><img src="images/openrefine/OpenRefine-download.png" /></p>
<p>At the time of writing, when you download Open Refine it actually downloads and installs as Google Refine (reflecting its history) and so that is the application you will need to look for and open.</p>
</div>
<div id="create-a-project" class="section level2">
<h2><span class="header-section-number">3.2</span> Create a Project</h2>
<p>We will use the Patentscope Pizza Medium file that can be downloaded from the repository <a href="https://github.com/poldham/opensource-patent-analytics/blob/master/2_datasets/pizza_medium/pizza_medium.csv?raw=true">here</a>.</p>
<p><img src="images/openrefine/create_project.png" /></p>
<p>The file will load and then attempt to guess the column separator. Choose <code>.csv.</code> Note that a wide range of files can be imported and that there are additional options such as storing blank cells as nulls that are selected by default. In the dataset that you will load we have prefilled blank cells with <code>NA</code> values to avoid potential problems using fill down in Open Refine discussed below.</p>
<p><img src="images/openrefine/create_project2.png" /></p>
<p>Click <code>create project</code> in the top right of the bar as the next step.</p>
</div>
<div id="open-refine-basics" class="section level2">
<h2><span class="header-section-number">3.3</span> Open Refine Basics</h2>
<p>A few basic features of Open Refine will soon have you working smoothly. Here is a quick tour.</p>
<p>###Open Refine runs in a browser</p>
<p>Open Refine is an application that lives on your computer but runs in a browser. However, it does not require an internet connection and it does not lose your work if you close the browser.</p>
<p>###Open Refine works on columns.</p>
<p>At the top of each column is a pull down menu. Be ready to use these menus quite a lot. In particular, you will often use the <code>Edit cells &gt; Common tranforms</code> as shown below for functions such as trimming white space.</p>
<p><img src="images/openrefine/menu_basics.png" /></p>
<p>Other important menus are <code>Edit column</code>, immediately below <code>Edit cells</code>, for copying or splitting columns into new columns.</p>
<p>###Open Refine works with Facets.</p>
<p>The term <code>facet</code> may initially be confusing but basically calls up a window that arranges the items in a column for inspection, sorting, and editing as we can see below. This is important because it becomes possible to identify problems and address them. It also becomes possible to apply a variety of clustering algorithms to clean up the data. Note that the size of the facet window can be adjusted by dragging the bottom of the window as we have done in this image.</p>
<p><img src="images/openrefine/facet_menu.png" /></p>
<p>Hovering over an item inside the facet window brings up a small <code>edit</code> button that allows editing, such as removing <i> and </i> from the title.</p>
<p>###Custom Facets</p>
<p>The Facet menu below brings up a custom menu with a range of options. Selecting <code>Custom text facet</code> (see below) brings up a pop up that allows for the use of code in <a href="https://github.com/OpenRefine/OpenRefine/wiki/GREL-Functions">Open Refine Expression Language (GREL)</a> to perform tasks not covered by the main menu items. This language is quite simple and can range from short snippets for finding and replacing text to more complex functions that can be reused in future. We will demonstrate the use of this function below.</p>
<p><img src="images/openrefine/facet_menu.png" /></p>
<p>###Reordering Columns</p>
<p>There are two options for reordering columns. The first is to select the column menu then <code>Edit column &gt; Move column to beginning</code>. The second option, shown below, is to select the <code>All</code> drop down menu in the first column and then <code>Edit columns &gt; Re-order/remove columns</code>. In the pop up menu of fields drag the desired field to the top of the list. In this case we have dragged the <code>priority_date</code> column to the top of the list. It will now appear as the first data column.</p>
<p><img src="images/openrefine/reorder_columns.png" /></p>
<p>###Undo and Redo</p>
<p>Open Refine keeps track of each action and allows you to go back several steps or to the beginning. This is particularly helpful when testing whether a particular approach to cleaning (e.g. splitting columns or using a snippet of code) will meet your needs. In particular it means you can explore and test approaches while not worrying about losing your previous work.</p>
<p><img src="images/openrefine/undo_redo.png" /></p>
<p>However, it can be important to plan the steps in your clean up operation to avoid problems at later stages. It may help to use a notepad as a checklist (see below). The main issue that can arise is where cleanup moves forward several steps without being fully completed in an earlier step. In some cases this can require returning to that earlier step, restarting and repeating earlier steps. As you become more familiar with Open Refine it will be easier to work out an appropriate sequence for your workflow.</p>
<p>###Exporting</p>
<p>When a clean up exercise is completed a file can be exported in a variety for formats. When working with patent data expect to create more than one file (e.g. core, applicants, inventors, IPC) to allow for analysis of aspects of the data in other tools. In this chapter we will create two files.</p>
<ol style="list-style-type: decimal">
<li>A cleaned version of the original data</li>
<li>An applicants file that separates the data by each applicant.</li>
</ol>
<p>##Basic Cleaning</p>
<p>This is the first step in working with a dataset and it will make sense to perform some basic cleaning tasks before going any further. The Pizza Medium dataset that we are working with in this article is raw in the sense that the only cleaning so far has been to remove the two empty rows at the head of the data table and to fill blank cells with NA values. The reason that it makes sense to do some basic cleaning before working with applicant, inventor or IPC data is that new datasets will be generated by this process.</p>
<p>Bear in mind that Open Refine is not the fastest programme and make sure that you allocate sufficient time for the clean up tasks and are prepared to be patient while the programme runs algorithms to process the data. Note that Open Refine will save your work and you can return to it later.</p>
<p>When working with Open Refine we will typically be working on one column at a time. However, the key <code>checklist</code> for cleaning steps is:</p>
<ol style="list-style-type: decimal">
<li>Make sure you have a back up of the original file. Creating a <code>.zip</code> file and marking it with the name <code>raw</code> can help to preserve the original.</li>
<li>Open and save a text file as a <code>code book</code> to write down the steps taken in cleaning the data (e.g. pizza_codebook.txt).</li>
<li>Regularise characters (e.g. title, lowercase, uppercase).</li>
<li>Remove leading and trailing white space.</li>
<li>Address encoding and related problems.</li>
</ol>
<p>Additional actions:</p>
<ol start="6" style="list-style-type: decimal">
<li>Transform dates</li>
<li>Access additional information and create new columns and/or rows.</li>
</ol>
<p>We will generally approach these tasks in each column and steps 6 and 7 will not always apply. The creation of a codebook will allow you to keep a note of all the steps taken to clean up a dataset. The codebook should be saved with the cleaned dataset (e.g. in the same folder) as a reference point if you need to do further work or if colleagues want to understand the transformation steps.</p>
<p>###Changing case</p>
<p>The first column in our Patentscope pizza dataset is the publication number. To inspect what is happening and needs to be cleaned in this column we will first select the column menu and choose <code>text facet</code> from the dropdown. This will generate the side menu panel that we can see below containing the data. We can then inspect the column for problems.</p>
<p><img src="images/openrefine/cleaning_pubno_createfacet.png" /></p>
<p>When we scroll down the side panel we can see that some publication numbers have a lowercase country code (in this case <code>ea</code> rather than <code>EA</code> for Eurasian Patent Organization using the <a href="http://www.wipo.int/pct/guide/en/gdvol1/annexes/annexk/ax_k.pdf">WIPO standardized country codes</a>). To address this we select the column menu <code>Edit cells &gt; Common transforms &gt; to Uppercase</code>.</p>
<p><img src="images/openrefine/cleaning_pubno_totitlecase.png" /></p>
<p>If we scroll down all the publication numbers will have been converted to upper case. This will make it easier to extract the publication country codes at a later stage.</p>
<p>###Regularise case</p>
<p>For the other text columns it is sensible to repeat the common transformations step and select <code>to titlecase</code>. Note that this will generally work well for the title field but may not always work as well on concatenated fields such as applicants and inventor names. Repeat this step following the separation of these concatenated fields (see below on applicants). If the abstract or claims were present we would not regularise those text fields.</p>
<p>###Remove leading and trailing whitespace</p>
<p>To remove leading and trailing white space in a column we select <code>Edit cells &gt; Common transforms &gt; Trim</code> leading and trailing white space across the columns.</p>
<p><img src="images/openrefine/title_trailingwhitespace.png" /></p>
<p>Note that following the splitting of concatenated cells with multiple entries such as the applicants and the inventors fields it is a good idea to repeat the trim exercise when the process is complete to avoid potential leading white space at the start of the new name entries.</p>
<p>###Add Columns</p>
<p>We can also add columns by selecting the column menu and <code>Edit Column &gt; Add column based on this column</code>. In this case we have added a column called publication_date.</p>
<p><img src="images/openrefine/add_column_publicationyear.png" /></p>
<p>We have a number of options with respect to dates (see below). In this case we want to separate out the date information into separate columns. To do that we can use <code>Edit column &gt; Split into several columns</code>. We can also choose the separator for the split, in this case <code>.</code> and whether to keep or delete the source column. In this case we selected to retain the original column and created three new date related columns. We could as necessary then delete the date and month column if we only needed the year field.</p>
<p><img src="images/openrefine/split_date.png" /></p>
<p>We can then rename these columns using the edit <code>Edit Column &gt; Rename this column</code>. Note that in this case the use of lowercase and underscores marks out columns we are creating or editing as a flag for internal use informing us that this is a column that we have created. At a later stage we will rename the original fields to mark them as original.</p>
<p><img src="images/openrefine/rename_column.png" /></p>
<p>###Address Encoding and related problems</p>
<p>The <a href="https://github.com/OpenRefine/OpenRefine/wiki/Recipes">Recipes</a> section of the documentation provides helpful tips and example code for dealing with encoding and related problems that are reproduced here:</p>
<ol style="list-style-type: decimal">
<li>corrupted characters. This arises from aggregation of data from different sources. In Patentscope the data is converted to UTF8. However, if problems are encountered select <code>Edit cells &gt; Transform</code> and try entering the following.</li>
</ol>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1-1" data-line-number="1"><span class="kw">value.reinterpret</span>(<span class="st">&quot;utf-8&quot;</span>)</a></code></pre></div>
<p>It may be necessary to explore and test other sets which can be identified <a href="http://java.sun.com/j2se/1.5.0/docs/guide/intl/encoding.doc.html">here</a>.</p>
<ol start="2" style="list-style-type: decimal">
<li>Escape html/XML characters e.g. &amp;amp</li>
</ol>
<p>The most likely source of patent data is XML but to be on the safe side the following should escape (remove) html and XML code appearing in the text.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2-1" data-line-number="1"><span class="kw">value.unescape</span>(<span class="st">&quot;html&quot;</span>)<span class="kw">.unescape</span>(<span class="st">&quot;xml&quot;</span>)</a></code></pre></div>
<ol start="3" style="list-style-type: decimal">
<li>Question marks</li>
</ol>
<p>Question marks often show up for characters that cannot be represented are a sign of encoding problems. In addition non-breaking spaces may be represented as <code>&amp;nbsp</code> (Unicode(16)). To find a Unicode value go to <code>Edit cells &gt; Transform</code> and then enter unicode(value) which will transform all the characters to unicode numbers. From there you can look up the problem.</p>
<p>A quick fix is proposed in the documentation that may work in some circumstances.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb3-1" data-line-number="1"><span class="kw">split</span>(<span class="kw">escape</span>(value,<span class="st">&#39;xml&#39;</span>),<span class="st">&quot;&amp;#160;&quot;</span>)[<span class="dv">0</span>]</a></code></pre></div>
<p>Within this particular dataset, we found that these quick tips did not work (presumably because the text had already been converted to UTF-8). However, if all else fails an alternative is to simply find and replace in Transform as in the example below.</p>
<p><img src="images/openrefine/replace_encoding.png" /></p>
<p>This is not a very satisfactory solution because it requires inspection of the dataset to identify the specific character problems and then replacing the value. That will be time consuming.</p>
<p>###Reformatting dates</p>
<p>One problem we may encounter is that the standard date definition on patent documents (e.g. 21.08.2009) may not be recognised as a date field in our analysis software because dates can be ambiguous from the perspective of software code. For example, how should 08/12/2009 or 12/08/2009 be interpreted?</p>
<p>Alternatively, as in this case, the decimal points may not be correctly interpreted as signifying a date in some software (e.g. R). We might anticipate this and transform the data into a more recognisable form such as 21/08/2009. A very simple way to do this is by using a replace function. In this case we select the menu for the <code>publication_date</code> field and then <code>Edit cells &gt; Transform</code>.</p>
<p><img src="images/openrefine/transform.png" /></p>
<p>This produces a menu where we enter a simple GREL replace code.</p>
<p><img src="images/openrefine/transform_replace_date.png" /></p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb4-1" data-line-number="1"><span class="kw">replace</span>(value, <span class="st">&#39;.&#39;</span>, <span class="st">&#39;/&#39;</span>)</a></code></pre></div>
<p>This simple replace code is basically the same as find and replace in Excel or Open Office. In addition, we can see the consequences of the choice in the panel before we run the command. This is extremely useful for spotting problems. For example, if attempting to split a field on a comma we may discover that there are multiple commas in a cell (see the <code>Priority Data</code> field for this). By testing the code in the panel we could then work to find a solution or edit the offending texts in the main facet panel.</p>
<p>To find other simple codes visit the <a href="https://github.com/OpenRefine/OpenRefine/wiki/Recipes">Open Refine Recipes page</a>.</p>
<p>We will now focus on extracting information from some of the columns before saving the dataset and moving on.</p>
<p>###Access additional information</p>
<p>There are a range of pieces of information that are hidden in data inside columns. For example, Patentscope data does not contain a publication country field. In particular, note that Patentscope merges all publications for an application record into one dossier. So we are only seeing one record for a set of documents (in Patentscope the wider dossier for a record is accessible through the Documents section of the website). This is very helpful in reducing duplication but it is important to bear in mind that we are not seeing the wider family in our data table. However, we can work with the information at the front of the publication number in the Patentscope records using a very simple code and create a new column (as above) based on the values returned as below.</p>
<p><img src="images/openrefine/publication_country.png" /></p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb5-1" data-line-number="1"><span class="kw">substring</span>(value, <span class="dv">0</span>, <span class="dv">2</span>)</a></code></pre></div>
<p>Note here that the code begins counting from 0 (e.g. 0, 1 = U, 2 = S). The first part of the code looks in the value field. 0 tells the code to begin counting from 0 and the 2 tells it to read the two characters from 0. We could change these values, e.g to 1 and 4 to capture only a chunk of a number.</p>
<p>##Fill blank cells</p>
<p>Filling blank cells with a value (NA for Not Available) to prevent calculation problems with analysis tools later can be performed by selecting each column, creating a text facet, scrolling down to the bottom of the facet choosing <code>(blank)</code>, edit and then entering NA for the value.</p>
<p><img src="images/openrefine/blank_na_facets.png" /></p>
<p>Note that this is somewhat time consuming (until a more rapid method is found) but has the benefit of being accurate. It is generally faster to open the file in Excel (or Open Office) and use find and replace with the find box left blank and NA in the replace field across the data table. For that reason, blank cells should not appear in the dataset you are using in this article. However, in later steps below we will be generating blank cells by splitting the applicant and inventor field. It is therefore important to know this procedure.</p>
<p>##Renaming columns</p>
<p>At this stage we have a set of columns that are mixed between the original sentence case and the additions in lower case with no spaces such as <code>publication_number</code>. This is a matter of personal preference but it is generally a good idea to regularise the case of all columns to make them easy to remember. In this case we will also add the word <code>original</code> to the columns to distinguish between those created by cleaning the data and those we have created.</p>
<p>##Exporting Data</p>
<p>When we are happy that we have worked through the core cleaning steps it is a good idea to export the new core dataset. It is important to do this before the steps described below because it preserves a copy of the core dataset that can be used for separation (or splitting activities) on applicants, inventors, IPC etc. during the next steps. It is important that this <code>clean</code> dataset is as clean as is reasonably possible before moving on. The reason for this is that <strong><em>any noise or problems will multiply</em></strong> when we move on to the next steps. This may require a major rerun of the cleaning steps on later files created for applicants or inventors. Therefore make sure that you are happy that the data is as clean as is reasonably possible at this stage. Then choose export from the menu and the desired format (preferably <code>.csv</code> or <code>.tab</code> if using analytics tools later).</p>
<p><img src="images/openrefine/export_core.png" /></p>
<p>##Splitting Applicants</p>
<p>This article was inspired by a <a href="http://www.patinformatics.com/blog/patent-assignee-cleanup-using-google-refine-open-refine-text-facets-and-clustering/">very useful tutorial</a> on cleaning assignee names with Google Refine by Anthony Trippe. Anthony is also the author of the forthcoming <a href="http://www.wipo.int/patentscope/en/programs/patent_landscapes/">WIPO Guidelines for Preparing Patent Landscape Reports</a> and the <a href="http://www.patinformatics.com">Patentinformatics LLC</a> website has played a pioneering role in promoting patent analytics. We will take this example forward using the our sample pizza patent dataset so that it can be visualised in a range of tools. If you have not done so already, you can download the dataset <a href="https://github.com/poldham/opensource-patent-analytics/blob/master/2_datasets/pizza_medium/pizza_medium.csv?raw=true">here</a>.</p>
<p>We actually have two options here and we will go through them so that you can work out your needs in a particular situation. Note that you can use Undo in Google Refine to go back to the point immediately before you tested these approaches. However, if you have followed the data cleaning steps above, make sure that you have already exported a copy of the main dataset.</p>
<p>###Situation 1 - First Applicants</p>
<p>As discussed by Anthony Tripp we could split the applicant column into separate columns by choosing, <code>Applicants &gt; Edit column &gt; Split into several columns</code>.</p>
<p><img src="images/openrefine/split_applicants_columns.png" /></p>
<p>We then need to select the separator. In this case (and normally with patent data), it is <code>;</code>.</p>
<p><img src="images/openrefine/split_selectseparator.png" /></p>
<p>This will produce a set of 18 columns.</p>
<p><img src="images/openrefine/split_columns.png" /></p>
<p>At this point, we could begin the clustering process to start cleaning the names that is discussed in situation 2. However, the disadvantage with this is that with this size of dataset we would need to do this 18 times in the absence of an easy way of combining the columns into a single column (applicants) with a name on each row. We might want to use this approach in circumstances where we are not focusing on the applicants and are happy to accept the first name in the list as the first applicant. In that case we would simply be reducing the applicant field to one applicant. Bear in mind that the first applicant listed in the series of names may not always be the first applicant as listed on an application and may not be an organisation name. Bearing these caveats in mind, we might also use this approach to reduce the concatenated inventors field to one inventor. For general purposes that would be clean and simple for visualisation purposes.</p>
<p>However, if we wanted to perform detailed applicant analysis for an area of technology, we would need to adopt a different approach.</p>
<p>###Situation 2 - All Applicants</p>
<p>One of the real strengths of Open Refine is that it is very easy to separate applicant and inventor names into individual rows. Instead of choosing Edit column we now choose <code>Edit cells</code> and then <code>split multi-valued cells</code>.</p>
<p><img src="images/openrefine/split_multivaluedcells.png" /></p>
<p>In the pop up menu choose <code>;</code> as the separator rather than the default comma.</p>
<p>We now have a dataset with 15,884 rows as we can see below.</p>
<p><img src="images/openrefine/split_multivaluedcells2.png" /></p>
<p>The advantage of this is that all our individual applicant names are now in a single column. However, note that the rest of the data has not been copied to the new rows. We will come back to this but as a precaution it is sensible to fill down on the publication number column as the key that links the individual applicants to the record. So let’s do that for peace of mind by selecting <code>publication number &gt; edit cells &gt; fill down</code>.</p>
<p><img src="images/openrefine/split_filldown.png" /></p>
<p>We should now have a column filled with the publication number values for each applicant as our key. Note here that there is a need for caution in using <code>fill down</code> in Open Refine as discussed in detail <a href="http://googlerefine.blogspot.co.uk/2012/03/fill-down-right-and-secure-way.html">here</a>. Basically, fill down is not performed by record, it simply fills down. That can mean that data becomes mixed up. This is another reason why it is important to fill blank values with NA either before starting work in Open Refine or as one of the initial clean up steps. Using NA early on will help prevent refine from filling down blank cells with the values of another record.</p>
<p>If you have not already done so above to assist the clean up process, and as general good practice, transform the mixed case in the applicants field to a single case type. To do that select <code>Applicants &gt; Edit Cells &gt; Common Transformations &gt; To titlecase</code>.</p>
<p><img src="images/openrefine/applicants_titlecase.png" /></p>
<p>We now move back to the applicants and select <code>Facet &gt; Text Facet</code>.</p>
<p><img src="images/openrefine/applicants_textfacet.png" /></p>
<p>What we will see (with Applicants moved to the first column by selecting <code>Applicants &gt; Edit Column &gt; Move column to beginning</code>) is a new side window with 9,368 choices.</p>
<p><img src="images/openrefine/applicants1.png" /></p>
<p>The cluster button will trigger a set of six clean up algorithms with user choices along the way. It is well worth reading the <a href="https://github.com/OpenRefine/OpenRefine/wiki/Clustering-In-Depth">documentation</a> on these steps to decide what will best fit your needs in future. These cleaning steps proceed from the strict to the lax in terms of matching criteria. The following is a brief summary of the details provided in the documentation page:</p>
<ol style="list-style-type: decimal">
<li>Fingerprinting. This method is the least likely in the set to produce false positives (and that is particularly important for East Asian names in patent data). It involves a series of steps including removing trailing white space, using all lowercase, removing punctuation and control characters, splitting into tokens on white space, splitting and joining and normalising to ASCII.</li>
<li>N-Gram Fingerprint. This is similar but uses n-grams (a sequence of characters or multiple sequences of characters) that are chunked, sorted and rejoined and normalised to ASCII text. The documentation highlights that this can produce more false positives but is food for finding clusters missed by fingerprinting.</li>
<li>Phonetic Fingerprint. This transforms tokens into the way they are pronounced and produces different fingerprints to methods 2 &amp; 3.</li>
<li>Nearest Neighbour Methods. This is a distance method but can be very very slow.</li>
<li>Levenshtein Distance. This famous algorithm measures the minimal number of edits that are required to change one string into another (and for this reason is widely known as edit distance). Typically, this will spot typological and spelling errors not spotted by the earlier approaches.</li>
<li>PPM a particular use of Kolmogorov complexity as described in this <a href="http://arxiv.org/abs/cs/0111054">article</a> that is implemented in Open Refine as <code>Prediction by Partial Matching</code>.</li>
</ol>
<p>It is important to gain an insight into these methods because they may affect the results you receive. In particular, caution is required on East Asian names where cultural naming traditions produce a lot of false positive matches on the same name for persons who are actually distinct persons (synonyms or “lumping” in the literature). This can have very dramatic impacts on the results of patent analysis for inventor names because it will treat all persons sharing the name <code>Smith, John</code> or <code>Wang, Wei</code> as the same person, when in practice they are multiple individual people.</p>
<p>We will now walk through each algorithm to view the results.</p>
<p><img src="images/openrefine/cluster_step1.png" /></p>
<p>This identifies 1187 clusters dominated by hidden characters in the applicants field (typically appearing after the name). At this stage we need to make some decisions about whether or not to accept or reject the proposed merger by checking the <code>Merge?</code> boxes.</p>
<p>This step was particularly good at producing a match on variant names and name reversals as we can see here.</p>
<p><img src="images/openrefine/applicants2.png" /></p>
<p>It pays to manually inspect the data before accepting it. One important option here is to use the slider on <code>Choices in Cluster</code> to move the range up or down and then make a decision about the appropriate cut off point. Then use select all in the bottom left for results you are happy with followed by <code>Merge Selected &amp; Re-Cluster</code>. In the next step we can change the keying function dropdown to Ngram-fingerprint.</p>
<p><img src="images/openrefine/applicants-ngramfingerprint2.png" /></p>
<p>This produces 98 clusters which inspection suggests are very accurate. The issues to watch out for here (and throughout) are very similar names for companies that may not be the same company (such as Ltd. and Inc.) or distinct divisions of the same company. It is also important, when working with inventor names, not to assume that the same name is the same inventor in the absence of other match criteria, or that apparently minor variations in initials (e.g. Smith, John A and Smith, John B) are the same person because they may well not be.</p>
<p>To see these potential problems in action try reducing the N-gram size to 1. At this point we see the following.</p>
<p><img src="images/openrefine/ngram1_problems.png" /></p>
<p>This measure is too lax and is grouping together companies that should not be grouped. In contrast increasing the N-gram value to 3 or 4 will tighten the cluster. We will select all on N-gram 2 and proceed to the next step.</p>
<p>At this point it is worth noting that the original 9,368 clusters have been reduced to 7,875 and if we sort on the count in the main window then Google is starting to emerge as the top applicant across our set of 10,000 records.</p>
<p>###Phonetic Fingerprint (Metaphone 3) clustering</p>
<p>As we can see below, Metaphone 3 clustering produces 413 looser clusters with false positive matches on International Business Machines but positive matches on Cooperative Verkoop.</p>
<p><img src="images/openrefine/applicants_metaphone3.png" /></p>
<p>At this point we could either manually review the 413 clusters and select as appropriate or change the settings to reduce the number of clusters using the <code>Choices in Clusters</code> slider until we see something manageable for manual review. At this stage we might also want to use the <code>browse this cluster</code> function that appears on hovering over a particular selection, to review the data (see the second entry in the image below).</p>
<p><img src="images/openrefine/applicants_browsecluster.png" /></p>
<p>In this case we are attempting to ascertain whether the Korea Institute of Oriental Medicine should be grouped with the Korea Institute of Science and Technology (which appears unlikely). If we open the browser function we can review the entries for shared characteristics as possible match criteria.</p>
<p><img src="images/openrefine/applicants_browsecluster1.png" /></p>
<p>For example, if the applicants shared inventors and/or the same title we may want to record this record in the larger grouping (remembering that we have exported the original cleaned up data). Or, as is more likely, we might want to capture most members of the group and remove the Korea Institute of Oriental Medicine. However, how to do this is not at all obvious.</p>
<p>In practice, selection of items at this stage feeds into the next stage of cleaning using the Cologne-phonetic algorithm. As we can see below, this algorithm identified 271 clusters that were almost entirely clustered on the names of individuals with a limited number of accurate hits.</p>
<p><img src="images/openrefine/applicants_cologne.png" /></p>
<p>###Levenshtein Edit Distance</p>
<p>The final steps in the process focus on Nearest Neighbour matches for our reduced number of clusters. Note that this may take some time to run (e.g. 10-15 minutes for the +7,000 clusters in this case). The results are displayed below</p>
<p><img src="images/openrefine/applicants_levenshtein.png" /></p>
<p>In some cases the default settings matched individual names on different initials, but in the majority of cases the clusters appeared valid and were accepted. In this case particular caution is required on the names of individuals and browsing the results to check the accuracy of matches.</p>
<p>###PPM</p>
<p>The PPM step is the final logarithm but took so long that we decided to abandon it relative to the likely gains.</p>
<p>###Preparing for export</p>
<p>In practice, the cleaning process will generate a new data table for export that focuses on the characteristics of applicants. To prepare for export with the cleaned group of applicant names there will be a choice on whether to retain the publication number as the single key, or, whether to use the fill down process illustrated above across the columns of the dataset. It is important to note that caution is required in the blanket use of fill down.</p>
<p>It is also important to bear in mind that the dataset that has been created contains many more rows than the original version. Prior to exporting we would therefore suggest two steps:</p>
<ol style="list-style-type: decimal">
<li>Rerun Common transforms &gt; to title case, to regularise any names that may have been omitted in the first round and rerun trim whitespace for any spaces arising from the splitting of names.</li>
<li>Rerun facets on each column, select blank at the end of the facet panel and fill with NA. Alternatively, do this immediately following export.</li>
</ol>
<p>##Round Up</p>
<p>In this chapter we have covered the main features of basic data cleaning using Open Refine. As should now be clear, while it requires an investment in familiarisation it is a powerful tool for cleaning up small to medium sized patent datasets such as our 10,000 pizza patent records. However, a degree of patience, caution and forward planning is required to create an effective workflow using this tool. It is likely that further investments of time (such as the use of regular expressions in GREL) would improve cleaning tasks prior to analysis.</p>
<p>Open Refine is also probably the easiest to use free tool for separating and cleaning applicant and inventor names without programming knowledge. For that reason alone, while noting the caveats highlighted above, Open Refine is a very valuable tool in the open source patent analytics toolbox.</p>
<p>##Useful Resources</p>
<p>The <a href="http://openrefine.org">Open Refine website</a> has links to lots of useful resources including video walkthroughs</p>
<p><a href="https://github.com/OpenRefine/OpenRefine/wiki/Recipes">Open Refine Wiki Recipes</a></p>
<p><a href="http://googlerefine.blogspot.co.uk">Open Refine Tips and Tricks</a></p>
<p><a href="http://stackoverflow.com/questions/tagged/openrefine">Stack Overflow questions on Open Refine</a></p>

</div>
</div>



<p>##Introduction</p>
<p>In this chapter we will be analysing and visualising patent data using Tableau Public.</p>
<p>Tableau Public is a free version of Tableau Desktop and provides a very good practical introduction to the use of patent data for analysis and visualisation. In many cases Tableau Public will represent the standard that other open source and free tools will need to meet.</p>
<p>This is a practical demonstration of the use of Tableau in patent analytics. We have created a set of cleaned patent data tables on <code>pizza patents</code> using a sample of 10,000 records from WIPO Patentscope that you can download as a .zip file from <a href="https://github.com/poldham/opensource-patent-analytics/blob/master/2_datasets/pizza_medium_clean/pizza_medium_clean.zip?raw=true">here</a> to use during the walkthrough. Details of the cleaning process to reach this stage are provided in the codebook that can be viewed <a href="https://github.com/poldham/opensource-patent-analytics/blob/master/2_datasets/pizza_medium_clean/pizza_medium_code_book_15052105.txt">here</a>. The <a href="http://poldham.github.io/openrefine-patent-cleaning/">Open Refine walkthrough</a> can be used to generate cleaned files very similar to those used in this walkthrough using your own data. You will not need to clean any data using our training set files.</p>
<p>This article will take you through the main features of Tableau Public and the types of analysis and visualisation that can be performed using Tableau. In the process you will be creating something very similar to this <a href="https://public.tableau.com/profile/wipo.open.source.patent.analytics.manual#!/vizhome/pizzapatents/Overview">workbook</a>.</p>
<p>##Installing Tableau</p>
<p>Tableau can be installed for your operating system by visiting the <a href="https://public.tableau.com/s/">Tableau Public website</a> and entering your email address as in the image below.</p>
<p><img src="images/tableau/providemail.png" /></p>
<p>While you are waiting for the app to download it is a good idea to select <code>Sign In</code> and then <code>Create one now for Free</code> to sign up for a Tableau Public Account that will allow you to load up your workbooks to the web and share them. We will deal with privacy issues in making workbooks public or private below but as its name suggests Tableau Public is not for sensitive commercial information.</p>
<p><img src="images/tableau/signup.png" /></p>
<p>This will lead you to an empty profile page.</p>
<p><img src="images/tableau/profile.png" /></p>
<p>While you are there you might want to check out the <a href="https://public.tableau.com/s/gallery">Gallery</a> of other Tableau Public workbooks to get some ideas on what it is possible to achieve with Tableau. You may want to view a <a href="https://public.tableau.com/profile/poldham#!/">Tableau Workbook</a> for scientific literature that accompanied this <a href="http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0034368">PLOS ONE article on synthetic biology</a>. While it is now a few years old it gives an idea of the possibilities of Tableau and the feel of an existing profile page.</p>
<p><img src="images/tableau/gallery.png" /></p>
<p>##Getting Started</p>
<p>When you first open the application you will see a blank page. Before we load some data, note the helpful <code>How-to-Videos</code> on the right and the link to a <code>visualisation of the day</code>. There are also quite a lot of training videos <a href="http://www.tableau.com/learn/training">here</a> and a very useful <a href="http://community.tableau.com/community/forums">community forum</a>. If you get stuck, or wonder how somebody produced a cool visualisation, this is the place to go.</p>
<p><img src="images/tableau/open.png" /></p>
<p>To avoid staring at a blank page we now need to load some data. In Tableau Public this is limited to text or Excel files. To download the data as a single <code>.zip</code> file click <a href="https://github.com/poldham/opensource-patent-analytics/blob/master/2_datasets/pizza_medium_clean/pizza_medium_clean.zip?raw=true">here</a> or visit the <a href="https://github.com/poldham/opensource-patent-analytics/blob/master/2_datasets/pizza_medium_clean/pizza_medium_clean.zip">GitHub repository</a>. unzip the file and you will see a collection of <code>.csv</code> files. The excel file and codebook should be ignored as supplementary.</p>
<p><img src="images/tableau/github.png" /></p>
<p>As we can see above there are a number of files in this dataset. The <code>core</code> or reference file is <code>pizza.csv</code>. All other files are aspects of that file, such as applicants, inventors and international patent classification codes. That is concatenated fields in pizza have been separated out and cleaned up. One file, <code>applicants_ipc</code> is a child file of <code>applicants</code> that will allow us to access IPC information for individual applicants. This may not make a lot of sense at the moment but don’t worry it will shortly.</p>
<p>To get started we will select the <code>pizza.csv</code> file:</p>
<p><img src="images/tableau/load_file.png" /></p>
<p>We will then see a new screen showing some of the data and the other files in the folder. At the bottom is a flag with <code>Go to Worksheet</code>, so let’s do that.</p>
<p><img src="images/tableau/pizza1.png" /></p>
<p>We will now see a screen that is divided in to <code>Dimensions</code> on the left, with <code>Measures</code> below. We can see that in the dimensions there are quite a large number of data fields. Note that Tableau will attempt to guess the type of data (for example numeric or date information is marked with <code>#</code>, geographic data is marked with a globe, text fields are marked with <code>Abc</code>). Note that Tableau does not always get this right and that it is possible to change a data type by selecting a field and right clicking as we can see below.</p>
<p><img src="images/tableau/tableau_fields.png" /></p>
<p>On the right hand side we can see a floating panel menu. This can be hidden as a menu bar by clicking the x. This panel displays the visualisation options that are available for the data field that we have selected. In this case two map options are available because Tableau has automatically recognised the country names as geographic information. Note that persuading Tableau to present the option that you want (for example visualising year on year data as a line graph) can involve changing the settings for the field until the option you want becomes available.</p>
<p>At the bottom of the screen we will see a worksheet number <code>Sheet 1</code> and then options for adding three types of sheet:</p>
<ol style="list-style-type: decimal">
<li>A New Worksheet</li>
<li>A New Dashboard</li>
<li>A New Story</li>
</ol>
<p>For the moment we will focus on building worksheets with the data and then move into creating Dashboards and then Stories around our pizza data.</p>
<p>##Publication Trends</p>
<p>One of the first things we normally want to do with patent data is to map trends, either in first filings, publications or family members. In the case of our pizza patents from Patentscope we have a single member of a dossier of files linked to a particular application. This data is fine for demonstration needs and we can easily map trends for this data.</p>
<p>To do that we simply drag the publication year in the dimensions to the columns field and the number of records from the measures field. Note that Tableau automatically counts the number of rows in a set to create this field. If working with data where accurate counts are important it is important to make sure that the data has been deduplicated on the relevant field before starting. While it does not apply in this case, another important tip is to always have a way of checking key counts in Tableau such as using quick pivot tables in Excel or Open Office. We do not need to worry about this now, but while Tableau is clever software it is still software: it will not always perform calculations as you expect them. For that reason a cross check of counts is a sensible if not vital part of a Tableau workflow.</p>
<p>Tableau will guess what we are after and draw a graph.</p>
<p><img src="images/tableau/publication_trend.png" /></p>
<p>As we can see we now have a graph that plunges off a cliff as we approach the present and contains one null. Null values are typically rows or columns containing blank cells. If there is only 1 null value then the data can probably be left as is (in this case it was a blank row at the bottom of the dataset introduced during cleaning in R). However, it pays to inspect nulls by right clicking on the file in <code>Data</code> and selecting <code>View data</code>. If there are large numbers of nulls then you may need to go back and inspect the data and ensure that blank cells are filled with <code>NA</code> values. Let’s go back to our graph.</p>
<p>What we see here is the <code>data cliff</code> that is common with patent data. That is, the cliff does not represent a radical decline in the use of the term <code>pizza</code>, it represents a radical decline in the availability of patent data the closer we move to the present day. The reason for this is that it generally, as a rough rule of thumb, takes about 24 months for an application to be published and can take longer for patent databases to catch up. As such, our <code>data cliff</code> reflects a lack of available data in recent years, not a lack of activity. Typically we need to pull back about 2 to 3 years to gain an impression of the trend.</p>
<p>Before we go any further and adjust the axis we will change the graph to something more attractive. To do that we will select filled graph in the floating panel. Behind that panel is a small colour button that will allow us to select a colour we like. The reason that we do this before adjusting the axis is that when we change the graphic type Tableau will revert any changes made to the axis.</p>
<p>Next we right click the x (lower) axis and adjust the time frame to something more sensible such as 1980 to 2013 by selecting the <code>fixed</code> option. As a very rough rule of thumb moving back two or three years from the present will take out the data cliff from the lack of published patent information. Note that if we were counting first filings (patent families) the decline would be earlier and much steeper. These lag effects, and ways to deal with them, have been investigated in detail by the <a href="http://www.oecd.org/sti/inno/intellectual-property-statistics-and-analysis.htm">OECD patent statistics team</a>, see in particular work on <a href="http://www.oecd.org/science/inno/39485567.pdf">nowcasting patent data</a>.</p>
<p>We now have a good looking graph with a sensible axis. Note here that if we were graphing multiple trends on the same graph (family and family members) we might prefer a straightforward line graph for the sake of clarity.</p>
<p><img src="images/tableau/publication_trend_fill.png" /></p>
<p>We will give this a name <code>Trends</code> and add a new worksheet by clicking the icon next to our existing sheet.</p>
<p>The next piece of information we would like is who the most active applicants are. This will also start to expose issues about the different actors who use the term <code>pizza</code> in the patent system and encourage us to think about ways to drill down into the data to get more accurate information on technologies we might be interested, such as, in this case, pizza boxes and <a href="http://www.google.co.uk/patents/US8720690">musical pizza boxes</a> in particular.</p>
<p>It is at this point that the work we did in a previous article on separating individual applicant names into their own rows and cleaning them up using Open Refine, becomes important. In this dataset we have taken this a step further using VantagePoint to separate out individuals from organisations. This information is found in the <code>Applicants Organisations</code> field in the dataset. Lets just drop that onto the worksheet as a row and then add the number of records as a column (tip, simply drop it onto the sheet).</p>
<p>At first sight everything seems pretty good. But now we need to rank our applicants. To do that we select the small icon in the menu bar with a stacked bar pointing down.</p>
<p><img src="images/tableau/sort_applicants.png" /></p>
<p>We now see, as we would in the Excel raw file, that there are a significant number of blank entries for applicants in the underlying data, followed by 85 records for Google and 77 for Microsoft. This is also a very good indicator that there may be multiple uses of the word pizza in the patent system unless these software companies have started selling pizzas online.</p>
<p>In reality this is <strong><em>a partial view of activity</em></strong> by the applicants because elsewhere in the data the names are concatenated together. This is normally more obvious than in the present dataset through the presence of multiple names separated by <code>;</code>(to see this scroll down to the first entry for Unilever).</p>
<p><img src="images/tableau/applicants_original.png" /></p>
<p>To understand why this is a partial view we will now import the <code>applicants.csv</code> file. The correct way to do this is to select the menu called <code>Data</code> then <code>New Data Source</code> and the file <code>applicants.csv</code>.</p>
<p>Next, drag <code>Applicants Orgs All</code> onto the Rows. Note that Tableau is interpreting these titles for us (the original is <code>applicants_orgs_all</code>). Then drag <code>Number of Records</code> from the dimensions onto the sheet or into the columns entry. Now choose the stacked bar icon as above to rank the applicants by the number of records. We will now see the following.</p>
<p><img src="images/tableau/applicants_organisations.png" /></p>
<p>Note the difference between the original applicants field (where Google scored a total of 85 records) and our separated and cleaned field where Google now scores 191 records. In short, before the separation and cleaning exercises we were only seeing 44% of activity in our dataset by Google involving the term pizza. This still does not mean that they have entered the online pizza business… . What it does tell us is that patent analysis that does not separate or split the concatenated data and clean up name variants is missing over 60% of the story when viewed in terms of applicant activity. As this makes clear, the gains from separating or splitting and cleaning data are huge even where, as in this case, the original data appeared to be quite ‘clean’. That appearance was deceptive.</p>
<p>Now we have a clearer view of what is happening with our applicants we can make this more attractive. To do that first select the blue bar in the floating panel. The worksheet will now be presented as ranked bars. Next, drag the number of records from Measures onto the <code>Label</code> button next to <code>Color</code>. That looks pretty good. If we wanted to go a step further we could now turn to the dimensions panel and drag <code>Applicants Orgs All</code>, onto the <code>Color</code> button. The bars will now turn to different colours for each applicant. If this is too bright simply grab the green <code>Applicants Orgs All</code> box from under the buttons menu and move it towards dimensions to remove it. Finally, if we want to adjust the right alignment of the text to the left, then first right click on the name of a company, pick <code>Format</code> then alignment and left. While the default is right align, in practice left align creates more readable labels. To change the default do this with the first worksheet you create before creating any others.</p>
<p>We now have an applicants data table that looks, depending on your aesthetic sensibilities, like this.</p>
<p><img src="images/tableau/applicants_cleaned.png" /></p>
<p>At this stage we might want to take a couple of actions. To make the labels more visible, drag the line between the names and the columns to the right. This will open up some space. Next, think about editing long names down to something short. For example, International Business Machines Corporation, who are also not famous for pizzas, is a little bit too long. Right click on the name and select <code>Edit alias</code> as in the image below.</p>
<p><img src="images/tableau/edit_alias.png" /></p>
<p>Now edit the name to IBM. As a tip note that where you discover you have missed a duplicate name in clean up (remember that we focus on good enough rather than perfect in data cleaning) it is also possible to highlight two rows, right click, look for a filing clip icon and group two entries onto a new name. However, the resulting named group must be used in all later analysis. It is also important to realise that data cleaning is not a Tableau strength, Tableau is about data analysis and exploration through visualisation. For data cleaning use a tool such as Open Refine.</p>
<p>##Adding New Data Sources</p>
<p>We will follow the same procedure that we used for applicants to add the remaining files as data sources. We will add the following four files (as they appear in the folder in alphabetical order).</p>
<ol style="list-style-type: decimal">
<li>applicants_ipc.csv</li>
<li>inventors.csv</li>
<li>ipc_class.csv</li>
<li>ipc_subclass.detail.csv</li>
</ol>
<p>To add the data sources either click the <code>Data</code> menu and <code>New Data Source</code> or (faster) the cylinder with a plus sign. Then select <code>Text file</code>, add each file and allow it to load.</p>
<p><img src="images/tableau/add_data.png" /></p>
<p>If all goes well the <code>Data</code> panel will now contain the following files.</p>
<p><img src="images/tableau/data_panel.png" /></p>
<p>Note here that the <code>applicants</code> data displays a blue tick. This is because it was the last data source that we used and is therefore active. The fields we see in Dimensions belong to that data source. Next click in the bottom menu to create a new worksheet and then click <code>inventors</code> in the <code>Data</code> field. The field names will now change slightly. It is important to keep an eye on the data source that you are using because it is quite easy to drop a field from one data source onto another. In some cases this is a good thing. But, if you receive a warning message you will be attempting to drop a data source on to another data source where there is no matching field. We will come back to this on data <code>blending</code>.</p>
<p>Next follow the same procedure for ranking applicants with inventors using the <code>Inventors All</code>. For anyone interested in seeing the dramatic impacts of concatenated fields try dropping the <code>Inventors Original</code> field onto the worksheet.</p>
<p>Using <code>Inventors All</code> you should now see the following ranked list of inventors.</p>
<p><img src="images/tableau/inventors_ranked.png" /></p>
<p>Now repeat this exercise for the remaining data sources by first creating a sheet and then selecting the data source. As you move through this select the following dimensions to add to the sheet and then drop number of</p>
<ol style="list-style-type: decimal">
<li>applicants_ipc. Drop <code>Ipc Subclass Detail</code> onto the sheet. Then drop number of records onto the sheet where the field says Abc. Note that a number <code>6</code> will appear in the first row. This is an artifact from the separation process. Select that cell, right click and then choose <code>Exclude</code>.</li>
</ol>
<p>Do not rank this data, but instead drag the field <code>Applicants Orgs All</code> onto the sheet so that it is the first row (tip, it is easiest to do this by dragging the field into the row bar before the IPC field). You will now see a list of company names followed by a list of IPCs. Congratulations, we now have an idea of who is patenting in a particular area of technology using the word pizza at the level of individual applicants.</p>
<p>Add a new sheet. Then click on <code>ipc_subclass_detail</code>. Note that if you click on the data source first, the dimensions panel will go orange. Don’t panic. The reason is that Tableau thinks you are trying to blend data from the ipc_subclass_detail source with applicants_ipc. If you do this simply click on ipc_subclass-detail again.</p>
<ol start="2" style="list-style-type: decimal">
<li>ipc_subclass-detail. Drop the <code>Ipc Subclass Detail</code> dimension on to the sheet. Then drop the number of records onto the sheet. Then click on the first cell containing <code>6</code> as an artifact and exclude. Repeat for <code>7</code>. Then select the bar chart in the floating <code>Show Me</code> panel, then drag <code>Number of Records</code> onto the <code>Label</code> button. Now rank the column using the descending button in the upper menu as before.</li>
</ol>
<p>At this point, if we had not trimmed the leading white space the ranked list would display indentations and there would be duplicates of the same IPC code. For that reason it is important to trim leading white space before attempting to visualise data (and this applies to all our separated fields).</p>
<p>##Creating an Overview Dashboard</p>
<p>You should now have five worksheets each of which displays aspects of our core <code>pizza</code> set. We have named the sheets as follows and suggest that you might want to do the same. Note that where there is more than one sheet containing similar but distinct information it will be helpful to give them distinct names (e.g. IPC Subclass and Applicants IPC Subclasses). We might even start using less technical labels by calling the IPC something clearer like Technology Area, to aid communication with non-IP specialists</p>
<p><img src="images/tableau/sheet_names.png" /></p>
<p>Let’s get a quick overview of the data so far. Next to the add worksheet button in the worksheets bar is a second icon to create a dashboard. Click on that and we will now see a sheet called <code>Dashboard 1</code>.</p>
<p>Dashboards are perhaps Tableau’s best known feature and are rightly very popular. We can fill our dashboard by dragging the worksheets from the <code>Dashboard</code> side menu. The order in which we do this can make life easier or more difficult to adjust later. Let’s do it in the following steps</p>
<ol style="list-style-type: decimal">
<li>Drag <code>Trends</code> onto the dashboard and it will now fill the view.</li>
<li>Drag <code>Organisations</code> onto the dashboard.</li>
</ol>
<p><img src="images/tableau/dashboard1.png" /></p>
<p>That is rather messy, but all is not lost. Simply click in the top right corner of the organisations panel on the right to remove it (in the original worksheet click on it and select <code>Hide</code>). We now have an <code>Organisations</code> column that still looks crunched.</p>
<p>Now select the top of the organisations box and a small inverted triangle will appear. Click on that and then choose <code>Fit &gt; Fit Width</code>.</p>
<p><img src="images/tableau/dashboard_fitwidth.png" /></p>
<p>The bars may now disappear. Click into the box on the line where the bars start and drag them back into view. At this point long names may start to be obscured. If desired, right click on a long name such as <code>Graphic Packaging International</code>, choose <code>Edit alias</code> and edit it down to something sensible such as <code>Graphic Packaging Int</code>.</p>
<p>We now have two panels on the dashboard. Let’s add two more. First drag technology areas below the line where Trends and Organisations finish. Grey shaded boxes will appear that show the placement, across the width is fine. This can take some time to get right, when the whole of the bottom area is highlighted let go of the mouse. If it goes somewhere strange either select the box and in the top right press <code>x</code> to remove it, or try moving it (in our experience it is often easier to remove it and try again).
Do not try to format this box yet. Instead, grab inventors and drag it into the space before the technology areas below.</p>
<p>We now have four panels in the dashboard but they need some tidying up. First, in the two boxes we have just edited repeat the <code>Fit Width</code> exercise and then drag the line for the bars around until they are in view and satisfactory. Next, we have names such as <code>Applicants Orgs All</code> that are our internal reference names. Click on them in each of the three panels one at a time and select <code>Hide Field Labels for Rows</code>.</p>
<p>Hmm… our Technology Areas panel is proving troublesome because even the edited version of the IPC is rather long.</p>
<p>Before we do any editing, first experiment with the <code>Size</code> menu in the bottom right. The default dashboard size in Tableau Public is actually quite small. Change the settings until you have something that looks cleaner even if there are still some overlaps. Options such as <code>Desktop</code>, <code>Laptop</code> and <code>Large blog</code> are generally decent sizes but in part the decision depends on where you believe it will be displayed.</p>
<p>To fix the long Technology areas labels we go back to the original sheet (tip: if you move the mouse to the top right in the panel an arrow with <code>Go to Sheet</code> will appear, it is very useful for large workbooks). Inside the original sheet, try dragging the line separating the text and bars so that the bars now cover some of the longer text. Then switch back to the dashboard. If you feel unhappy with the result then right click in the panel in the dashboard and then choose <code>Edit alias</code>. This is useful for simply making labels in the view more visible (it does not change the original data).</p>
<p>If all goes well you will now have a dashboard that looks more or less like this. Note that depending on the worksheet settings you may want to make the font size consistent (right click and choose Format, then font size). Note also that if you increase the font size (the default is 8 point) then you may need to edit some of the labels again.</p>
<p><img src="images/tableau/dashboard_completed.png" /></p>
<p>We have now done quite a lot of work and produced an Overview dashboard. It is time to save the workbook to the server before doing anything else.</p>
<p>##Saving, Display and Privacy Settings</p>
<p>The only option for saving a Tableau Public workbook is to save it online. To save the file go to <code>File</code> and Save to Tableau Public. If you want to save the workbook as a new file (after previously saving) then choose Save As.</p>
<p><img src="images/tableau/save_public.png" /></p>
<p>You will then be asked to enter your username and password (Tableau does not remember the password) and the file will upload. Tableau will then compress the data. As of June 2015 it is possible to store 10GB of data overall and to have up to 10 million rows in a workbook (which is generally more than enough).</p>
<p>Tableau will then open a web browser at your profile page and it will look a lot like this.</p>
<p><img src="images/tableau/profile_dashboard.png" /></p>
<p>Having read the message, click <code>Got it</code> on the right. Do you notice anything strange. Yes, we can only see the Dashboard and not any of the other sheets. To change this and any other details click on <code>edit details</code> near the title and some menus will open up as follows.</p>
<p><img src="images/tableau/profile_viewtabs3.png" /></p>
<p>To make sure the worksheets are visible select the check box marked <code>Show workbook sheets as tabs</code> and then <code>Save</code>.
<img src="images/tableau/profile_viewtabs3.png" /></p>
<p>To access this demonstration workbook go <a href="https://public.tableau.com/profile/wipo.open.source.patent.analytics.manual#!/vizhome/pizzapatents/Overview">here</a>.</p>
<p>##Privacy and Security</p>
<p>As emphasised above, Tableau Public is by definition a place for publicly sharing workbooks and visualisations. It is not for sensitive data. In the past users, such as journalists, relied on what might be called ‘security by obscurity’ but the trend towards storing data on a Tableau public profile (the only option) makes that less of an option. If this a concern there are two actions that might potentially be considered that limit the visibility of a workbook and its wider use. Logically, the answer to any concerns about Tableau Public and sensitive information is <strong><em>not to include sensitive information in the first place</em></strong>. The following are not recommendations but simply highlight the available options.</p>
<ol style="list-style-type: decimal">
<li>In the discussion on the settings above, there is a check box to prevent users from downloading a workbook. You might want to select that option where a workbook contains information that you do not want to be seen other than what you choose to make visible.</li>
<li>It is possible to create a setting so that a workbook does not show up on a user’s profile. This is hard to spot, and appears by hovering over the workbook in the Profile view.</li>
</ol>
<p><img src="images/tableau/profile_visibility.png" /></p>
<p>As the message points out using this option does not prevent a workbook being found through search engines or seen by users. It just means it is not visible on the profile page.</p>
<p>As such, Tableau public is fundamentally about sharing information with others through visualisation. That is, it is for information that you want others to see. Here it is briefly worth returning to the completed dashboard above and clicking the share button.</p>
<p><img src="images/tableau/profile_sharing.png" /></p>
<p>As we can see here, Tableau generates embed codes for use on websites or for emailing as a link along with twitter and facebook.</p>
<p>##Round Up</p>
<p>In this chapter we have introduced the visualisation of patent data using a set of nearly 10,000 patent documents from WIPO Patentscope that mention pizza. As should by now be clear Tableau Public is a very powerful free tool for data visualisation. It requires attention to detail and care in construction but is one of the best free tools that is out there for visualisation and dashboarding.</p>
<p>To take working with Tableau on pizza patents forward on your own here are some tips.</p>
<ol style="list-style-type: decimal">
<li>You already know how to use Tableau to create a map of publication countries.</li>
<li>The pizza source file contains a set of publication numbers. Try a) creating a visualisation with the publication numbers, b) looking in the pizza source file for a set of URL and then exploring what can be done with <code>Worksheet &gt; Action</code> with that URL.</li>
<li>In dashboards consider using one field as a filter for another field (such as applicant and title). What data source or data sources would you need to do that?</li>
<li>What kinds of stories does the pizza data tell us and how might we visualise them using the information provided on applicants and its subset Applicants IPCs?</li>
</ol>
<p>If you get stuck, and it does take time to become familiar with Tableau’s potential, perhaps try exploring this <a href="https://public.tableau.com/profile/poldham#!/">workbook on synthetic biology</a> and the use of Tableau images in this article <a href="http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0034368">PLOS ONE article</a>. As a tip, try clicking on the bars and then the titles to understand Actions. Downloading workbooks prepared by others can be a very good way of learning the tips and tricks of tableau visualisation and dashboarding.</p>
<p>If you would like to download the pizza workbook it is <a href="https://public.tableau.com/profile/wipo.open.source.patent.analytics.manual#!/vizhome/pizzapatents/Overview">here</a>.</p>
<p>However, one of the most important issues exposed by working with Tableau is that you must ensure that fields you want to visualise are <code>tidy</code>, that is not concatenated, and also that they are as clean as it is reasonable to make them. For researchers wishing to work up their own data we suggest the Open Refine article as a good starting point.</p>

<p>#Gephi</p>
<!---update dataset links--->
<p>This chapter focuses on visualising patent data in networks using the open source software <a href="http://gephi.github.io">Gephi</a>. Gephi is one of a growing number of free network analysis and visualisation tools with others including <a href="http://www.cytoscape.org">Cytoscape</a>, <a href="http://tulip.labri.fr/TulipDrupal/">Tulip</a>, <a href="http://www.graphviz.org">GraphViz</a>, <a href="http://mrvar.fdv.uni-lj.si/pajek/">Pajek</a> for Windows, and <a href="http://www.vosviewer.com/Home">VOSviewer</a> to name but a few. In addition, network visualisation packages are available for R and Python. We have chosen to focus on Gephi because it is a good all round network visualisation tool that is quite easy to use and to learn.</p>
<p><img src="images/gephi9/gephi_front.png" /></p>
<p>In this chapter we will focus on creating a simple network visualisation of the relationship between patent applicants (assignees). However, network visualisation can be used to visualise a range of fields and relationships, such as inventors, key words, IPC and CPC codes, and citations among other options.</p>
<p>For this chapter we will use a dataset on drones from the <a href="https://www.lens.org/lens/search?q=%22drone%22+OR+%22drones%22&amp;predicate=%26%26&amp;l=en">Lens patent database</a>. The dataset consists of 5884 patent documents containing the terms “drone or drones” in the full text deduplicated to individual families from the full publication set. The dataset has been extensively cleaned in Vantage Point by separating out applicant and inventor names and then using fuzzy logic matching to clean up names. Very very similar results can be achieved using Open Refine as described in Chapter 9 of this Manual.</p>
<p>The dataset can be downloaded from Github in a zip file to unzip <a href="https://github.com/wipo-analytics/drones_data/raw/master/use_me/gephi/gephi_drones_fulltext_cleaned_5884.csv.zip">here</a>.</p>
<p>##Installing Gephi</p>
<p>You should install gephi 9.1 (the latest release) rather than an earlier version. Note that any later updates may not contain the key functionality that is needed below (as it takes a while for some of the plugins and features to catch up).</p>
<p>To install for your operating system follow these <a href="https://gephi.org/users/download/">instructions</a></p>
<p>After you have finished this chapter you may want to follow the <a href="https://gephi.github.io/tutorials/gephi-tutorial-quick_start.pdf">Quick start guide</a> although we will cover those topics in the article. The <a href="http://gephi.github.io/users/">Learn section</a> of the website provides additional tutorials.</p>
<p>##Opening Gephi and Installing Plugins</p>
<p>When you have installed Gephi, open it and you should see the following welcome screen.</p>
<p><img src="images/gephi9/welcome.png" /></p>
<p>Before we do anything else, we need to install a plugin developed by <a href="http://www.em-lyon.com/en/faculty-research-education/faculty-research/international-business-school-professors/Permanent-Professors/Clement-LEVALLOIS">Clement Levallois</a> to convert Excel and csv files into gephi network files. To install the plugin select the <code>Tools</code> menu in the menu bar and then <code>Plugins</code>.</p>
<p><img src="images/gephi9/plugin.png" /></p>
<p>You will see a pop up menu for the plugins. At this point you may want to press <code>Reload Catalog</code> to make sure everything is loaded. Then head to <code>Available Plugins</code>. Click on <code>name</code> to sort them alphabetically. You now want to look for a plugin called <code>Convert Excel and csv files to networks</code>. Select the check box, press <code>Install</code> and follow through the menus. Just keep pressing at the prompts and then you will need to restart at the end.</p>
<p><img src="images/gephi9/install.png" /></p>
<p>You will need to restart Gephi for it to take effect but if you return to the Plugins menu and then choose the installed tab you should see this.</p>
<p><img src="images/gephi9/result.png" /></p>
<p>You are good to go. While you are there you may want to check out the other plugins to get an idea of what is available. For more on the conversion plugin see this description <a href="https://marketplace.gephi.org/plugin/excel-csv-converter-to-network/">Excel/csv converter to network plugin</a>.</p>
<p>##Importing a file to Gephi with the converter plugin</p>
<p>We will concentrate on using the <code>drones</code> patent dataset in the zipped .csv version <a href="https://github.com/wipo-analytics/drones_data/raw/master/use_me/gephi/gephi_drones_fulltext_cleaned_5884.csv.zip">here</a> and don’t forget to unzip the file. While Gephi works with .csv files, the import plugin includes a timeline option that only works with Excel. For that reason we will use the Excel version.</p>
<p>###Step 1. Open Gephi and Choose File &gt; Import</p>
<p>For this to work we need to use the <code>Import</code> function under the File menu.</p>
<p><img src="images/gephi9/import.png" /></p>
<p>You should now see a menu like that below. Make sure that you choose the co-occurrence option.</p>
<p><img src="images/gephi9/wizard.png" /></p>
<p>Next you will be asked to select the file to use. We will download and then unzip the <a href="https://github.com/wipo-analytics/drones_data/raw/master/use_me/gephi/gephi_drones_fulltext_cleaned_5884.csv.zip">gephi_drones_fulltext_cleaned_5884.csv</a> file that is located on the WIPO Analytics website on Git Hub.</p>
<p><img src="images/gephi9/select.png" /></p>
<p>When you have chosen <code>Data importer(co-occurrences)</code> then choose <code>Next</code>. Make sure the column headers stays selected (unless using your own data). You will then need to choose a delimiter. In this case it is a comma but in other cases it may be a semicolon or a tab.</p>
<p><img src="images/figures_gephi/fig5_idelimiter.png" /></p>
<p>We now need to choose the agents, that is the actors or objects that we want to create a network map with. We will use <code>patent_assignees_cleaned</code> as this is a relatively small set. We will choose the same field in the two boxes because we are interested in co-occurrence analysis.</p>
<p><img src="images/gephi9/applicants.png" /></p>
<p>In the next step we need to specify the delimiter for splitting the contents of the <code>applicants_use_me</code> column. In all the fields it is a semicolon so let’s choose that. Note that if you are doing this with raw Lens data that you have not previously cleaned the Lens delimited is a double semi-colon (which is not helpful) and will need to be replaced prior to import.</p>
<p><img src="images/gephi9/delimiter.png" /></p>
<p>We will then be asked if we want a dynamic network. This presently only works with Excel files and even then it does not always work well. We will leave this blank as we are using a .csv file. Note that if we were using an Excel file the choices we would use would normally be the publication year or publication date or the priority year or date for patent data.</p>
<p><img src="images/gephi9/dynamic.png" /></p>
<p>The next menu provides us with a list of options. Unfortunately, with one exception, it is not entirely clear what the consequences of these choices are so experimentation may be needed.</p>
<p><img src="images/figures_gephi/fig8_options.png" /></p>
<p>Choice1. Create links between <code>applicants_use_me</code>.
Choice 2. Remove duplicates. We don’t need that as we know that they are unique.
Choice 3. Remove self-loops. Generally we do want this (otherwise known as removing the diagonal to prevent actors counting onto themselves - this will produce a large black hoop or handle for a self-loop in Gephi).</p>
<p>We will choose to create the links and to remove the self loops.</p>
<p>Next we will see a create network screen setting out our choices.</p>
<p><img src="images/gephi9/create.png" /></p>
<p>Press Finish</p>
<p>Next we will see an import screen.</p>
<p><img src="images/gephi9/warning.png" /></p>
<p>It is quite common to see warning messages on this screen.</p>
<p>In this case some of the applicants cells in the worksheet are blank because no data is available. When you see warning messages it is a good idea to check the underlying file to make sure that you understand the nature of the warning.</p>
<p>A second common warning with dynamic networks is that the year field is not correctly formatted. In that case, check that the format of the date/year field is what gephi is expecting in the underlying data. You can review the data in the data laboratory.</p>
<p>Note that the import screen also provides options on the type of graph. Normally for networks of authors, inventors and actors leave this as an undirected (unordered) network. Undirected is the basic default for patent data and scientific literature. We will also see the number of nodes (dots) and edges (connections). It is important to keep an eye on these values. If the nodes are much lower than you expect then it is useful to go back to your data and inspect for problems such as concatenation of cells and so on.</p>
<p>Click OK. You should now see a raw network that looks like this.</p>
<p><img src="images/gephi9/network.png" /></p>
<p>Note that we can see the number of Nodes and Edges in the top right. If we switch to the top left, we will see three tabs, for <code>Overview</code>, <code>Data Laboratory</code> and <code>Preview</code>. Choose <code>Data Laboratory</code>.</p>
<p>In the Data laboratory we can see the ID, Label, type of field and the frequency (the count of the number of times the name appears). Note that these fields are editable by clicking inside the entry and can also be grouped (for example where a variant of the same name has been missed during the name cleaning process in a tool such as Open Refine).</p>
<p><img src="images/gephi9/laboratory.png" /></p>
<p>In some cases you may have filled any blank cells in the dataset with NA (for Not Available). If this is the case NA will show up as a node on the network. You can address this type of issue in the Data Laboratory by right clicking on the NA value and then Delete. Note also that you can always exclude or combine nodes after you have laid out the network by editing in the Data Laboratory.</p>
<p>The second part of the Data Laboratory is the Edges under Data Table in the Data Laboratory. The edges table involves a source and a target, where the source is the source node and the target is another node where there is a link between the nodes. We can see the edges table sorted alphabetically (click the source heading to sort) where the value in weight is the number of shared records.</p>
<p><img src="images/gephi9/edges.png" /></p>
<p>Again, note that it is possible to export the edges set and import a set. Also note the menus at the bottom of the screen which allow column values to be copied over. This can be useful where the label value is not populated meaning that a name will not display on the node when the graph is laid out.</p>
<p>Most of the time we can simply proceed with laying out the network without paying much attention to the data laboratory. However, it is important to become familiar with the data laboratory to avoid unexpected problems or to spot corruption in the data.</p>
<p>##Sizing and Colouring Nodes</p>
<p>When we look at the <code>Overview</code> screen we have quite a wide range of options. We will start on the upper right with the Statistics panel.
<img src="images/gephi9/statistics.png" /></p>
<p>The <code>Run</code> buttons will calculate a range of statistics on the network. Probably the two most useful are:</p>
<ol style="list-style-type: decimal">
<li>Modularity Class. This algorithm iterates over the connections (edges) and allocates the nodes to communities or clusters based on the strength of the connections. This algorithm is explained in detail in this article <a href="http://arxiv.org/abs/0803.0476">Vincent D Blondel, Jean-Loup Guillaume, Renaud Lambiotte, Etienne Lefebvre, Fast unfolding of communities in large networks, in Journal of Statistical Mechanics: Theory and Experiment 2008 (10), P1000</a>. The ability to detect communities in networks based on the strength of connections is a powerful tool in patent analytics.</li>
<li>Network Diameter. This calculates two measures of <code>betweeness</code>, that is <code>betweeness centrality</code> (how often a node appears on the shortest path between nodes) and centrality(the average distance from a starting node to other nodes in the network). Network Diameter also calculates Eccentricity which is the distance between a given node and the farthest node from it in the network. For background on this see the Wikipedia entry and also <a href="http://www.inf.uni-konstanz.de/algo/publications/b-fabc-01.pdf">Ulrik Brandes, A Faster Algorithm for Betweenness Centrality, in Journal of Mathematical Sociology 25(2):163-177, (2001)</a></li>
</ol>
<p>Whereas Modularity Class identifies communities (particularly in large networks), centrality measures examine the position of a node in the graph relative to other nodes. This can be useful for identifying key actors in networks based on the nature of their connections with other actors (rather than simply the number of records).</p>
<p>If we run Modularity Class as in the figure a pop up message will inform us that there are 246 communities in the network. Given that there are only 362 nodes this suggests a weakly connected network made up of small individual clusters.</p>
<div id="filtering-the-data" class="section level3">
<h3><span class="header-section-number">3.3.1</span> Filtering the data</h3>
<p>We have a total of 5,265 nodes which is quite dense. After running the modularity class algorithm above, we will now move over to the Filters tab next to Statistics. Our aim here is to reduce the size of the network</p>
<p>Move over to the left where it says Ranking and then select the red inverted triangle. Set the largest value to 200 and the smallest to 20 (it is up to you what you choose). Then apply. The network will now change.</p>
<p>Open the Filters menu and choose Attributes. That will open a set of Folders and we would like to use Range. When the Range folder is open drag frequency into the Queries area below (marked with a red icon and Drag message when empty). Then either drag the range bar until you see a frequency of 5 as the minimum or change the number by clicking on it. Note that as we drag the results the number of Nodes and Edges in the Context above will change. We are looking for a manageable number. In the image below I have set the number to 5.</p>
<p><img src="images/gephi9/filter.png" /></p>
</div>
<div id="setting-node-size" class="section level3">
<h3><span class="header-section-number">3.3.2</span> Setting Node Size</h3>
<p>Next we want to size the nodes. On the left, look for the Appearance tab and then with Nodes in grey choose the Ranking button. Here the minimum size is set to 20 and the maximum to 150. Note that the default setting is 10 and this is generally too small for easy visibility. Press Apply and you will see the network changes to display the size of nodes based on the frequency. You can always adjust the size of nodes later if you are unhappy with them.</p>
<p><img src="images/gephi9/size.png" /></p>
</div>
<div id="colouring-the-nodes" class="section level3">
<h3><span class="header-section-number">3.3.3</span> Colouring the Nodes</h3>
<p>To colour the nodes choose the small palette icon next to the size icon. We now have choices on Unique (simply grey), Partition or Ranking. In this case we will choose Ranking and frequency. Note that a range of colour palettes can be accessed by clicking on the small icon to the right of the colour bar under ranking. When you have found a palette that you like then click Apply.</p>
<p><img src="images/gephi9/colour.png" /></p>
<p>An alternative way of colouring the graph in earlier versions of gephi was to partition on Modularity class. This would colour the nodes as ‘communities’ of closely linked nodes. However, at present in Gephi 9 this option does not appear to be consistently available but may return in a future update.</p>
<p>There are a range of other options for colouring nodes including a colour plugin that will detect if a column with a colour value is present in the imported data. This can be very useful if you have colour coded categories of data prior to importing to gephi.</p>
<p>##Laying out the Graph</p>
<p>In the bottom left panel called Layout in the figure above there are a range of network visualisation options with more that can be imported from the plugin menus. Among the most useful are Fruchterman-Reingold, Force Atlas, OpenOrd and Circular with specialist plugins for georeferenced layouts that are well worth experimenting with.</p>
<p>We will illustrate network layout using Fruchterman-Reingold. The first setting is the area for the graph. The default is 10,000 but we will start with 20,000 because 10,000 tends to be too crunched. The default for the gravity setting is 10. This is intended to stop the nodes dispersing too far but is often too tight when labels are applied. Try changing the setting to 5 (which reduces the gravitational pull). The settings in the different layout options can take some getting used too and it is worthwhile creating a record of useful settings. Gephi does not save your settings so make sure you write down useful settings.</p>
<p><img src="images/gephi9/layout_settings.png" /></p>
<p>We are now good to go. But, before we start take note of two important options for later use.</p>
<p>The first is the NoOverlap plugin we installed above. This will help us to deal with overlapping nodes after layout. The second is Expansion which will help us to increase the size of a network are to make it easier to see the labels. Note also the Contraction option which will allow us to pull a network back in if it expands too far.</p>
<p>Now make sure that Fruchterman-Reingold is selected with the settings mentioned above and click <code>Run</code>.</p>
<p>You can leave the network to run and the nodes will start to settle. If the network disappears from view (depending on your mouse) try scrolling to zoom out. Our aim is to arrive at a situation where lines only cross through nodes where they are connected. As you become more experienced with layout you may want to assist the nodes with moving into a clear position for a tidier graph.</p>
<p>You will now have a network that looks something like this (note that 15,000 for the Area may have been enough).</p>
<p><img src="images/gephi9/layout.png" /></p>
<p>We can see that some of the nodes are set very close together. That will affect the ability to label the nodes in a clear way. To address this we first use the NoOverlap function and later we may want to use the Expansion function in the Layout drop down menu items.</p>
<p>Choose nooverlap from the menu and Run.</p>
<p><img src="images/gephi9/noverlap.png" /></p>
<p>While the difference is very minor in this case we have at least moved the nodes into separate positions. At a later stage you may want to use the Expansion function. This will increase the size of the network and is useful when working with labels.</p>
<p><img src="images/gephi9/noverlap_laidout.png" /></p>
<div id="save-your-work" class="section level4">
<h4><span class="header-section-number">3.3.3.1</span> Save your work</h4>
<p>At this stage we will save our work. One feature of Gephi as a Java programme is that there is no undo option. As a result it is a good idea to save work at a point where you are fairly happy with the layout as it stands.</p>
<p>Go to File and choose Save As and give the file name a <code>.gephi</code> extension. Do not forget to do this or gephi will not know how to read the file. If all goes well the file will save. On some occasions Java may throw an exception and you will basically have to start again. That is one reason to save work in Gephi regularly because it is a beta programme and subject to the predilections of Java on your computer.</p>
<p>##Adding Labels</p>
<p>The next step is to add some labels. In the bottom menu bar there are a range of options. What we want is the small grey triangle on the right of this menu bar that will open up a new bar. Click in the triangle and you will see a set of options. Choose labels and then at the far left check the <code>Node</code> box. We will not see any labels yet.</p>
<p>To the right is a menu with size. This is set to scaled. To see some labels move the scale slider as far as it will go. We will see labels come into view and a first hint that we will need to do some more work on laying out the graph to make it readable.</p>
<p><img src="images/gephi9/labels.png" /></p>
<p>Next, change size to Node Size, the screen will now fill with text. Go to the scaler and pull it back until there is something more or less readable.</p>
<p><img src="images/gephi9/labels2.png" /></p>
<p>At this stage we may need to take a couple of actions.</p>
<ol style="list-style-type: decimal">
<li>Where it is clear that our nodes are too close together we will need to run Expansion from the layout menu. As a general rule of thumb you should only need to do this twice at most… but it may depend on your graph.</li>
<li>If you have very long labels such as Massachusetts Institute of Technology you will probably want to head over to the Observatory and edit the Node Label to something manageable such as MIT. This can make a big difference in cleaning up the labels.</li>
</ol>
<p>In the image below we have used Expansion twice and then manually resize the labels using the slider.</p>
<p>You will now have something that looks more or less like this.</p>
<p><img src="images/gephi9/expanded.png" /></p>
<p>Note that you can use the slider to the right in the bottom menu to adjust the sizes and you could of course adjust the font. In some cases you may be happy with a rough and ready network rather than the detailed adjustments that are required for a final network graph.</p>
<p>Note the small camera icon on the left of the bottom menu. Either press that to take a screenshot or hold to bring up a configure menu that will allow you to choose a size.</p>
<p><img src="images/gephi9/screenshot.png" /></p>
<p>If you pursuing this option you may also want to adjust the font or the colour and to use the bottom menu to arrive at a result you are happy with for display. In some cases (as we will deal with below) manually moving the nodes will allow you to arrive at a cleaner network for a screenshot.</p>
<p>Screenshots can be a very useful step in exploring data or sharing data internally. For publication quality graphics you will need to move over to using the Preview Options and engage in the <code>gephi shuffle</code> to progressively clean up the network for a publication quality graphic.</p>
<p>##Using the Preview Options</p>
<p>A more involved option for network visualisation is to move over to the Preview tab next to the Data Laboratory.</p>
<p>The default option uses curved edges. To use this press <code>Refresh</code>. This is fine but we can’t see any labels. In the presets now try default curved. You can play around with the different settings until you find a version that you like.</p>
<p>The main issue that we have here is that the labels are too large and the line weigths may also be very heavy.</p>
<p>To address the line weigth look for and check the rescale weight option under edges.</p>
<p><img src="images/gephi9/rescale_weight.png" /></p>
<p>Note here the difference with the visualisation in the Overview. With Gephi what you see is not what you get.</p>
<p>To arrive at a more readable network the first option is to adjust the size of the font in the <code>Node Labels</code> panel of the preview settings. Note here that label size is set to be proportional to the font (uncheck that and experiment if you wish). If we stick with proportional font size then we will start smaller and move upwards. For example, if we adjust the font size to 3 then the proportional font size will be reduced. In deciding on the font size an important consideration will be how many nodes you want to be legible to the reader. In this case setting the font size to 3 and this produces a pretty legible network.</p>
<p><img src="images/gephi9/font3.png" /></p>
<p>That is quite an acceptable graph for seeing the larger nodes. However, note that the labels of some of the nodes are overlapping some of the other nodes. This can produce a very cluttered look. The larger the base font size the more cluttered the graph will look and the more adjustment it is likely to need.</p>
<p>To make adjustments to this network we will use size 3. We will now need to move back and forwards between the Preview and the Overview adjusting the position of the nodes. For very complex graphs it can help to print out the preview to see what you need to adjust. Another sensible way to proceed is to mentally divide the graph into quarters and proceed clockwise quarter by quarter adjusting the nodes as you go. It is a very good idea to save your work at this point and as you go along.</p>
<p>In the first and second quarter moving clockwise things look good with no overlapping labels. However, some adjustments are needed in the third quarter in the middle of the network where Campanella and Kurs are overlapping. To make the adjustment move to the Overview tab, then select the small hand in the left vertical menu for grab. Locate Campanella and move it out of the way so it is not overlapping. Be gentle. Now go back to Preview and hit Refresh. When doing this quarter by quarter it can be helpful to zoom in in the Overview and in the Preview. For each of the overlapping nodes quarter by quarter make an adjustment periodically checking back by using Refresh in Preview and saving as you go along. Note that the aim is minor adjustments rather than major adjustments to node position (it is also possible to attempt to use Label Adjust in the Layout options but in practice this can distort the network). In the process it is also worth watching out for edges that intersect with nodes where there is no actual link. In those cases adjust the node position by trying to move it to the side of the unrelated edge. Note that this is often not possible with complex graphs and you will need to explain in the text that nodes may intersect with unrelated edges. Also check that any edits to labels do not contain mistakes (such as CATECH rather than CALTECH) and adjust accordingly. Typically long labels cause problems at this point and can be edited down in the Data Laboratory.</p>
<p>Through a series of minor adjustments in a clockwise direction you should arrive at a final network graph. Expect to spend about 20 minutes on clean up when you are familiar with Gephi depending on the number of nodes. It is worth noting that you will often need to go back to make final adjustments.</p>
<p><img src="images/gephi9/final.png" /></p>
<p>The basic principle here is that each node should have a readable label when you zoom in and that edges should not intersect with unrelated nodes (except if this is unavoidable). In this case we have taken a screen shot of the core of the network.</p>
<p><img src="images/gephi9/zoom.png" /></p>
<p>It is quite common when you arrive at a publication quality graphic to suddenly discover a mistake in the network. For example, at the data cleaning stage you may have decided not to group two companies with very similar names. However, at the network visualization stage the network suggests that in practice the two companies are one and the same. In this case check the data and head over to the Data Laboratory to group the nodes to a single node. As this suggests, time spent on data cleaning and data preparation will normally reap dividends in terms of time saved later in the analysis and visualization process.</p>
<p>##Exporting from Preview</p>
<p>At this stage we will want to do a final check and then export the data. Arriving at a publication quality export can in fact be the most time consuming and troublesome aspect of using Gephi. Before going any further save your work.</p>
<p>When exporting note that what you see on the screen and what you get are not exactly the same thing. The main issues are borders around nodes and the weight of lines in the edges. To adjust for this in the Nodes panel in Preview change the border width to a lower setting or 0 (the option we are choosing here). In the edges panel, if you do not want heavy lines then adjust the thickness or opacity (or both). In this case we have reduced the opacity of the edges to 50 and left the thickness as is. If you change something remember to hit Refresh.</p>
<p>Next select the export button in the bottom left. We will export to .pdf.</p>
<p>When you choose Export note that there is an <code>Options</code> menu for tighter control of the export.</p>
<p><img src="images/gephi9/export.png" /></p>
<p>The defaults are sensible and we will use those. If you are tempted to adjust them note that Gephi does not remember your settings, even when saved, so write them down. In reality the defaults work very well.</p>
<p>If all goes well you will end up with an image that looks like this.</p>
<p><img src="images/gephi9/drones.png" /></p>
<p>Because the default size is portrait you will want to crop the image. For publication you will also want to outline the text (to fix the font across system). This can be done with the free GIMP software or paid software such as Adobe Illustrator.</p>
<p>Congratulations, you have now created your first Gephi network graph.</p>
<p>##Resources</p>
<ol style="list-style-type: decimal">
<li><a href="http://gephi.github.io/">Gephi website</a></li>
<li><a href="https://github.com/gephi/gephi">Gephi github repository</a></li>
<li>Quick start <a href="https://gephi.github.io/tutorials/gephi-tutorial-quick_start.pdf">guide</a></li>
<li>Installation instructions for <a href="http://gephi.github.io/users/install/">All platforms</a>.
Gephi 8 suffers from a known issue for Mac users. That is, it uses Java 6 which is not installed by default on Macs. To resolve this you should follow the instructions posted <a href="http://sumnous.github.io/blog/2014/07/24/gephi-on-mac/">here</a> and works very well in most cases. It basically involves downloading a mac version of Java containing Java 6 and then running three or four commands in the Terminal on the mac to configure Gephi. If that doesn’t work try this more <a href="https://lbartkowski.wordpress.com/2014/11/28/gephi-0-8-2-on-apple-osx-yosemite/">detailed account</a>.</li>
<li><a href="https://marketplace.gephi.org/plugin/excel-csv-converter-to-network/">Excel/csv converter to network plugin</a></li>
<li>For ideas on patent network visualisation you might want to try this article on <a href="http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0034368">synthetic biology</a>, this <a href="http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0078737">article</a> on species names in patent data, and the use of exploratory network analysis using IPC/CPC co-occurrence analysis in the <a href="http://www.wipo.int/patentscope/en/programs/patent_landscapes/reports/animal_gr.html">WIPO Patent Landscape for Animal Genetic Resources</a>. For more try this <a href="https://www.google.co.uk/webhp?sourceid=chrome-instant&amp;ion=1&amp;espv=2&amp;ie=UTF-8#q=patent%20network%20analysis">Google Search</a>.</li>
</ol>

<p>#Patent Analytics with Plotly</p>
<p>##Introduction</p>
<p>In this chapter we provide an introduction to the online graphing service <a href="http://plotly.com">Plotly</a> to create graphics for use in patent analysis.</p>
<p>Plotly is an online graphing service that allows you to import excel, text and other files for visualisation. It also has API services for R, Python, MATLAB and a Plotly Javascript Library. A recent update to the <code>plotly</code> package in R allows you to easily produce graphics directly in RStudio and to send the graphics to Plotly online for further editing and to share with others.</p>
<p>Plotly’s great strength is that it produces attractive interactive graphics that can easily be shared with colleagues or made public. It also has a wide variety of graph types including contour and heat maps and is built with the very popular <a href="http://d3js.org">D3.js</a> Javascript library for interactive graphics. For examples of graphics created with Plotly see the <a href="https://plot.ly/feed/">public gallery</a>. Plotly was founded in 2012 and is therefore quite new, however, Plotly is becoming a great tool for creating and sharing graphics. In this chapter our aim is to get you started with using Plotly online with .csv or Excel files. In the second part of the chapter we will focus on using the <code>plotly</code> package in RStudio to generate and export graphs.</p>
<p>##Getting Started with Plotly</p>
<p>We need to start out by creating an account using the Create Account Button.</p>
<p><img src="images/plotlyfigs/fig1_plotly.png" /></p>
<p>Next we will see an invitation to take a tour (which is worth doing) and Plotly helpfully points out that we can load files from Google Drive or Dropbox. We then select the <code>Workspace</code> option to begin work.</p>
<p>##Importing Files</p>
<p>When you first arrive you will see a Workspace with a Grid (Plotly’s term for a table or worksheet).</p>
<p><img src="images/plotlyfigs/fig2_workspace.png" /></p>
<p>In the workspace you will see an Import Icon that provides a range of options for importing data. Don’t import anything yet! You can also copy data from a file and paste it into the Grid.</p>
<p>At the time of writing, the reason not to use these options at the moment is that while the data may import fine first time, in other cases it will not. Using the options on this page you will receive no information if an import fails. We also encountered problems with saving data that had been pasted into the worksheet (even where it appeared to work). To avoid potential frustration head over to <code>Organize</code>.</p>
<p><img src="images/plotlyfigs/fig3_import.png" /></p>
<p>From the Organize page select the New button and then Upload. Now select your local file. When you upload the file a status message will display and if all goes well then you will see a completed message. If not a red message will display informing you that there has been a problem (how you fix these problems is unclear).</p>
<p>For this experiment we used two datasets from the <a href="https://github.com/wipo-analytics/opensource-patent-analytics/tree/master/2_datasets">Open Source Patent Analytics Manual data repository</a>. When using the Github repository click on the file of interest until you see a <code>View Raw</code> message. Then right click to download the data file from there. You can download them for your own use directly from the following links.</p>
<ol style="list-style-type: decimal">
<li><a href="https://github.com/wipo-analytics/opensource-patent-analytics/raw/master/2_datasets/wipo/wipotrends_cleaned.csv">WIPO trends</a> application trends by year and with % change.</li>
<li><a href="https://github.com/wipo-analytics/opensource-patent-analytics/raw/master/2_datasets/pizza_medium_clean/pcy.csv">Pizza patents by country and year</a>. This is a simple dataset containing counts of patent documents containing the word pizza from <a href="https://patentscope.wipo.int/search/en/search.jsf">WIPO Patentscope</a> broken down by country and year.</li>
</ol>
<p>One important point to note is that Plotly is not a data processing tool. While there are some data tools, your data will generally need to be in a form that is suitable for plotting at the time of input. In part this reflects the use of APIs which allow for users of Python, R and Matlab to send their data to Plotly directly for sharing with others. This is one of the great strengths of Plotly and we will cover this below. However, we also experienced problems in loading and graphing datasets that were easy to work with in Tableau (as a benchmark). This suggests a need to invest time in understanding the formats that Plotly understands.</p>
<p>We experienced a different type of problem with the simple WIPO trends data where Plotly concatenated the first row (containing labels) and the first data row into one heading row. However, in most cases import seemed to be fine. To turn a row into a heading row try right clicking the row with the headings and right clicking on <code>use row as col headers</code>. Then right click again to remove the original row.</p>
<p>##Creating a graphic</p>
<p>We will start with the simple WIPO trends data by opening up that Grid.</p>
<p><img src="images/plotlyfigs/fig5_grid.png" /></p>
<p>Note here that in the Grid we have options to select the x or y axis for plotting. There is also an Options menu that we will come back to.</p>
<p>The Type of plot can be changed by selecting the drop down menu as we can see below.</p>
<p><img src="images/plotlyfigs/fig6_gtype.png" /></p>
<p>Sticking with a line graph, when we create the plot we can add a title and then change the theme (in this case to Catherine).</p>
<p><img src="images/plotlyfigs/fig7_line.png" /></p>
<p>We could also add a fit line by selecting the <code>FIT DATA</code> menu icon. This will ask you to create a fit and then you have a range of preset functions or you can add your own. Here we have simply chosen the default Linear fit.</p>
<p><img src="images/plotlyfigs/fig8_fit.png" /></p>
<p>We can then save the plot and use the export button to save the plot in a variety of formats and sizes. It is also very easy to add annotations using the Notes icon. Confusingly, the large blue Share button only seems to save the file and despite saving the plot we were not able to locate it again. While Plotly certainly looks nice, and appears to have attractive functions it is not intuitive and the difficulties involved in importing and sharing can be frustrating and time consuming. In short, time is needed to invest in and explore the potential of this tool.</p>
<p>###Adding a Second Axis</p>
<p>If we go back to our original WIPO trends data we have a percentage score for the year on year change in patent applications. We might want to show this on a plot with a second axis for the percentage.</p>
<p>To do that in the Grid view select the button “choose as y” in the Growth-rate column to add a second item for the y axis.</p>
<p><img src="images/plotlyfigs/fig9_secondy.png" /></p>
<p>When we choose Line plot we will now see the two sets of data with the percentage trailing on the bottom. We now need to create a second y axis on the right and assign the percentage data to that.</p>
<p>To do this select the Traces icon and a menu will pop up showing the data traces. The drop down menu under Traces will show Applications, so, select Growth Rate % from the drop down menu. Then where you see Lines/Markers select the dot. This will prevent the percentage scores displaying as a line.</p>
<p><img src="images/plotlyfigs/fig11_addaxis.png" /></p>
<p>Next, in the same panel under <code>Axes</code> select <code>New Axis/Subplot</code> and a new screen will pop up. We have some choices here but will simply choose to create a new axis on the right.</p>
<p><img src="images/plotlyfigs/fig11_addyaxis.png" /></p>
<p>The result will look something like this.</p>
<p><img src="images/plotlyfigs/fig12_result1.png" /></p>
<p>Our issue now is equalising the axes and changing the size of the points for the percentage scores. Finally we can add a title.</p>
<p>Before we go any further let’s note that we have a significant minus axis value of -3.6% in 2009 when patent applications declined. There is also a minus value in 2002.</p>
<p><img src="images/plotlyfigs/fig13_dip.png" /></p>
<p>If we wanted to retain these values we would probably want to turn off the second set of grid lines. We would also want to resize the points.</p>
<p>To turn off the grid lines on the second y axis Click on the Axes icon in the main menu on the left. Then from the <code>All Axes</code> drop down under Axes select Y Axis 2. Then click the Lines submenu icon and turn Lines and Grid lines to OFF. Also turn the Zero line to OFF unless you want to retain it.</p>
<p><img src="images/plotlyfigs/fig13_turnoffgrid.png" /></p>
<p>To resize the points we need to go back to the Traces main menu on the left and select Growth Rate from the list of Traces. Then choose the Style tab and change the marker size to something larger such as 8.</p>
<p><img src="images/plotlyfigs/fig13_resizepoint.png" /></p>
<p>To finish off the graphic we will want to add some labels. We can simply type in the Axis labels and a title into the text boxes provided. By choosing the Legend icon we could turn the legend on or off. Note that while this graph could be seen as self explanatory it may not be for the reader. We can also simply drag the axes labels to a different position.</p>
<p>It is possible that we would want to remove the negative values from the plot (in that case the values would need to be explained in the accompanying text). To do that select <code>Axes</code>, then <code>Y Axis2</code> then in <code>Autorange</code> choose <code>Non-negative</code> to show only values greater than zero on the plot.</p>
<p>If we wished we could also apply a fit line by choosing the Fit Data icon. We will choose Linear.<br />
<img src="images/plotlyfigs/fig13_final.png" /></p>
<p>Finally, to finish off the plot we might want to add annotations using the NOTES icon. Simply click on the plus sign in the pop up menu for a new annotation and then select the arrow and text and move it into the position you want.</p>
<p><img src="images/plotlyfigs/fig13_final_annotated.png" /></p>
<p>In this case we have added a couple of markers that may help to understand trends in activity. First, we have a dip in patent applications between 2001 and 2002. One possible explanation here is that this is a knock on effect of the collapse of the dot.com bubble where share prices reached a peak in 2000, declined rapidly and recovered before declining again into 2001. Patent data typically displays lag effects and it is reasonable to think that the decline in application activity from 2001 reflects these wider economic adjustments. Similarly, there is a significant dip in applications between 2008 and 2009 that it appears reasonable to assume reflects the knock on effects of the global economic crisis of 2007-2008. Note here that these are crude way markers to assist with interpreting the graphics. We could choose to add other timeline style events or layer graphics to help understand the potential or actual relationships between wider economic activity and trends in patent applications worldwide.</p>
<p>##Saving and Sharing</p>
<p>To save the plot we simply click Save. However, it is here that one of Plotly’s major strengths becomes apparent. As soon as we save the plot we can also invite others by email, we can create a public or private shareable link. For the collaborators, they must have a Plotly account already for this to work.</p>
<p><img src="images/plotlyfigs/fig13_email.png" /></p>
<p>The next option is to share a link. Note here that the default is to share a private link. To change that select the lock icon. The private link is particularly well suited for patent professionals.</p>
<p><img src="images/plotlyfigs/fig13_privatelink.png" /></p>
<p>You could also grab an embed code to embed the plot in a web page</p>
<p><img src="images/plotlyfigs/fig13_embed.png" /></p>
<p>Alternatively, surprise your friends and relatives by posting the plot on facebook or share with a wider audience on Twitter.</p>
<p>In this example we have focused on developing a very simple plot using <code>plotly</code>. In practice there are a wide range of possible plotting options with a growing number of tutorials provided <a href="http://help.plot.ly/">here</a>.</p>
<p>##Working with Plotly in R</p>
<p>We are following the instructions for setting up <a href="https://plot.ly/r/getting-started/">Plotly in R</a>. We will be using <a href="https://www.rstudio.com/">RStudio</a> for this experiment. Download RStudio for your operating system <a href="https://www.rstudio.com/products/rstudio/download/">here</a> and make sure that you also install R at the same time from the link on the RStudio page <a href="https://cran.rstudio.com">here</a>. For Python try these <a href="https://plot.ly/python/getting-started/">installation instructions</a> to get started.</p>
<p>In RStudio first we need to install the <code>plotly</code> package. We will also install some other helpful packages for working with data in R. Either select the Packages tab in RStudio and type <code>plotly</code> and install, or type the following in the console and press Enter.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb6-1" data-line-number="1"><span class="kw">install.packages</span>(<span class="st">&quot;plotly&quot;</span>) <span class="co"># the main event</span></a>
<a class="sourceLine" id="cb6-2" data-line-number="2"><span class="kw">install.packages</span>(<span class="st">&quot;readr&quot;</span>) <span class="co"># import csv files</span></a>
<a class="sourceLine" id="cb6-3" data-line-number="3"><span class="kw">install.packages</span>(<span class="st">&quot;dplyr&quot;</span>) <span class="co"># wrangle data</span></a>
<a class="sourceLine" id="cb6-4" data-line-number="4"><span class="kw">install.packages</span>(<span class="st">&quot;tidyr&quot;</span>) <span class="co"># tidy data</span></a></code></pre></div>
<p>Then load the libraries.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb7-1" data-line-number="1"><span class="kw">library</span>(plotly)</a>
<a class="sourceLine" id="cb7-2" data-line-number="2"><span class="kw">library</span>(readr)</a>
<a class="sourceLine" id="cb7-3" data-line-number="3"><span class="kw">library</span>(dplyr)</a>
<a class="sourceLine" id="cb7-4" data-line-number="4"><span class="kw">library</span>(tidyr)</a></code></pre></div>
<p>We now need to set our credentials for the API. When logged in to <code>plotly</code> follow <a href="https://plot.ly/settings/api">this link</a> to obtain your API key. Note also that you can obtain a streaming API token on the same page. Streaming will update a graphic from inside RStudio.</p>
<p>When you have obtained your token use the following command to store your username and the API key in your environment.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb8-1" data-line-number="1"><span class="kw">Sys.setenv</span>(<span class="st">&quot;plotly_username&quot;</span> =<span class="st"> &quot;your_plotly_username&quot;</span>)</a>
<a class="sourceLine" id="cb8-2" data-line-number="2"><span class="kw">Sys.setenv</span>(<span class="st">&quot;plotly_api_key&quot;</span> =<span class="st"> &quot;your_api_key&quot;</span>)</a></code></pre></div>
<p>Next we will load a dataset of WIPO Patentscope data containing sample data on patent documents containing the word pizza organised by country and year (pcy = pizza, country, year).</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb9-1" data-line-number="1"><span class="kw">library</span>(readr)</a>
<a class="sourceLine" id="cb9-2" data-line-number="2">pcy &lt;-<span class="st"> </span><span class="kw">read_csv</span>(<span class="st">&quot;https://github.com/wipo-analytics/opensource-patent-analytics/raw/master/2_datasets/pizza_medium_clean/pcy.csv&quot;</span>)</a></code></pre></div>
<p>Because patent data generally contains a data cliff for more recent years we will filter out recent years using <code>filter()</code> from the <code>dplyr</code> package by specifying a year that is less than or equal to 2012. To take out the long tail of limited historic data we will specify greater than or equal to 1990.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb10-1" data-line-number="1"><span class="kw">library</span>(dplyr)</a>
<a class="sourceLine" id="cb10-2" data-line-number="2">pcy &lt;-<span class="st"> </span><span class="kw">filter</span>(pcy, pubyear <span class="op">&gt;=</span><span class="st"> </span><span class="dv">1990</span>, pubyear <span class="op">&lt;=</span><span class="st"> </span><span class="dv">2012</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb10-3" data-line-number="3"><span class="st">  </span><span class="kw">print</span>()</a></code></pre></div>
<pre><code>## # A tibble: 223 x 4
##    pubcountry pubcode pubyear     n
##    &lt;chr&gt;      &lt;chr&gt;     &lt;int&gt; &lt;int&gt;
##  1 Canada     CA         1990    19
##  2 Canada     CA         1991    49
##  3 Canada     CA         1992    66
##  4 Canada     CA         1993    59
##  5 Canada     CA         1994    50
##  6 Canada     CA         1995    39
##  7 Canada     CA         1996    36
##  8 Canada     CA         1997    45
##  9 Canada     CA         1998    46
## 10 Canada     CA         1999    47
## # ... with 213 more rows</code></pre>
<p>To create the plot in <code>plotly</code> we use the <code>plot_ly()</code> function. We will specify the dataset, the x and y axis and then the colour for the country data (known as a trace in <code>plotly</code> language). We will then add a title using the <code>%&gt;%</code> pipe operator for “this” then “that”. To specify the visual we want we specify the mode as “lines”&quot; (try “markers” for a scatter plot).</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb12-1" data-line-number="1"><span class="kw">library</span>(plotly)</a>
<a class="sourceLine" id="cb12-2" data-line-number="2">s &lt;-<span class="st"> </span><span class="kw">plot_ly</span>(pcy, <span class="dt">x =</span> pubyear, <span class="dt">y =</span> n, <span class="dt">color =</span> pubcountry, <span class="dt">mode =</span> <span class="st">&quot;lines&quot;</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb12-3" data-line-number="3"><span class="st">  </span><span class="kw">layout</span>(<span class="dt">title =</span> <span class="st">&quot;Patenscope Pizza Patent Trends&quot;</span>)</a>
<a class="sourceLine" id="cb12-4" data-line-number="4">s</a></code></pre></div>
<p><img src="images/plotlyfigs/fig16_plotlys.png" /></p>
<p>Our data has more entries than there are colours in the default color palette. Plotly will record a warning on the number of colours but we can now clearly see a plot. If we have entered our credentials for the API (above) we can also push the graph online along with the data for further editing or to share with others.</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb13-1" data-line-number="1"><span class="kw">library</span>(plotly)</a>
<a class="sourceLine" id="cb13-2" data-line-number="2"><span class="kw">plotly_POST</span>(s)</a></code></pre></div>
<p>This will open a browser window and ask you to sign up or login before being taken to the graph.</p>
<p><img src="images/plotlyfigs/fig17_plotlys_post.png" /></p>
<p>As this makes clear, it is easy to generate a <code>plotly</code> graph in R but we will want to dig into the <code>plotly</code> package in a little more detail.</p>
<p>To change colours it is helpful to note that <code>plotly</code> installs and then calls the <code>RColorBrewer</code> package (it will display in the Packages list). To see the colour palettes we first tick RColorBrewer in Packages (or <code>library(RColorBrewer)</code>) to load it.</p>
<p>To view the available palettes you could simply use <code>View(brewer.pal.info)</code> or the following chunk which arranges the data by the number of colours.</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb14-1" data-line-number="1"><span class="kw">library</span>(RColorBrewer)</a>
<a class="sourceLine" id="cb14-2" data-line-number="2"><span class="kw">library</span>(dplyr)</a>
<a class="sourceLine" id="cb14-3" data-line-number="3">brewer.pal.info<span class="op">$</span>names &lt;-<span class="st"> </span><span class="kw">row.names</span>(brewer.pal.info)</a>
<a class="sourceLine" id="cb14-4" data-line-number="4"><span class="kw">select</span>(brewer.pal.info, <span class="dv">4</span><span class="op">:</span><span class="dv">1</span>) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb14-5" data-line-number="5"><span class="st">  </span><span class="kw">arrange</span>(<span class="kw">desc</span>(maxcolors))</a></code></pre></div>
<pre><code>##       names colorblind category maxcolors
## 1    Paired       TRUE     qual        12
## 2      Set3      FALSE     qual        12
## 3      BrBG       TRUE      div        11
## 4      PiYG       TRUE      div        11
## 5      PRGn       TRUE      div        11
## 6      PuOr       TRUE      div        11
## 7      RdBu       TRUE      div        11
## 8      RdGy      FALSE      div        11
## 9    RdYlBu       TRUE      div        11
## 10   RdYlGn      FALSE      div        11
## 11 Spectral      FALSE      div        11
## 12  Pastel1      FALSE     qual         9
## 13     Set1      FALSE     qual         9
## 14    Blues       TRUE      seq         9
## 15     BuGn       TRUE      seq         9
## 16     BuPu       TRUE      seq         9
## 17     GnBu       TRUE      seq         9
## 18   Greens       TRUE      seq         9
## 19    Greys       TRUE      seq         9
## 20  Oranges       TRUE      seq         9
## 21     OrRd       TRUE      seq         9
## 22     PuBu       TRUE      seq         9
## 23   PuBuGn       TRUE      seq         9
## 24     PuRd       TRUE      seq         9
## 25  Purples       TRUE      seq         9
## 26     RdPu       TRUE      seq         9
## 27     Reds       TRUE      seq         9
## 28     YlGn       TRUE      seq         9
## 29   YlGnBu       TRUE      seq         9
## 30   YlOrBr       TRUE      seq         9
## 31   YlOrRd       TRUE      seq         9
## 32   Accent      FALSE     qual         8
## 33    Dark2       TRUE     qual         8
## 34  Pastel2      FALSE     qual         8
## 35     Set2       TRUE     qual         8</code></pre>
<p>This indicates that the maximum number of colours in a palette is 12. Let’s try <code>Paired</code> for illustration. This has the advantage of being colour blind friendly.</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb16-1" data-line-number="1"><span class="kw">library</span>(plotly)</a>
<a class="sourceLine" id="cb16-2" data-line-number="2"><span class="kw">library</span>(dplyr)</a>
<a class="sourceLine" id="cb16-3" data-line-number="3">s1 &lt;-<span class="st"> </span><span class="kw">plot_ly</span>(pcy, <span class="dt">x =</span> pubyear, <span class="dt">y =</span> n, <span class="dt">color =</span> pubcountry, <span class="dt">colors =</span> <span class="st">&quot;Paired&quot;</span>, <span class="dt">mode =</span> <span class="st">&quot;lines&quot;</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb16-4" data-line-number="4"><span class="st">  </span><span class="kw">layout</span>(<span class="dt">title =</span> <span class="st">&quot;Pizza Patent trends&quot;</span>)</a>
<a class="sourceLine" id="cb16-5" data-line-number="5">s1</a></code></pre></div>
<p><img src="images/plotlyfigs/fig18_plotlys.png" /></p>
<p>As we can see this will then produce a plot with the color palette, <code>plotly</code> will show a warning that the base palette (“Set2”) has 8 colours but will then specify that it is displaying the palette that we requested.</p>
<p>In practice we would want to break this plot into subplots for two reasons. First, the data and value ranges vary widely between countries and second, it is better to ensure that colours are distinct.</p>
<p>To do this we need to run some calculations on the data. We will use functions from <code>dplyr</code> and <code>tidyr</code> to quickly tally the data grouping by the publication code. Then we will add the data to discreet groups based on the scores using <code>mutate()</code> (to add a variable) and <code>ntile()</code> to divide the countries into groups based on the number of records (n) and add this to the new variable called group. Finally, we arrange the data in descending order based on the number of records.</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb17-1" data-line-number="1"><span class="kw">library</span>(dplyr)</a>
<a class="sourceLine" id="cb17-2" data-line-number="2"><span class="kw">library</span>(tidyr)</a>
<a class="sourceLine" id="cb17-3" data-line-number="3">total &lt;-<span class="st"> </span><span class="kw">tally</span>(<span class="kw">group_by</span>(pcy, pubcode)) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb17-4" data-line-number="4"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">group =</span> <span class="kw">ntile</span>(nn, <span class="dv">3</span>)) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb17-5" data-line-number="5"><span class="st">  </span><span class="kw">rename</span>(<span class="dt">records =</span> nn) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb17-6" data-line-number="6"><span class="st">  </span><span class="kw">arrange</span>(<span class="kw">desc</span>(records)) </a>
<a class="sourceLine" id="cb17-7" data-line-number="7">total</a></code></pre></div>
<pre><code>## # A tibble: 16 x 3
##    pubcode records group
##    &lt;chr&gt;     &lt;int&gt; &lt;int&gt;
##  1 US         3835     3
##  2 WO         1366     3
##  3 CA         1186     3
##  4 EP         1074     3
##  5 KR          307     3
##  6 JP          205     2
##  7 DE           83     2
##  8 ZA           66     2
##  9 CN           60     2
## 10 IL           29     2
## 11 MX           23     1
## 12 RU           10     1
## 13 PT            9     1
## 14 EA            4     1
## 15 ES            3     1
## 16 SG            2     1</code></pre>
<p>When we view total we now see that countries have been divided into 3 groups based on their number of records. Groups 1 and 2 are unlikely to provide a meaningful graph and group 1 in particular could be dropped. However, we could usefully display this information as a bar chart using <code>plot_ly</code> and selecting <code>type = &quot;bar&quot;</code>.</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb19-1" data-line-number="1"><span class="kw">library</span>(plotly)</a>
<a class="sourceLine" id="cb19-2" data-line-number="2"><span class="kw">library</span>(dplyr)</a>
<a class="sourceLine" id="cb19-3" data-line-number="3">total_bar &lt;-<span class="st"> </span><span class="kw">plot_ly</span>(total, <span class="dt">x =</span> pubcode , <span class="dt">y =</span> records, <span class="dt">type =</span> <span class="st">&quot;bar&quot;</span>)</a>
<a class="sourceLine" id="cb19-4" data-line-number="4">total_bar</a></code></pre></div>
<p><img src="images/plotlyfigs/fig19_plotlybar.png" /></p>
<p>Having divided our data into three groups it would now be useful to plot them separately. Here we face the problem that our original data in pcy displays values by year while total displays the total number of records and group. We need first to add the group identifiers to the pcy table. To do that we will modify total to drop the count of records in <code>records</code> using the <code>dplyr</code> <code>select()</code> function. Then we will use <code>left_join()</code> to join the <code>total</code> and <code>total_group</code> tables together. Note that the function will use the shared field “pubcode” for joining.</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb20-1" data-line-number="1"><span class="kw">library</span>(dplyr)</a>
<a class="sourceLine" id="cb20-2" data-line-number="2">total_group &lt;-<span class="st"> </span><span class="kw">select</span>(total, pubcode, group) </a>
<a class="sourceLine" id="cb20-3" data-line-number="3">total_group</a></code></pre></div>
<pre><code>## # A tibble: 16 x 2
##    pubcode group
##    &lt;chr&gt;   &lt;int&gt;
##  1 US          3
##  2 WO          3
##  3 CA          3
##  4 EP          3
##  5 KR          3
##  6 JP          2
##  7 DE          2
##  8 ZA          2
##  9 CN          2
## 10 IL          2
## 11 MX          1
## 12 RU          1
## 13 PT          1
## 14 EA          1
## 15 ES          1
## 16 SG          1</code></pre>
<p>Then we join the two tables and rename <code>n</code> to <code>records</code> for graphing.</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb22-1" data-line-number="1"><span class="kw">library</span>(dplyr)</a>
<a class="sourceLine" id="cb22-2" data-line-number="2">total_grouped &lt;-<span class="st"> </span><span class="kw">left_join</span>(pcy, total_group) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb22-3" data-line-number="3"><span class="st">  </span><span class="kw">rename</span>(<span class="dt">records =</span> n)</a></code></pre></div>
<pre><code>## Joining, by = &quot;pubcode&quot;</code></pre>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb24-1" data-line-number="1">total_grouped</a></code></pre></div>
<pre><code>## # A tibble: 223 x 5
##    pubcountry pubcode pubyear records group
##    &lt;chr&gt;      &lt;chr&gt;     &lt;int&gt;   &lt;int&gt; &lt;int&gt;
##  1 Canada     CA         1990      19     3
##  2 Canada     CA         1991      49     3
##  3 Canada     CA         1992      66     3
##  4 Canada     CA         1993      59     3
##  5 Canada     CA         1994      50     3
##  6 Canada     CA         1995      39     3
##  7 Canada     CA         1996      36     3
##  8 Canada     CA         1997      45     3
##  9 Canada     CA         1998      46     3
## 10 Canada     CA         1999      47     3
## # ... with 213 more rows</code></pre>
<p>The next step is to generate a set of three plots corresponding with our three groups. We will call them pizza3, pizza2 and pizza1 and use the full publication country name in <code>pubcountry</code> as the colour for the lines.</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb26-1" data-line-number="1"><span class="kw">library</span>(plotly)</a>
<a class="sourceLine" id="cb26-2" data-line-number="2"><span class="kw">library</span>(dplyr)</a>
<a class="sourceLine" id="cb26-3" data-line-number="3">pizza3 &lt;-<span class="st"> </span><span class="kw">filter</span>(total_grouped, group <span class="op">==</span><span class="st"> </span><span class="dv">3</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb26-4" data-line-number="4"><span class="st">  </span><span class="kw">plot_ly</span>(<span class="dt">x =</span> pubyear, <span class="dt">y =</span> records, <span class="dt">color =</span> pubcountry, <span class="dt">type =</span> <span class="st">&quot;lines&quot;</span>, <span class="dt">mode =</span> <span class="st">&quot;lines&quot;</span>)</a>
<a class="sourceLine" id="cb26-5" data-line-number="5">pizza2 &lt;-<span class="st"> </span><span class="kw">filter</span>(total_grouped, group <span class="op">==</span><span class="st"> </span><span class="dv">2</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb26-6" data-line-number="6"><span class="st">  </span><span class="kw">plot_ly</span>(<span class="dt">x =</span> pubyear, <span class="dt">y =</span> records, <span class="dt">color =</span> pubcountry, <span class="dt">type =</span> <span class="st">&quot;lines&quot;</span>, <span class="dt">mode =</span> <span class="st">&quot;lines&quot;</span>)</a>
<a class="sourceLine" id="cb26-7" data-line-number="7">pizza1 &lt;-<span class="st"> </span><span class="kw">filter</span>(total_grouped, group <span class="op">==</span><span class="st"> </span><span class="dv">1</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb26-8" data-line-number="8"><span class="st">  </span><span class="kw">plot_ly</span>(<span class="dt">x =</span> pubyear, <span class="dt">y =</span> records, <span class="dt">color =</span> pubcountry, <span class="dt">type =</span> <span class="st">&quot;lines&quot;</span>, <span class="dt">markers =</span> <span class="st">&quot;lines&quot;</span>)</a></code></pre></div>
<p>We now have a total of four draft plots, total_bar and pizza 3 to 1 for our groups. Plotly will allow us to display plots side by side. Note that this can create quite a crunched display in RStudio and is best viewed by selecting the small <code>show in new window</code> button in the RStudio Viewer.
<!---add names to subplots---></p>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb27-1" data-line-number="1"><span class="kw">library</span>(plotly)</a>
<a class="sourceLine" id="cb27-2" data-line-number="2">sub &lt;-<span class="st"> </span><span class="kw">subplot</span>(total_bar, pizza3, pizza2, pizza1)</a>
<a class="sourceLine" id="cb27-3" data-line-number="3">sub</a>
<a class="sourceLine" id="cb27-4" data-line-number="4"><span class="kw">plotly_POST</span>(sub)</a></code></pre></div>
<p>You will now see an image that looks a lot like this.</p>
<p><img src="images/plotlyfigs/fig20_plotlymulti.png" /></p>
<p>The figure reveals no coherent trend for the countries in Group 1 on the right and it makes sense to drop this data. Group 2 is potentially more interesting but the low overall numbers and the spikes for data for Japan suggests very low activity and a lack of complete data. Furthermore, ideally we would want to allocate different colours to the different names in our trends panels (probably by allocating different palettes) which could take considerable time relative to the gains in terms of displaying low frequency data. We will let the bar chart do that work and finish with a simple two plot graphic to send to <code>plotly</code> online.</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb28-1" data-line-number="1"><span class="kw">library</span>(plotly)</a>
<a class="sourceLine" id="cb28-2" data-line-number="2">sub1 &lt;-<span class="st"> </span><span class="kw">subplot</span>(total_bar, pizza3)</a>
<a class="sourceLine" id="cb28-3" data-line-number="3"><span class="kw">plotly_POST</span>(sub1)</a></code></pre></div>
<p><img src="images/plotlyfigs/fig21_plotlymulti_online.png" /></p>
<p>It is then easy to edit the labels and make final adjustments online.</p>
<p>We can also share the graph via social media, download the data, or edit the graph. Note that the default setting for a graph sent via the API appears to be public (with no obvious way to change that).</p>
<p>It is here that Plotly’s potential importance as a tool for sharing data and graphics becomes apparent. It is a powerful tool. Recent updates to the R package and the introduction of dashboards demonstrates ongoing improvements to this new service.</p>
<p>##Round Up</p>
<p>In this chapter we have provided a brief introduction to Plotly to help you get started with using this tool for patent analytics. Plotly provides visually appealing and interactive graphics that can readily be shared with colleagues, pasted into websites and shared publicly. The availability of APIs is also a key feature of Plotly for those working in Python, R or other programmatic environments.</p>
<p>However, Plotly can also be confusing. For example, we found it hard to understand why particular datasets would not upload correctly (when they can easily be read in Tableau). We also found it hard to understand the format that the data needed to be in to plot correctly. So, Plotly can be somewhat frustrating although it has very considerable potential for sharing appealing graphics. The recent addition of <a href="https://plot.ly/dashboards/">dashboards</a> is also a promising development. Finally, for R users, the <code>plotly</code> package now closely integrates with the very popular <code>ggplot2</code> package through the ggplotly() function which allows for the creation of interactive <code>ggplot2</code> graphics.</p>
<p>In this chapter we have only touched on the potential of Plotly as a powerful free tool for creating interactive graphics. Other kinds of plots that are well worth exploring include Bubble maps, contour maps and heat maps. To experiment for yourself try the <a href="http://help.plot.ly/tutorials/">Plotly tutorials</a>.</p>

<p>#Patent Infographics with R</p>
<p>In this chapter we will use RStudio to prepare patent data for visualisation in an infographic using online software tools.</p>
<p>Infographics are a popular way of presenting data in a way that is easy for a reader to understand without reading a long report. Infographics are well suited to presenting summaries of data with simple messages about key findings. A good infographic can encourage the audience to read a detailed report and is a tool for engagement with audiences during presentations of the findings of patent research.</p>
<p>Some patent offices have already been creating infographics as part of their reports to policy makers and other clients. The Instituto Nacional de Propiedade Industrial (INPI) in Brazil produces regular two page <a href="http://www.inpi.gov.br/menu-servicos/informacao/radares-tecnologicos">Technology Radar</a> (Radar Tecnologico) consisting of charts and maps that briefly summarise more detailed research on subjects such as <a href="http://www.inpi.gov.br/menu-servicos/arquivos-cedin/n08_radar_tecnologico_nano_residuos_versao_resumida_ingles_atualizada_20160122.pdf">Nanotechnology in Waste Management</a>. <a href="http://www.wipo.int/patentscope/en/programs/patent_landscapes/">WIPO Patent Landscape Reports</a>, which go into depth on patent activity for a particular area, are accompanied by one page infographics that have proved very popular such as the infographic accompanying a recent report on <a href="http://www.wipo.int/export/sites/www/patentscope/en/programs/patent_landscapes/reports/documents/assistivedevices_infographic.pdf">assistive devices</a>.</p>
<p>A growing number of companies are offering online infographic software services such as <a href="https://infogr.am/app/#/library">infogr.am</a>,<a href="http://www.easel.ly">easel.ly</a> <a href="https://magic.piktochart.com/templates">piktochart.com</a>, <a href="https://www.canva.com/create/infographics/">canva.com</a> or <a href="https://venngage.com">venngage.com</a> to mention only a selection of the offerings out there. The <a href="http://www.coolinfographics.com/tools/">Cool Infographics website</a> provides a useful overview of available tools.</p>
<p>One feature of many of these services is that they are based on a freemium model. Creating graphics is free but the ability to export files and the available formats for export of your masterpiece (e.g. high resolution or .pdf) often depend on upgrading to a monthly account at varying prices. In this chapter we test drive <a href="https://infogr.am/app/#/library">infogr.am</a> as a chart friendly service, albeit with export options that depend on a paid account.</p>
<p>This chapter is divided into two sections.</p>
<ol style="list-style-type: decimal">
<li>In part 1 we focus on using RStudio to prepare patent data for visualisation in infographics software using the <code>dplyr</code>, <code>tidyr</code> and <code>stringr</code> packages. This involves dealing with common problems with patent data such as concatenated fields, white space and creating counts of data fields.</li>
<li>In part 2 we produce an infographic from the data using <a href="https://infogr.am/app/#/library">infogr.am</a>.</li>
</ol>
<p>##Getting Started</p>
<p>To start with we need to ensure that RStudio and R for your operating system are installed by following the instructions on the RStudio website <a href="https://www.rstudio.com/products/rstudio/download/">here</a>. Do not forget to follow the link to also <a href="https://cran.rstudio.com">install R for your operating system</a>.</p>
<p>When working in RStudio it is good practice to work with projects. This will keep all of the files for a project in the same folder. To create a project go to File, New Project and create a project. Call the project something like infographic. Any file you create and save for the project will now be listed under the Files tab in RStudio.</p>
<p>R works using packages (libraries) and there are around 7,490 of them for a whole range of purposes. We will use just a few of them. To install a package we use the following. Copy and paste the code into the Console and press enter.</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb29-1" data-line-number="1"><span class="kw">install.packages</span>(<span class="st">&quot;tidyverse&quot;</span>)  <span class="co"># the group of packages you will need</span></a></code></pre></div>
<p>Packages can also be installed by selecting the Packages tab and typing the name of the package.</p>
<p>To load the packages (libraries) use the following or check the tick box in the Packages pane.</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb30-1" data-line-number="1"><span class="kw">library</span>(readr)</a>
<a class="sourceLine" id="cb30-2" data-line-number="2"><span class="kw">library</span>(dplyr)</a>
<a class="sourceLine" id="cb30-3" data-line-number="3"><span class="kw">library</span>(tidyr)</a>
<a class="sourceLine" id="cb30-4" data-line-number="4"><span class="kw">library</span>(stringr)</a>
<a class="sourceLine" id="cb30-5" data-line-number="5"><span class="kw">library</span>(ggplot2)</a></code></pre></div>
<p>We are now ready to go.</p>
<p>##Load a .csv file using <code>readr</code></p>
<p>We will work with the <code>pizza_medium_clean</code> dataset in the online <a href="https://github.com/wipo-analytics/opensource-patent-analytics/tree/master/2_datasets">Github Manual repository</a>. If manually downloading a file remember to click on the file name and select <code>Raw</code> to download the actual file.</p>
<p>We can use the easy to use <code>read_csv()</code> function from the <code>readr</code> package to quickly read in our pizza data directly from the Github repository. Note the <code>raw</code> at the beginning of the filename.</p>
<div class="sourceCode" id="cb31"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb31-1" data-line-number="1"><span class="kw">library</span>(readr)</a>
<a class="sourceLine" id="cb31-2" data-line-number="2">pizza &lt;-<span class="st"> </span><span class="kw">read_csv</span>(<span class="st">&quot;https://github.com/wipo-analytics/opensource-patent-analytics/blob/master/2_datasets/pizza_medium_clean/pizza.csv?raw=true&quot;</span>)</a></code></pre></div>
<p><code>readr</code> will display a warning for the file arising from its efforts to parse publication dates on import. We will ignore this as we will not be using this field.</p>
<p>As an alternative to importing directly from Github download the file and in RStudio use <code>File &gt; Import Dataset &gt; From .csv</code>. If you experience problems with direct import of a file the File &gt; Import Dataset approach will give you a range of easy to use controls for figuring this out (e.g. where .csv is actually a tab separated file).</p>
<p>##Viewing Data</p>
<p>We can view data in a variety of ways.</p>
<ol style="list-style-type: decimal">
<li>In the console:</li>
</ol>
<div class="sourceCode" id="cb32"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb32-1" data-line-number="1">pizza</a></code></pre></div>
<pre><code>## # A tibble: 9,996 × 31
##                                                             applicants_cleaned
##                                                                          &lt;chr&gt;
## 1                                                                         &lt;NA&gt;
## 2  Ventimeglia Jamie Joseph; Ventimeglia Joel Michael; Ventimeglia Thomas Jose
## 3                                             Cordova Robert; Martinez Eduardo
## 4                                                      Lazarillo De Tormes S L
## 5                                                                         &lt;NA&gt;
## 6                                                           Depoortere, Thomas
## 7                                                             Frisco Findus Ag
## 8                                                   Bicycle Tools Incorporated
## 9                                                           Castiglioni, Carlo
## 10                                                                        &lt;NA&gt;
## # ... with 9,986 more rows, and 30 more variables:
## #   applicants_cleaned_type &lt;chr&gt;, applicants_organisations &lt;chr&gt;,
## #   applicants_original &lt;chr&gt;, inventors_cleaned &lt;chr&gt;,
## #   inventors_original &lt;chr&gt;, ipc_class &lt;chr&gt;, ipc_codes &lt;chr&gt;,
## #   ipc_names &lt;chr&gt;, ipc_original &lt;chr&gt;, ipc_subclass_codes &lt;chr&gt;,
## #   ipc_subclass_detail &lt;chr&gt;, ipc_subclass_names &lt;chr&gt;,
## #   priority_country_code &lt;chr&gt;, priority_country_code_names &lt;chr&gt;,
## #   priority_data_original &lt;chr&gt;, priority_date &lt;chr&gt;,
## #   publication_country_code &lt;chr&gt;, publication_country_name &lt;chr&gt;,
## #   publication_date &lt;chr&gt;, publication_date_original &lt;chr&gt;,
## #   publication_day &lt;int&gt;, publication_month &lt;int&gt;,
## #   publication_number &lt;chr&gt;, publication_number_espacenet_links &lt;chr&gt;,
## #   publication_year &lt;int&gt;, title_cleaned &lt;chr&gt;, title_nlp_cleaned &lt;chr&gt;,
## #   title_nlp_multiword_phrases &lt;chr&gt;, title_nlp_raw &lt;chr&gt;,
## #   title_original &lt;chr&gt;</code></pre>
<ol start="2" style="list-style-type: decimal">
<li><p>In Environment click on the blue arrow to see in the environment. Keep clicking to open a new window with the data.</p></li>
<li><p>Use the <code>View()</code> command (for data.frames and tables)</p></li>
</ol>
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb34-1" data-line-number="1"><span class="kw">View</span>(pizza)</a></code></pre></div>
<p>If possible use the View() command or environment. The difficulty with the console is that large amounts of data will simply stream past.</p>
<p>##Identifying Types of Object</p>
<p>We often want to know what type of object we are working with and more details about the object so we know what to do later. Here are some of the most common commands for obtaining information about objects.</p>
<div class="sourceCode" id="cb35"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb35-1" data-line-number="1"><span class="kw">class</span>(pizza)  ## type of object</a>
<a class="sourceLine" id="cb35-2" data-line-number="2"><span class="kw">names</span>(pizza)  ## names of variables</a>
<a class="sourceLine" id="cb35-3" data-line-number="3"><span class="kw">str</span>(pizza)  ## structure of object</a>
<a class="sourceLine" id="cb35-4" data-line-number="4"><span class="kw">dim</span>(pizza)  ## dimensions of the object</a></code></pre></div>
<p>The most useful command in this list is <code>str()</code> because this allows us to access the structure of the object and see its type.</p>
<div class="sourceCode" id="cb36"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb36-1" data-line-number="1"><span class="kw">str</span>(pizza, <span class="dt">max.level =</span> <span class="dv">1</span>)</a></code></pre></div>
<p><code>str()</code> is particularly useful because we can see the names of the fields (vectors) and their type. Most patent data is a character vector with dates forming integers.</p>
<p>##Working with Data</p>
<p>We will often want to select aspects of our data to focus on a specific set of columns or to create a graph. We might also want to add information, notably numeric counts.</p>
<p>The <code>dplyr</code> package provides a set of very handy functions for selecting, adding and counting data. The <code>tidyr</code> and <code>stringr</code> packages are sister packages that contain a range of other useful functions for working with our data. We have covered some of these in other chapters on graphing using R but will go through them quickly and then pull them together into a function that we can use across our dataset.</p>
<p>###Select</p>
<p>In this case we will start by using the <code>select()</code> function to limit the data to specific columns. We can do this using their names or their numeric position (best for large number of columns e.g. 1:31). In <code>dplyr</code>, unlike most R packages, existing character columns do not require <code>&quot;&quot;</code>.</p>
<div class="sourceCode" id="cb37"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb37-1" data-line-number="1"><span class="kw">library</span>(dplyr)</a>
<a class="sourceLine" id="cb37-2" data-line-number="2">pizza_number &lt;-<span class="st"> </span><span class="kw">select</span>(pizza, publication_number, publication_year)</a></code></pre></div>
<p>We now have a new data.frame that contains two columns. One with the year and one with the publication number. Note that we have created a new object called pizza_number using <code>&lt;-</code> and that after <code>select()</code> we have named our original data and the columns we want. A fundamental feature of select is that it will drop columns that we do not name. So it is best to create a new object using <code>&lt;-</code> if you want to keep your original data for later work.</p>
<p>###Adding data with <code>mutate()</code></p>
<p><code>mutate()</code> is a <code>dplyr</code> function that allows us to add data based on existing data in our data frame, for example to perform a calculation. In the case of patent data we normally lack a numeric field to use for counts. We can however assign a value to our publication field by using sum() and the number 1 as follows.</p>
<div class="sourceCode" id="cb38"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb38-1" data-line-number="1"><span class="kw">library</span>(dplyr)</a>
<a class="sourceLine" id="cb38-2" data-line-number="2">pizza_number &lt;-<span class="st"> </span><span class="kw">mutate</span>(pizza_number, <span class="dt">n =</span> <span class="kw">sum</span>(<span class="dt">publication_number =</span> <span class="dv">1</span>))</a></code></pre></div>
<p>When we view <code>pizza_number</code> we now have a value of 1 in the column <code>n</code> for each publication number. Note that in patent data a priority, application, publication or family number may occur multiple times and we would want to reduce the dataset to distinct records. For that we would use <code>n_distinct(pizza_number$publication_number)</code> from <code>dplyr</code> or <code>unique(pizza_number$publication_number)</code> from base R. Because the publication numbers are unique we can proceed.</p>
<p>###Counting data using <code>count()</code></p>
<p>At the moment, we have multiple instances of the same year (where a patent publication occurs in that year). We now want to calculate how many of our documents were published in each year. To do that we will use the <code>dplyr</code> function <code>count()</code>. We will use the publication_year and add <code>wt =</code> (for weight) with <code>n</code> as the value to count.</p>
<div class="sourceCode" id="cb39"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb39-1" data-line-number="1"><span class="kw">library</span>(dplyr)</a>
<a class="sourceLine" id="cb39-2" data-line-number="2">pizza_total &lt;-<span class="st"> </span><span class="kw">count</span>(pizza_number, publication_year, <span class="dt">wt =</span> n)</a>
<a class="sourceLine" id="cb39-3" data-line-number="3">pizza_total</a></code></pre></div>
<pre><code>## # A tibble: 58 × 2
##    publication_year    nn
##               &lt;int&gt; &lt;dbl&gt;
## 1              1940     1
## 2              1954     1
## 3              1956     1
## 4              1957     1
## 5              1959     1
## 6              1962     1
## 7              1964     2
## 8              1966     1
## 9              1967     1
## 10             1968     8
## # ... with 48 more rows</code></pre>
<p>When we now examine pizza_total, we will see the publication year and a summed value for the records in that year.</p>
<p>This raises the question of how we know that R has calculated the count correctly. We already know that there are 9996 records in the pizza dataset. To check our count is correct we can simply use sum and select the column we want to sum using <code>$</code>.</p>
<div class="sourceCode" id="cb41"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb41-1" data-line-number="1"><span class="kw">library</span>(dplyr)</a>
<a class="sourceLine" id="cb41-2" data-line-number="2"><span class="kw">sum</span>(pizza_total<span class="op">$</span>nn)</a></code></pre></div>
<pre><code>## [1] 9996</code></pre>
<p>So, all is good and we can move on. The <code>$</code> sign is one of the main ways of subsetting to tell R that we want to work with a specific column (the others are “[” and “[[”).</p>
<p>###Rename a field with <code>rename()</code></p>
<p>Next we will use <code>rename()</code> from <code>dplyr</code> to rename the fields. Note that understanding which field require quote marks can take some effort. In this case renaming the character vector publication_year as “pubyear” requires quotes while renaming the numeric vector “n” does not.</p>
<div class="sourceCode" id="cb43"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb43-1" data-line-number="1"><span class="kw">library</span>(dplyr)</a>
<a class="sourceLine" id="cb43-2" data-line-number="2">pizza_total &lt;-<span class="st"> </span><span class="kw">rename</span>(pizza_total, <span class="dt">pubyear =</span> publication_year, <span class="dt">publications =</span> nn) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb43-3" data-line-number="3"><span class="st">    </span><span class="kw">print</span>()</a></code></pre></div>
<pre><code>## # A tibble: 58 × 2
##    pubyear publications
##      &lt;int&gt;        &lt;dbl&gt;
## 1     1940            1
## 2     1954            1
## 3     1956            1
## 4     1957            1
## 5     1959            1
## 6     1962            1
## 7     1964            2
## 8     1966            1
## 9     1967            1
## 10    1968            8
## # ... with 48 more rows</code></pre>
<p>###Make a quickplot with <code>qplot()</code></p>
<p>Using the <code>qplot()</code> function in <code>ggplot2</code> we can now draw a quick line graph. Note that qplot() is unusual in R because the data (pizza_total) appears after the coordinates. We will specify that we want a line using <code>geom =</code> (if geom is left out it will be a scatter plot). This will give us an idea of what our plot might look like in our infographic and actions we might want to take on the data.</p>
<div class="sourceCode" id="cb45"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb45-1" data-line-number="1"><span class="kw">library</span>(ggplot2)</a>
<a class="sourceLine" id="cb45-2" data-line-number="2"><span class="kw">qplot</span>(<span class="dt">x =</span> pubyear, <span class="dt">y =</span> publications, <span class="dt">data =</span> pizza_total, <span class="dt">geom =</span> <span class="st">&quot;line&quot;</span>)</a></code></pre></div>
<p><img src="manual_bookdown_files/figure-html/qplot-1.png" width="1152" style="display: block; margin: auto;" /></p>
<!---![](images_foot/infogram/fig1_infographic.png)--->
<p>The plot reveals a data cliff in recent years. This normally reflects a lack of data for the last 2-3 years as recent documents feed through the system en route to publication.</p>
<p>It is a good idea to remove the data cliff by cutting the data 2-3 years prior to the present. In some cases two years is sufficient, but 3 years is a good rule of thumb.</p>
<p>We also have long tail of data with limited data from 1940 until the late 1970s. Depending on our purposes with the analysis we might want to keep this data (for historical analysis) or to focus in on a more recent period.</p>
<p>We will limit our data to specific values using the <code>dplyr</code> function <code>filter()</code>.</p>
<p>###Filter data using <code>filter()</code></p>
<p>In contrast with <code>select()</code> which works with columns, <code>filter()</code> in <code>dplyr</code> works with rows. In this case we need to filter on the values in the pubyear column. To remove the data prior to 1990 we will use the greater than or equal to operator <code>&gt;=</code> on the pubyear column and we will use the less than or equal to <code>&lt;=</code> operator on the values after 2012.</p>
<p>One strength of <code>filter()</code> in <code>dplyr</code> is that it is easy to filter on multiple values in the same expression (unlike the very similar filter function in base R). The use of <code>filter()</code> will also remove the 30 records where the year is recorded as NA (Not Available). We will write this file to disk using the simple <code>write_csv()</code> from <code>readr</code>. To use <code>write_csv()</code> we first name our data (<code>pizza_total</code>) and then provide a file name with a .csv extension. In this case and other examples below we have used a descriptive file name bearing in mind that Windows systems have limitations on the length and type of characters that can be used in file names.</p>
<div class="sourceCode" id="cb46"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb46-1" data-line-number="1"><span class="kw">library</span>(dplyr)</a>
<a class="sourceLine" id="cb46-2" data-line-number="2"><span class="kw">library</span>(readr)</a>
<a class="sourceLine" id="cb46-3" data-line-number="3">pizza_total &lt;-<span class="st"> </span><span class="kw">filter</span>(pizza_total, pubyear <span class="op">&gt;=</span><span class="st"> </span><span class="dv">1990</span>, pubyear <span class="op">&lt;=</span><span class="st"> </span><span class="dv">2012</span>)</a>
<a class="sourceLine" id="cb46-4" data-line-number="4"><span class="kw">write_csv</span>(pizza_total, <span class="st">&quot;pizza_total_1990_2012.csv&quot;</span>)</a>
<a class="sourceLine" id="cb46-5" data-line-number="5">pizza_total</a></code></pre></div>
<pre><code>## # A tibble: 23 × 2
##    pubyear publications
##      &lt;int&gt;        &lt;dbl&gt;
## 1     1990          139
## 2     1991          154
## 3     1992          212
## 4     1993          201
## 5     1994          162
## 6     1995          173
## 7     1996          180
## 8     1997          186
## 9     1998          212
## 10    1999          290
## # ... with 13 more rows</code></pre>
<p>When we print pizza_total to the console we will see that the data now covers the period 1990-2012. When using <code>filter()</code> on values in this way it is important to remember to apply this filter to any subsequent operations on the data (such as applicants) so that it matches the same data period.</p>
<p>To see our .csv file we can head over to the Files tab and, assuming that we have created a project, the file will now appear in the list of project files. Clicking on the file name will display the raw unformatted data in RStudio.</p>
<p>##Simplify code with pipes <code>%&gt;%</code></p>
<p>So far we have handled the code one line at a time. But, one of the great strengths of using a programming language is that we can run multiple lines of code together. There are two basic ways that we can do this.</p>
<p>We will create a new temporary object <code>df</code> to demonstrate this.</p>
<ol style="list-style-type: decimal">
<li>The standard way</li>
</ol>
<div class="sourceCode" id="cb48"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb48-1" data-line-number="1"><span class="kw">library</span>(dplyr)</a>
<a class="sourceLine" id="cb48-2" data-line-number="2"><span class="kw">library</span>(ggplot2)</a>
<a class="sourceLine" id="cb48-3" data-line-number="3">df &lt;-<span class="st"> </span><span class="kw">select</span>(pizza, publication_number, publication_year)</a>
<a class="sourceLine" id="cb48-4" data-line-number="4">df &lt;-<span class="st"> </span><span class="kw">mutate</span>(df, <span class="dt">n =</span> <span class="kw">sum</span>(<span class="dt">publication_number =</span> <span class="dv">1</span>))</a>
<a class="sourceLine" id="cb48-5" data-line-number="5">df &lt;-<span class="st"> </span><span class="kw">count</span>(df, publication_year, <span class="dt">wt =</span> n)</a>
<a class="sourceLine" id="cb48-6" data-line-number="6">df &lt;-<span class="st"> </span><span class="kw">rename</span>(df, <span class="dt">pubyear =</span> publication_year, <span class="dt">publications =</span> nn)</a>
<a class="sourceLine" id="cb48-7" data-line-number="7">df &lt;-<span class="st"> </span><span class="kw">filter</span>(df, pubyear <span class="op">&gt;=</span><span class="st"> </span><span class="dv">1990</span>, pubyear <span class="op">&lt;=</span><span class="st"> </span><span class="dv">2012</span>)</a>
<a class="sourceLine" id="cb48-8" data-line-number="8"><span class="kw">qplot</span>(<span class="dt">x =</span> pubyear, <span class="dt">y =</span> publications, <span class="dt">data =</span> df, <span class="dt">geom =</span> <span class="st">&quot;line&quot;</span>)</a></code></pre></div>
<p>The code we have just created is six lines long. If we select all of this code and run it in one go it will produce our graph.</p>
<p>One feature of this code is that each time we run a function on the object total we name it at the start of each function (e.g. mutate(df…)) and then we overwrite the object.</p>
<p>We can save quite a lot of typing and reduce the complexity of the code using the pipe operator introduced by the the <code>magrittr</code> package and then adopted in Hadley Wickham’s data wrangling and tidying packages.</p>
<ol start="2" style="list-style-type: decimal">
<li>Using pipes <code>%&gt;%</code></li>
</ol>
<p>Pipes are now a very popular way of writing R code because they simplify writing R code and speed it up. The most popular pipe is <code>%&gt;%</code> which means “this” then “that”. In this case we are going to create a new temporary object <code>df1</code> by first applying select to pizza, then mutate, count, rename and filter. Note that we only name our dataset once (in <code>select()</code>) and we do not need to keep overwriting the object.</p>
<div class="sourceCode" id="cb49"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb49-1" data-line-number="1"><span class="kw">library</span>(dplyr)</a>
<a class="sourceLine" id="cb49-2" data-line-number="2"><span class="kw">library</span>(ggplot2)</a>
<a class="sourceLine" id="cb49-3" data-line-number="3">df1 &lt;-<span class="st"> </span><span class="kw">select</span>(pizza, publication_number, publication_year) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">n =</span> <span class="kw">sum</span>(<span class="dt">publication_number =</span> <span class="dv">1</span>)) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb49-4" data-line-number="4"><span class="st">    </span><span class="kw">count</span>(publication_year, <span class="dt">wt =</span> n) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">rename</span>(<span class="dt">pubyear =</span> publication_year, <span class="dt">publications =</span> nn) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb49-5" data-line-number="5"><span class="st">    </span><span class="kw">filter</span>(pubyear <span class="op">&gt;=</span><span class="st"> </span><span class="dv">1990</span>, pubyear <span class="op">&lt;=</span><span class="st"> </span><span class="dv">2012</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">qplot</span>(<span class="dt">x =</span> pubyear, <span class="dt">y =</span> publications, </a>
<a class="sourceLine" id="cb49-6" data-line-number="6">    <span class="dt">data =</span> ., <span class="dt">geom =</span> <span class="st">&quot;line&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">print</span>()</a></code></pre></div>
<p><img src="manual_bookdown_files/figure-html/piped-1.png" width="1152" style="display: block; margin: auto;" /></p>
<!---![](images_foot/infogram/fig2_infographic_qplot.png)--->
<p>In the standard code we typed <code>df</code> nine times to arrive at the same result. Using pipes we typed df1 once. Of greater importance is that the use of pipes simplifies the structure of R code by introducing a basic “this” then “that” logic which makes it easier to understand.</p>
<p>One point to note about this code is that <code>qplot()</code> requires us to name our data (in this case <code>df1</code>). However, <code>df1</code> is actually the final output of the code and does not exist as an input object before the final line is run. So, if we attempt to use <code>data = df1</code> in <code>qplot()</code> we will receive an error message. The way around this is to use <code>.</code> in place of our data object. That way <code>qplot()</code> will know we want to graph the outputs of the earlier code. Finally, we need to add an explicit call to <code>print()</code> to display the graph (without this the code will work but we will not see the graph).</p>
<p>If we now inspect the structure of the df1 object (using <code>str(df1)</code>) in the console, it will be a list. The reason for this is that it is an object with mixed components, including a data.frame with our data plus additional data setting out the contents of the plot. As there is no direct link between R and our infographics software this will create problems for us later because the infographics software won’t know how to interpret the list object. So, it is generally a good idea to use a straight data.frame by excluding the call to <code>qplot</code> and adding it later when needed as follows.</p>
<div class="sourceCode" id="cb50"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb50-1" data-line-number="1"><span class="kw">library</span>(dplyr)</a>
<a class="sourceLine" id="cb50-2" data-line-number="2"><span class="kw">library</span>(ggplot2)</a>
<a class="sourceLine" id="cb50-3" data-line-number="3">df2 &lt;-<span class="st"> </span><span class="kw">select</span>(pizza, publication_number, publication_year) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">n =</span> <span class="kw">sum</span>(<span class="dt">publication_number =</span> <span class="dv">1</span>)) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb50-4" data-line-number="4"><span class="st">    </span><span class="kw">count</span>(publication_year, <span class="dt">wt =</span> n) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">rename</span>(<span class="dt">pubyear =</span> publication_year, <span class="dt">publications =</span> nn) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb50-5" data-line-number="5"><span class="st">    </span><span class="kw">filter</span>(pubyear <span class="op">&gt;=</span><span class="st"> </span><span class="dv">1990</span>, pubyear <span class="op">&lt;=</span><span class="st"> </span><span class="dv">2012</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">print</span>()</a></code></pre></div>
<pre><code>## # A tibble: 23 × 2
##    pubyear publications
##      &lt;int&gt;        &lt;dbl&gt;
## 1     1990          139
## 2     1991          154
## 3     1992          212
## 4     1993          201
## 5     1994          162
## 6     1995          173
## 7     1996          180
## 8     1997          186
## 9     1998          212
## 10    1999          290
## # ... with 13 more rows</code></pre>
<p>Note that in this case the only change is that we need to explicitly include the reference to the df2 data frame as the data argument in the call to <code>qplot()</code>.</p>
<div class="sourceCode" id="cb52"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb52-1" data-line-number="1"><span class="kw">library</span>(ggplot2)</a>
<a class="sourceLine" id="cb52-2" data-line-number="2"><span class="kw">qplot</span>(<span class="dt">x =</span> pubyear, <span class="dt">y =</span> publications, <span class="dt">data =</span> df2, <span class="dt">geom =</span> <span class="st">&quot;line&quot;</span>)</a></code></pre></div>
<p><img src="manual_bookdown_files/figure-html/df2_qplot-1.png" width="1152" style="display: block; margin: auto;" /></p>
<p>##Harmonising data</p>
<p>One challenge with creating multiple tables from a baseline dataset is keeping track of subdatasets. At the moment we have two basic objects we will be working with:</p>
<ol style="list-style-type: decimal">
<li><code>pizza</code> - our raw dataset</li>
<li><code>pizza_total</code> - created via <code>pizza_number</code> limited to 1990_2012.</li>
</ol>
<p>In the remainder of the chapter we will want to create some additional datasets from our pizza dataset. These are:</p>
<ol style="list-style-type: decimal">
<li>Country trends</li>
<li>Applicants</li>
<li>International Patent Classification (IPC) Class</li>
<li>Phrases</li>
<li>Google</li>
<li>Google IPC</li>
<li>Google phrases</li>
</ol>
<p>We need to make sure that any data that we generate from our raw dataset matches the period for the <code>pizza_total</code> dataset. If we do not do this there is a risk that we will generate subdatasets with counts for the raw pizza dataset.</p>
<p>To handle this we will use <code>filter()</code> to create a new baseline dataset with an unambiguous name.</p>
<div class="sourceCode" id="cb53"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb53-1" data-line-number="1"><span class="kw">library</span>(dplyr)</a>
<a class="sourceLine" id="cb53-2" data-line-number="2">pizza_<span class="dv">1990</span>_<span class="dv">2012</span> &lt;-<span class="st"> </span><span class="kw">rename</span>(pizza, <span class="dt">pubyear =</span> publication_year) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(pubyear <span class="op">&gt;=</span><span class="st"> </span></a>
<a class="sourceLine" id="cb53-3" data-line-number="3"><span class="st">    </span><span class="dv">1990</span>, pubyear <span class="op">&lt;=</span><span class="st"> </span><span class="dv">2012</span>)</a>
<a class="sourceLine" id="cb53-4" data-line-number="4">pizza_<span class="dv">1990</span>_<span class="dv">2012</span></a></code></pre></div>
<pre><code>## # A tibble: 8,262 × 31
##                    applicants_cleaned applicants_cleaned_type
##                                 &lt;chr&gt;                   &lt;chr&gt;
## 1                                &lt;NA&gt;                  People
## 2             Lazarillo De Tormes S L               Corporate
## 3                                &lt;NA&gt;                  People
## 4                  Depoortere, Thomas                  People
## 5                    Frisco Findus Ag               Corporate
## 6          Bicycle Tools Incorporated               Corporate
## 7                  Castiglioni, Carlo                  People
## 8                                &lt;NA&gt;                  People
## 9               Bujalski, Wlodzimierz                  People
## 10 Ehrno Flexible A/S; Stergaard, Ole       Corporate; People
## # ... with 8,252 more rows, and 29 more variables:
## #   applicants_organisations &lt;chr&gt;, applicants_original &lt;chr&gt;,
## #   inventors_cleaned &lt;chr&gt;, inventors_original &lt;chr&gt;, ipc_class &lt;chr&gt;,
## #   ipc_codes &lt;chr&gt;, ipc_names &lt;chr&gt;, ipc_original &lt;chr&gt;,
## #   ipc_subclass_codes &lt;chr&gt;, ipc_subclass_detail &lt;chr&gt;,
## #   ipc_subclass_names &lt;chr&gt;, priority_country_code &lt;chr&gt;,
## #   priority_country_code_names &lt;chr&gt;, priority_data_original &lt;chr&gt;,
## #   priority_date &lt;chr&gt;, publication_country_code &lt;chr&gt;,
## #   publication_country_name &lt;chr&gt;, publication_date &lt;chr&gt;,
## #   publication_date_original &lt;chr&gt;, publication_day &lt;int&gt;,
## #   publication_month &lt;int&gt;, publication_number &lt;chr&gt;,
## #   publication_number_espacenet_links &lt;chr&gt;, pubyear &lt;int&gt;,
## #   title_cleaned &lt;chr&gt;, title_nlp_cleaned &lt;chr&gt;,
## #   title_nlp_multiword_phrases &lt;chr&gt;, title_nlp_raw &lt;chr&gt;,
## #   title_original &lt;chr&gt;</code></pre>
<p>In this case we start with a call to <code>rename()</code> to make this consistent with our pizza_total table and then use a pipe to filter the data on the year. Note here that when filtering raw data on a set of values it is important to inspect it first to check that the field is clean (e.g. not concatenated). If for some reason your data is concatenated (which happens quite a lot with patent data) then lookup <code>?tidyr::separate_rows</code>.</p>
<p>We are now in a position to create our country trends table.</p>
<p>##Country Trends using <code>spread()</code></p>
<p>There are two basic data formats: long and wide. Our pizza dataset is in long format because each column is a variable (e.g. <code>publication_country</code>) and each row in <code>publication_country</code> contains a country name. This is the most common and useful data format.</p>
<p>However, in some cases, such as <code>infogr.am</code> our visualisation software will expect the data to be in wide format. In this case each country name would become a variable (column name) with the years forming the rows and the number of records per year the observations. The key to this is the <code>tidyr()</code> function <code>spread()</code>.</p>
<p>As above we will start off by using <code>select()</code> to create a table with the fields that we want. We will then use <code>mutate()</code> to add a numeric field and then count up that data. To illustrate the process run this code (we will not create an object).</p>
<div class="sourceCode" id="cb55"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb55-1" data-line-number="1"><span class="kw">library</span>(dplyr)</a>
<a class="sourceLine" id="cb55-2" data-line-number="2"><span class="kw">select</span>(pizza_<span class="dv">1990</span>_<span class="dv">2012</span>, publication_country_name, publication_number, pubyear) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb55-3" data-line-number="3"><span class="st">    </span><span class="kw">mutate</span>(<span class="dt">n =</span> <span class="kw">sum</span>(<span class="dt">publication_number =</span> <span class="dv">1</span>)) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">count</span>(publication_country_name, </a>
<a class="sourceLine" id="cb55-4" data-line-number="4">    pubyear, <span class="dt">wt =</span> n) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">print</span>()</a></code></pre></div>
<pre><code>## Source: local data frame [223 x 3]
## Groups: publication_country_name [?]
## 
##    publication_country_name pubyear    nn
##                       &lt;chr&gt;   &lt;int&gt; &lt;dbl&gt;
## 1                    Canada    1990    19
## 2                    Canada    1991    49
## 3                    Canada    1992    66
## 4                    Canada    1993    59
## 5                    Canada    1994    50
## 6                    Canada    1995    39
## 7                    Canada    1996    36
## 8                    Canada    1997    45
## 9                    Canada    1998    46
## 10                   Canada    1999    47
## # ... with 213 more rows</code></pre>
<p>When we run this code we will see the results in long format. We now want to take our <code>publication_country_name</code> column and spread it to form columns with <code>nn</code> as the values.</p>
<p>In using spread note that it takes a data argument (<code>pizza_1990_2012</code>), a key (<code>publication_country_name</code>), and value column (<code>nn</code>) (created from <code>count()</code>). We are using pipes so the data only needs to be named in the first line. For additional arguments see <code>?spread()</code>.</p>
<div class="sourceCode" id="cb57"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb57-1" data-line-number="1"><span class="kw">library</span>(dplyr)</a>
<a class="sourceLine" id="cb57-2" data-line-number="2"><span class="kw">library</span>(tidyr)</a>
<a class="sourceLine" id="cb57-3" data-line-number="3">country_totals &lt;-<span class="st"> </span><span class="kw">select</span>(pizza_<span class="dv">1990</span>_<span class="dv">2012</span>, publication_country_name, publication_number, pubyear) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb57-4" data-line-number="4"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">n =</span> <span class="kw">sum</span>(<span class="dt">publication_number =</span> <span class="dv">1</span>)) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb57-5" data-line-number="5"><span class="st">  </span><span class="kw">count</span>(publication_country_name, pubyear, <span class="dt">wt =</span> n) <span class="op">%&gt;%</span><span class="st"> </span><span class="co"># note n</span></a>
<a class="sourceLine" id="cb57-6" data-line-number="6"><span class="st">  </span><span class="kw">spread</span>(publication_country_name, nn) <span class="co"># note double nn</span></a>
<a class="sourceLine" id="cb57-7" data-line-number="7">country_totals</a></code></pre></div>
<pre><code>## # A tibble: 23 × 17
##    pubyear Canada China `Eurasian Patent Organization`
## *    &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt;                          &lt;dbl&gt;
## 1     1990     19    NA                             NA
## 2     1991     49    NA                             NA
## 3     1992     66    NA                             NA
## 4     1993     59    NA                             NA
## 5     1994     50    NA                             NA
## 6     1995     39    NA                             NA
## 7     1996     36     1                             NA
## 8     1997     45    NA                             NA
## 9     1998     46    NA                             NA
## 10    1999     47     2                              2
## # ... with 13 more rows, and 13 more variables: `European Patent
## #   Office` &lt;dbl&gt;, Germany &lt;dbl&gt;, Israel &lt;dbl&gt;, Japan &lt;dbl&gt;, `Korea,
## #   Republic of` &lt;dbl&gt;, Mexico &lt;dbl&gt;, `Patent Co-operation Treaty` &lt;dbl&gt;,
## #   Portugal &lt;dbl&gt;, `Russian Federation` &lt;dbl&gt;, Singapore &lt;dbl&gt;, `South
## #   Africa` &lt;dbl&gt;, Spain &lt;dbl&gt;, `United States of America` &lt;dbl&gt;</code></pre>
<p>We now have data in wide format.</p>
<p>In some cases, such as infogr.am, visualisation software may expect the country names to be the name of rows and the column names to be years . We can modify our call to <code>spread()</code> by replacing the <code>publication_country_name</code> with <code>pubyear</code>. Then we will write the data to disk for use in our infographic.</p>
<div class="sourceCode" id="cb59"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb59-1" data-line-number="1"><span class="kw">library</span>(dplyr)</a>
<a class="sourceLine" id="cb59-2" data-line-number="2"><span class="kw">library</span>(readr)</a>
<a class="sourceLine" id="cb59-3" data-line-number="3">country_totals &lt;-<span class="st"> </span><span class="kw">select</span>(pizza_<span class="dv">1990</span>_<span class="dv">2012</span>, publication_country_name, publication_number, pubyear) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb59-4" data-line-number="4"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">n =</span> <span class="kw">sum</span>(<span class="dt">publication_number =</span> <span class="dv">1</span>)) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb59-5" data-line-number="5"><span class="st">  </span><span class="kw">count</span>(publication_country_name, pubyear, <span class="dt">wt =</span> n) <span class="op">%&gt;%</span><span class="st"> </span><span class="co"># note n</span></a>
<a class="sourceLine" id="cb59-6" data-line-number="6"><span class="st">  </span><span class="kw">spread</span>(pubyear, nn) <span class="co"># note nn</span></a>
<a class="sourceLine" id="cb59-7" data-line-number="7">country_totals</a></code></pre></div>
<pre><code>## Source: local data frame [16 x 24]
## Groups: publication_country_name [16]
## 
##        publication_country_name `1990` `1991` `1992` `1993` `1994` `1995`
## *                         &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;
## 1                        Canada     19     49     66     59     50     39
## 2                         China     NA     NA     NA     NA     NA     NA
## 3  Eurasian Patent Organization     NA     NA     NA     NA     NA     NA
## 4        European Patent Office     22     29     36     29     26     29
## 5                       Germany      2      2      2      2      5      2
## 6                        Israel     NA     NA      1     NA     NA      1
## 7                         Japan     NA     NA     NA     NA     NA     NA
## 8            Korea, Republic of     NA     NA     NA      1     NA     NA
## 9                        Mexico     NA     NA     NA     NA     NA     NA
## 10   Patent Co-operation Treaty      8     13     31     16     20     22
## 11                     Portugal     NA     NA     NA     NA     NA     NA
## 12           Russian Federation     NA     NA     NA     NA     NA     NA
## 13                    Singapore     NA     NA     NA     NA     NA     NA
## 14                 South Africa      2      3      3      3      3      1
## 15                        Spain     NA     NA     NA     NA     NA     NA
## 16     United States of America     86     58     73     91     58     79
## # ... with 17 more variables: `1996` &lt;dbl&gt;, `1997` &lt;dbl&gt;, `1998` &lt;dbl&gt;,
## #   `1999` &lt;dbl&gt;, `2000` &lt;dbl&gt;, `2001` &lt;dbl&gt;, `2002` &lt;dbl&gt;, `2003` &lt;dbl&gt;,
## #   `2004` &lt;dbl&gt;, `2005` &lt;dbl&gt;, `2006` &lt;dbl&gt;, `2007` &lt;dbl&gt;, `2008` &lt;dbl&gt;,
## #   `2009` &lt;dbl&gt;, `2010` &lt;dbl&gt;, `2011` &lt;dbl&gt;, `2012` &lt;dbl&gt;</code></pre>
<div class="sourceCode" id="cb61"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb61-1" data-line-number="1"><span class="kw">write_csv</span>(country_totals, <span class="st">&quot;pizza_country_1990_2012.csv&quot;</span>)</a></code></pre></div>
<p>To restore the data to long format we would need to use <code>gather()</code> as the counterpart to <code>spread()</code>. <code>gather()</code> takes a dataset, a key for the name of the column we want to gather the countries into, a value for the numeric count (in this case n), and finally the positions of the columns to gather in. Note here that we need to look up the column positions in <code>country_totals</code> (e.g. using <code>View()</code>) or count the columns using <code>ncol(country_totals)</code>.</p>
<div class="sourceCode" id="cb62"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb62-1" data-line-number="1"><span class="kw">library</span>(dplyr)</a>
<a class="sourceLine" id="cb62-2" data-line-number="2"><span class="kw">gather</span>(country_totals, year, n, <span class="dv">2</span><span class="op">:</span><span class="dv">24</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">print</span>()</a></code></pre></div>
<pre><code>## Source: local data frame [368 x 3]
## Groups: publication_country_name [16]
## 
##        publication_country_name  year     n
##                           &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt;
## 1                        Canada  1990    19
## 2                         China  1990    NA
## 3  Eurasian Patent Organization  1990    NA
## 4        European Patent Office  1990    22
## 5                       Germany  1990     2
## 6                        Israel  1990    NA
## 7                         Japan  1990    NA
## 8            Korea, Republic of  1990    NA
## 9                        Mexico  1990    NA
## 10   Patent Co-operation Treaty  1990     8
## # ... with 358 more rows</code></pre>
<p>The combination of spread and gather work really well to prepare data in formats that are expected by other software. However, one of the main issues we encounter with patent data is that our data is not tidy because various fields are concatenated.</p>
<p>##Tidying data - Separating and Gathering</p>
<p>In patent data we often see concatenated fields with a separator (normally a <code>;</code>). These are typically applicant names, inventor names, International Patent Classification (IPC) codes, or document numbers (priority numbers, family numbers). We need to <code>tidy</code> this data prior to data cleaning (such as cleaning names) or to prepare for analysis and visualisation. For more on the concept of tidy data read <a href="http://vita.had.co.nz/papers/tidy-data.pdf">Hadley Wickham’s Tidy Data article</a>. The new <a href="http://r4ds.had.co.nz/tidy-data.html">R for Data Science book</a> by Garrett Grolemund and Hadley Wickham (see Chapter 12) is also strongly recommended.</p>
<p>To tidy patent data we will typically need to do two things.</p>
<ol style="list-style-type: decimal">
<li><p>Separate the data so that each cell contains a unique data point (e.g. a name, code or publication number). This normally involves separating data into columns.</p></li>
<li><p>Gathering the data back in. This involves transforming the data in the columns we have created into rows.</p></li>
</ol>
<p>Separating data into columns is very easy in tools such as Excel. However, gathering the data back into separate rows is remarkably difficult. Happily, this is very easy to do in R with the <code>tidyr</code> package.</p>
<p>The <code>tidyr</code> package contains three functions that are very useful when working with patent data. When dealing with concatenated fields in columns the key function is <code>separate_rows</code>.</p>
<p>Here we will work with the <code>applicants_cleaned</code> field in the pizza dataset. This field contains concatenated names with a <code>;</code> as the separator. For example, on lines 1_9 there are single applicant names or NA values. However, on lines 10 and line 59 we see:</p>
<div class="sourceCode" id="cb64"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb64-1" data-line-number="1">Ehrno Flexible A<span class="op">/</span>S; Stergaard, Ole</a>
<a class="sourceLine" id="cb64-2" data-line-number="2">Farrell Brian; Mcnulty John; Vishoot Lisa</a></code></pre></div>
<p>The problem here is that when we are dealing with thousands of lines of applicant names we don’t know how many names might be concatenated into each cell as a basis for separating the data into columns. Once we had split the columns (for example using Text to Columns in Excel) we would then need to work out how to gather the columns into rows. The <code>separate_rows()</code> function from <code>tidyr</code> makes light work of this problem. To use the function we name the dataset, the column we want to separate into rows and the separator (sep).</p>
<div class="sourceCode" id="cb65"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb65-1" data-line-number="1"><span class="kw">library</span>(dplyr)</a>
<a class="sourceLine" id="cb65-2" data-line-number="2"><span class="kw">library</span>(tidyr)</a>
<a class="sourceLine" id="cb65-3" data-line-number="3">pizza1 &lt;-<span class="st"> </span><span class="kw">separate_rows</span>(pizza_<span class="dv">1990</span>_<span class="dv">2012</span>, applicants_cleaned, <span class="dt">sep =</span> <span class="st">&quot;;&quot;</span>)</a>
<a class="sourceLine" id="cb65-4" data-line-number="4">pizza1</a></code></pre></div>
<pre><code>## # A tibble: 12,729 × 31
##    applicants_cleaned_type   applicants_organisations
##                      &lt;chr&gt;                      &lt;chr&gt;
## 1                   People                       &lt;NA&gt;
## 2                Corporate    Lazarillo De Tormes S L
## 3                   People                       &lt;NA&gt;
## 4                   People                       &lt;NA&gt;
## 5                Corporate           Frisco Findus Ag
## 6                Corporate Bicycle Tools Incorporated
## 7                   People                       &lt;NA&gt;
## 8                   People                       &lt;NA&gt;
## 9                   People                       &lt;NA&gt;
## 10       Corporate; People         Ehrno Flexible A/S
## # ... with 12,719 more rows, and 29 more variables:
## #   applicants_original &lt;chr&gt;, inventors_cleaned &lt;chr&gt;,
## #   inventors_original &lt;chr&gt;, ipc_class &lt;chr&gt;, ipc_codes &lt;chr&gt;,
## #   ipc_names &lt;chr&gt;, ipc_original &lt;chr&gt;, ipc_subclass_codes &lt;chr&gt;,
## #   ipc_subclass_detail &lt;chr&gt;, ipc_subclass_names &lt;chr&gt;,
## #   priority_country_code &lt;chr&gt;, priority_country_code_names &lt;chr&gt;,
## #   priority_data_original &lt;chr&gt;, priority_date &lt;chr&gt;,
## #   publication_country_code &lt;chr&gt;, publication_country_name &lt;chr&gt;,
## #   publication_date &lt;chr&gt;, publication_date_original &lt;chr&gt;,
## #   publication_day &lt;int&gt;, publication_month &lt;int&gt;,
## #   publication_number &lt;chr&gt;, publication_number_espacenet_links &lt;chr&gt;,
## #   pubyear &lt;int&gt;, title_cleaned &lt;chr&gt;, title_nlp_cleaned &lt;chr&gt;,
## #   title_nlp_multiword_phrases &lt;chr&gt;, title_nlp_raw &lt;chr&gt;,
## #   title_original &lt;chr&gt;, applicants_cleaned &lt;chr&gt;</code></pre>
<p>Our original dataset contained 8,262 rows. Our new dataset split on applicant names contains 12,729 rows. The function has moved our target column from column 1 to column 31 in the data frame. We can easily move it back to inspect.</p>
<div class="sourceCode" id="cb67"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb67-1" data-line-number="1"><span class="kw">library</span>(dplyr)</a>
<a class="sourceLine" id="cb67-2" data-line-number="2">pizza1 &lt;-<span class="st"> </span><span class="kw">select</span>(pizza1, <span class="dv">31</span>, <span class="dv">1</span><span class="op">:</span><span class="dv">30</span>)</a></code></pre></div>
<p><code>separate_rows()</code> has done a great job but one of the problems with concatenated names is extra white space around the separator. We will deal with this next.</p>
<p>###Trimming with <code>stringr</code></p>
<p>If we inspect the bottom of the column by subsetting into it using <code>$</code> we will see that a lot of the names have a leading whitespace space. This results from the separate exercise where the <code>;</code> is actually <code>;space</code>. Take a look at the last few rows of the data using <code>tail()</code>.</p>
<div class="sourceCode" id="cb68"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb68-1" data-line-number="1"><span class="kw">tail</span>(pizza1<span class="op">$</span>applicants_cleaned, <span class="dv">20</span>)</a></code></pre></div>
<pre><code>##  [1] &quot;Yahoo! Inc&quot;                       
##  [2] &quot;Clarcor Inc&quot;                      
##  [3] &quot;Holden Jeffrey A&quot;                 
##  [4] &quot; Vengroff Darren E&quot;               
##  [5] &quot;Casper Jeffrey L&quot;                 
##  [6] &quot; Erickson Braden J&quot;               
##  [7] &quot; Oppenheimer Alan A&quot;              
##  [8] &quot; Ray Madonna M&quot;                   
##  [9] &quot; Weber Jean L&quot;                    
## [10] &quot;Pandey Neena&quot;                     
## [11] &quot; Sharma Sudhanshu&quot;                
## [12] &quot; Verizon Patent And Licensing Inc&quot;
## [13] &quot;Pandey Neena&quot;                     
## [14] &quot; Sharma Sudhanshu&quot;                
## [15] &quot;Brown Michael&quot;                    
## [16] &quot; Urban Scott&quot;                     
## [17] &quot;Brown Michael&quot;                    
## [18] &quot; Urban Scott&quot;                     
## [19] &quot;Cole Lorin R&quot;                     
## [20] &quot; Middleton Scott W&quot;</code></pre>
<p>This is a big issue because any counts that we make later on using the applicants_cleaned field will treat “Oppenheimer Alan A” and &quot; Oppenheimer Alan A&quot; as separate names when they should be grouped together.</p>
<p>We can address this in a couple of ways. One approach is to recognise that actually our separator is not a simple <code>&quot;;&quot;</code> but <code>&quot;;space&quot;</code> in our call to <code>separate_rows()</code>. In that case the call to <code>separate_rows()</code> would actually be <code>sep = &quot;; &quot;</code>. We will add a line of code to illustrate the impact of this change.</p>
<div class="sourceCode" id="cb70"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb70-1" data-line-number="1">tmp &lt;-<span class="st"> </span><span class="kw">separate_rows</span>(pizza_<span class="dv">1990</span>_<span class="dv">2012</span>, applicants_cleaned, <span class="dt">sep =</span> <span class="st">&quot;; &quot;</span>)</a>
<a class="sourceLine" id="cb70-2" data-line-number="2"><span class="kw">tail</span>(tmp<span class="op">$</span>applicants_cleaned, <span class="dv">20</span>)</a></code></pre></div>
<pre><code>##  [1] &quot;Yahoo! Inc&quot;                       &quot;Clarcor Inc&quot;                     
##  [3] &quot;Holden Jeffrey A&quot;                 &quot;Vengroff Darren E&quot;               
##  [5] &quot;Casper Jeffrey L&quot;                 &quot;Erickson Braden J&quot;               
##  [7] &quot;Oppenheimer Alan A&quot;               &quot;Ray Madonna M&quot;                   
##  [9] &quot;Weber Jean L&quot;                     &quot;Pandey Neena&quot;                    
## [11] &quot;Sharma Sudhanshu&quot;                 &quot;Verizon Patent And Licensing Inc&quot;
## [13] &quot;Pandey Neena&quot;                     &quot;Sharma Sudhanshu&quot;                
## [15] &quot;Brown Michael&quot;                    &quot;Urban Scott&quot;                     
## [17] &quot;Brown Michael&quot;                    &quot;Urban Scott&quot;                     
## [19] &quot;Cole Lorin R&quot;                     &quot;Middleton Scott W&quot;</code></pre>
<p>Another way to address this, is to use the <code>str_trim()</code> function from the <code>stringr</code> package.</p>
<p>We can address this problem using a function from the <code>stringr</code> package <code>str_trim()</code>. We have a choice with <code>str_trim()</code> on whether to trim the whitespace on the right, left or both. Here we will choose both.</p>
<p>Because we are seeking to modify an existing column (not to create a new vector or data.frame) we will use <code>$</code> to select the column and as the data for the <code>str_trim()</code> function. That will apply the function to the applicants column in pizza1.</p>
<div class="sourceCode" id="cb72"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb72-1" data-line-number="1"><span class="kw">library</span>(stringr)</a>
<a class="sourceLine" id="cb72-2" data-line-number="2">pizza1<span class="op">$</span>applicants_cleaned &lt;-<span class="st"> </span><span class="kw">str_trim</span>(pizza1<span class="op">$</span>applicants_cleaned, <span class="dt">side =</span> <span class="st">&quot;both&quot;</span>)</a>
<a class="sourceLine" id="cb72-3" data-line-number="3"><span class="kw">tail</span>(pizza1<span class="op">$</span>applicants_cleaned, <span class="dv">20</span>)</a></code></pre></div>
<pre><code>##  [1] &quot;Yahoo! Inc&quot;                       &quot;Clarcor Inc&quot;                     
##  [3] &quot;Holden Jeffrey A&quot;                 &quot;Vengroff Darren E&quot;               
##  [5] &quot;Casper Jeffrey L&quot;                 &quot;Erickson Braden J&quot;               
##  [7] &quot;Oppenheimer Alan A&quot;               &quot;Ray Madonna M&quot;                   
##  [9] &quot;Weber Jean L&quot;                     &quot;Pandey Neena&quot;                    
## [11] &quot;Sharma Sudhanshu&quot;                 &quot;Verizon Patent And Licensing Inc&quot;
## [13] &quot;Pandey Neena&quot;                     &quot;Sharma Sudhanshu&quot;                
## [15] &quot;Brown Michael&quot;                    &quot;Urban Scott&quot;                     
## [17] &quot;Brown Michael&quot;                    &quot;Urban Scott&quot;                     
## [19] &quot;Cole Lorin R&quot;                     &quot;Middleton Scott W&quot;</code></pre>
<p>Note that when using <code>str_trim()</code> we use subsetting to modify the applicants column in place. There is possibly a more efficient way of doing this with pipes but this appears difficult because the data.frame needs to exist for <code>str_trim()</code> to act on in place or we end up with a vector of applicant names rather than a data.frame. A solution to this problem is provided on Stack Overflow<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>.</p>
<p>In practice, the most efficient solution in this case is to recognise that the separator for <code>separate_rows</code> is <code>&quot;;space&quot;</code>. However, that will not always be true making the tools in <code>stringr</code> invaluable. To learn more about string manipulation in R try <a href="http://r4ds.had.co.nz/strings.html">Chapter 14 of R for Data Science by Garrett Grolemund and Hadley Wickham</a>.</p>
<p>We can tie the steps so far together using pipes into the following simpler code that we will become the applicants table for use in the infographic. We will add a call to rename and rename applicants_cleaned to tidy up.</p>
<div class="sourceCode" id="cb74"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb74-1" data-line-number="1"><span class="kw">library</span>(dplyr)</a>
<a class="sourceLine" id="cb74-2" data-line-number="2"><span class="kw">library</span>(tidyr)</a>
<a class="sourceLine" id="cb74-3" data-line-number="3"><span class="kw">library</span>(stringr)</a>
<a class="sourceLine" id="cb74-4" data-line-number="4">applicants &lt;-<span class="st"> </span><span class="kw">rename</span>(pizza, <span class="dt">pubyear =</span> publication_year) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(pubyear <span class="op">&gt;=</span><span class="st"> </span></a>
<a class="sourceLine" id="cb74-5" data-line-number="5"><span class="st">    </span><span class="dv">1990</span>, pubyear <span class="op">&lt;=</span><span class="st"> </span><span class="dv">2012</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">separate_rows</span>(applicants_cleaned, <span class="dt">sep =</span> <span class="st">&quot;; &quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb74-6" data-line-number="6"><span class="st">    </span><span class="kw">rename</span>(<span class="dt">applicants =</span> applicants_cleaned) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">select</span>(<span class="dv">31</span>, <span class="dv">1</span><span class="op">:</span><span class="dv">30</span>)  <span class="co"># moves separated column to the beginning</span></a>
<a class="sourceLine" id="cb74-7" data-line-number="7">applicants</a></code></pre></div>
<pre><code>## # A tibble: 12,729 × 31
##                    applicants applicants_cleaned_type
##                         &lt;chr&gt;                   &lt;chr&gt;
## 1                        &lt;NA&gt;                  People
## 2     Lazarillo De Tormes S L               Corporate
## 3                        &lt;NA&gt;                  People
## 4          Depoortere, Thomas                  People
## 5            Frisco Findus Ag               Corporate
## 6  Bicycle Tools Incorporated               Corporate
## 7          Castiglioni, Carlo                  People
## 8                        &lt;NA&gt;                  People
## 9       Bujalski, Wlodzimierz                  People
## 10         Ehrno Flexible A/S       Corporate; People
## # ... with 12,719 more rows, and 29 more variables:
## #   applicants_organisations &lt;chr&gt;, applicants_original &lt;chr&gt;,
## #   inventors_cleaned &lt;chr&gt;, inventors_original &lt;chr&gt;, ipc_class &lt;chr&gt;,
## #   ipc_codes &lt;chr&gt;, ipc_names &lt;chr&gt;, ipc_original &lt;chr&gt;,
## #   ipc_subclass_codes &lt;chr&gt;, ipc_subclass_detail &lt;chr&gt;,
## #   ipc_subclass_names &lt;chr&gt;, priority_country_code &lt;chr&gt;,
## #   priority_country_code_names &lt;chr&gt;, priority_data_original &lt;chr&gt;,
## #   priority_date &lt;chr&gt;, publication_country_code &lt;chr&gt;,
## #   publication_country_name &lt;chr&gt;, publication_date &lt;chr&gt;,
## #   publication_date_original &lt;chr&gt;, publication_day &lt;int&gt;,
## #   publication_month &lt;int&gt;, publication_number &lt;chr&gt;,
## #   publication_number_espacenet_links &lt;chr&gt;, pubyear &lt;int&gt;,
## #   title_cleaned &lt;chr&gt;, title_nlp_cleaned &lt;chr&gt;,
## #   title_nlp_multiword_phrases &lt;chr&gt;, title_nlp_raw &lt;chr&gt;,
## #   title_original &lt;chr&gt;</code></pre>
<p>We will want to create a plot with the applicants data in our infographic software. For that we need to introduce a field to count on. We might also want to establish a cut off point based on the number of records per applicant.</p>
<p>In this code we will simply print the applicants ranked in descending order. The second to last line of the code provides a filter on the number of records. This value can be changed after inspecting the data. The final line omits NA values (otherwise the top result) where an applicant name is not available.</p>
<div class="sourceCode" id="cb76"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb76-1" data-line-number="1"><span class="kw">library</span>(tidyr)</a>
<a class="sourceLine" id="cb76-2" data-line-number="2"><span class="kw">library</span>(dplyr)</a>
<a class="sourceLine" id="cb76-3" data-line-number="3">applicant_count &lt;-<span class="st"> </span><span class="kw">select</span>(applicants, applicants, publication_number) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">n =</span> <span class="kw">sum</span>(<span class="dt">publication_number =</span> <span class="dv">1</span>)) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb76-4" data-line-number="4"><span class="st">    </span><span class="kw">count</span>(applicants, <span class="dt">wt =</span> n) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">arrange</span>(<span class="kw">desc</span>(nn)) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(nn <span class="op">&gt;=</span><span class="st"> </span><span class="dv">1</span>) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb76-5" data-line-number="5"><span class="st">    </span><span class="kw">na.omit</span>()</a>
<a class="sourceLine" id="cb76-6" data-line-number="6">applicant_count</a></code></pre></div>
<pre><code>## # A tibble: 6,178 × 2
##                              applicants    nn
##                                   &lt;chr&gt; &lt;dbl&gt;
## 1  Graphic Packaging International, Inc   154
## 2             Kraft Foods Holdings, Inc   132
## 3                            Google Inc   123
## 4                 Microsoft Corporation    88
## 5                 The Pillsbury Company    83
## 6                    General Mills, Inc    77
## 7                                Nestec    77
## 8          The Procter &amp; Gamble Company    59
## 9                        Pizza Hut, Inc    57
## 10                           Yahoo! Inc    54
## # ... with 6,168 more rows</code></pre>
<p>If we inspect applicant count using <code>View(applicant_count)</code> we have 6,178 rows. That is far too many to display in an infographic. So, next we will filter the data on the value for the top ten (54). Then we will write the data to a .csv file using the simple <code>write_csv()</code> from <code>readr</code>.</p>
<div class="sourceCode" id="cb78"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb78-1" data-line-number="1"><span class="kw">library</span>(dplyr)</a>
<a class="sourceLine" id="cb78-2" data-line-number="2"><span class="kw">library</span>(tidyr)</a>
<a class="sourceLine" id="cb78-3" data-line-number="3"><span class="kw">library</span>(readr)</a>
<a class="sourceLine" id="cb78-4" data-line-number="4">applicant_count &lt;-<span class="st"> </span><span class="kw">select</span>(applicants, applicants, publication_number) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">n =</span> <span class="kw">sum</span>(<span class="dt">publication_number =</span> <span class="dv">1</span>)) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb78-5" data-line-number="5"><span class="st">    </span><span class="kw">count</span>(applicants, <span class="dt">wt =</span> n) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">arrange</span>(<span class="kw">desc</span>(nn)) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(nn <span class="op">&gt;=</span><span class="st"> </span><span class="dv">54</span>) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb78-6" data-line-number="6"><span class="st">    </span><span class="kw">na.omit</span>()</a>
<a class="sourceLine" id="cb78-7" data-line-number="7">applicant_count</a></code></pre></div>
<pre><code>## # A tibble: 10 × 2
##                              applicants    nn
##                                   &lt;chr&gt; &lt;dbl&gt;
## 1  Graphic Packaging International, Inc   154
## 2             Kraft Foods Holdings, Inc   132
## 3                            Google Inc   123
## 4                 Microsoft Corporation    88
## 5                 The Pillsbury Company    83
## 6                    General Mills, Inc    77
## 7                                Nestec    77
## 8          The Procter &amp; Gamble Company    59
## 9                        Pizza Hut, Inc    57
## 10                           Yahoo! Inc    54</code></pre>
<div class="sourceCode" id="cb80"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb80-1" data-line-number="1"><span class="kw">write_csv</span>(applicant_count, <span class="st">&quot;pizza_applicants_1990_2012.csv&quot;</span>)</a></code></pre></div>
<p>When we inspect <code>applicant_count</code> we will see that Graphic Packaging International is the top result with 154 results with Google ranking third with 123 results followed by Microsoft. This could suggest that Google and Microsoft are suddenly entering the market for online pizza sales or pizza making software or, as is more likely, that there are uses other uses of the word pizza in patent data that we are not aware of.</p>
<p>As part of our infographic we will want to explore this intriguing result in more detail. We can do this by creating a subdataset for Google using <code>filter()</code>.</p>
<p>##Selecting applicants using <code>filter()</code></p>
<p>As we saw above, while <code>select()</code> functions with columns, <code>filter()</code> from <code>dplyr</code> works with rows. Here we will filter the data to select the rows in the applicants column that contain Google Inc. and then write that to a .csv for use in our infographic. Note the use of double <code>==</code> and the quotes around “Google Inc”.</p>
<div class="sourceCode" id="cb81"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb81-1" data-line-number="1"><span class="kw">library</span>(dplyr)</a>
<a class="sourceLine" id="cb81-2" data-line-number="2"><span class="kw">library</span>(readr)</a>
<a class="sourceLine" id="cb81-3" data-line-number="3">google &lt;-<span class="st"> </span><span class="kw">filter</span>(applicants, applicants <span class="op">==</span><span class="st"> &quot;Google Inc&quot;</span>)</a>
<a class="sourceLine" id="cb81-4" data-line-number="4">google</a></code></pre></div>
<pre><code>## # A tibble: 123 × 31
##    applicants applicants_cleaned_type applicants_organisations
##         &lt;chr&gt;                   &lt;chr&gt;                    &lt;chr&gt;
## 1  Google Inc       Corporate; People               Google Inc
## 2  Google Inc               Corporate               Google Inc
## 3  Google Inc       Corporate; People               Google Inc
## 4  Google Inc       Corporate; People               Google Inc
## 5  Google Inc               Corporate               Google Inc
## 6  Google Inc               Corporate               Google Inc
## 7  Google Inc               Corporate               Google Inc
## 8  Google Inc       Corporate; People               Google Inc
## 9  Google Inc               Corporate               Google Inc
## 10 Google Inc               Corporate               Google Inc
## # ... with 113 more rows, and 28 more variables:
## #   applicants_original &lt;chr&gt;, inventors_cleaned &lt;chr&gt;,
## #   inventors_original &lt;chr&gt;, ipc_class &lt;chr&gt;, ipc_codes &lt;chr&gt;,
## #   ipc_names &lt;chr&gt;, ipc_original &lt;chr&gt;, ipc_subclass_codes &lt;chr&gt;,
## #   ipc_subclass_detail &lt;chr&gt;, ipc_subclass_names &lt;chr&gt;,
## #   priority_country_code &lt;chr&gt;, priority_country_code_names &lt;chr&gt;,
## #   priority_data_original &lt;chr&gt;, priority_date &lt;chr&gt;,
## #   publication_country_code &lt;chr&gt;, publication_country_name &lt;chr&gt;,
## #   publication_date &lt;chr&gt;, publication_date_original &lt;chr&gt;,
## #   publication_day &lt;int&gt;, publication_month &lt;int&gt;,
## #   publication_number &lt;chr&gt;, publication_number_espacenet_links &lt;chr&gt;,
## #   pubyear &lt;int&gt;, title_cleaned &lt;chr&gt;, title_nlp_cleaned &lt;chr&gt;,
## #   title_nlp_multiword_phrases &lt;chr&gt;, title_nlp_raw &lt;chr&gt;,
## #   title_original &lt;chr&gt;</code></pre>
<div class="sourceCode" id="cb83"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb83-1" data-line-number="1"><span class="kw">write_csv</span>(google, <span class="st">&quot;google_1990_2012.csv&quot;</span>)</a></code></pre></div>
<p>Note that the correct result for the period 1990 to 2012 for Google is 123 records from 191 records across the whole pizza dataset. The correct result will be achieved only where you use the filtered, separated and trimmed data we created in the applicants data frame.</p>
<p>##Generating IPC Tables</p>
<p>In the next step we will want to generate two tables containing International Patent Classification (IPC) data. IPC codes and the Cooperative Patent Classification (CPC, not present in this dataset) provide information on the technologies involved in a patent document. The IPC is hierarchical and proceeds from the general class level to the detailed group and subgroup level. Experience reveals that the majority of patent documents receive more than one IPC code to more fully describe the technological aspects of patent documents.</p>
<p>The pizza dataset contains IPC codes on the class and the subclass level in concatenated fields. One important consideration in using IPC data is that the descriptions are long and can be difficult for non-specialists to grasp. This can make visualising the data difficult and often requires manual efforts to edit labels for display.</p>
<p>We now want to generate three IPC tables.</p>
<ol style="list-style-type: decimal">
<li>A general IPC table for the pizza dataset</li>
<li>A general IPC table for the Google dataset</li>
<li>A more detailed IPC subclass table for the Google dataset</li>
</ol>
<p>For ease of presentation in an infographic we will use the <code>ipc_class</code> field. For many patent analytics purposes this will be too general. However it has the advantage of being easy to visualise.</p>
<p>To generate the table we can use a generic function based on the code developed for dealing with the applicants data. We will call the function patent_count().</p>
<div class="sourceCode" id="cb84"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb84-1" data-line-number="1">patent_count &lt;-<span class="st"> </span><span class="cf">function</span>(data, <span class="dt">col =</span> <span class="st">&quot;&quot;</span>, <span class="dt">count_col =</span> <span class="st">&quot;&quot;</span>, <span class="dt">n_results =</span> n_results, </a>
<a class="sourceLine" id="cb84-2" data-line-number="2">    <span class="dt">sep =</span> <span class="st">&quot;[^[:alnum:]]+&quot;</span>) {</a>
<a class="sourceLine" id="cb84-3" data-line-number="3">    p_count &lt;-<span class="st"> </span>dplyr<span class="op">::</span><span class="kw">select_</span>(data, col, count_col) <span class="op">%&gt;%</span><span class="st"> </span>tidyr<span class="op">::</span><span class="kw">separate_rows_</span>(col, </a>
<a class="sourceLine" id="cb84-4" data-line-number="4">        <span class="dt">sep =</span> sep) <span class="op">%&gt;%</span><span class="st"> </span>dplyr<span class="op">::</span><span class="kw">mutate_</span>(<span class="dt">n =</span> <span class="kw">sum</span>(<span class="dt">count_col =</span> <span class="dv">1</span>)) <span class="op">%&gt;%</span><span class="st"> </span>dplyr<span class="op">::</span><span class="kw">select</span>(<span class="dv">2</span><span class="op">:</span><span class="dv">3</span>)</a>
<a class="sourceLine" id="cb84-5" data-line-number="5">    p_count <span class="op">%&gt;%</span><span class="st"> </span>dplyr<span class="op">::</span><span class="kw">group_by_</span>(col) <span class="op">%&gt;%</span><span class="st"> </span>dplyr<span class="op">::</span><span class="kw">tally</span>() <span class="op">%&gt;%</span><span class="st"> </span>dplyr<span class="op">::</span><span class="kw">arrange</span>(<span class="kw">desc</span>(nn)) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb84-6" data-line-number="6"><span class="st">        </span>dplyr<span class="op">::</span><span class="kw">rename</span>(<span class="dt">records =</span> nn) <span class="op">%&gt;%</span><span class="st"> </span>dplyr<span class="op">::</span><span class="kw">ungroup</span>() <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">na.omit</span>() <span class="op">%&gt;%</span><span class="st"> </span>.[<span class="dv">1</span><span class="op">:</span>n_results, </a>
<a class="sourceLine" id="cb84-7" data-line-number="7">        ]</a>
<a class="sourceLine" id="cb84-8" data-line-number="8">}</a></code></pre></div>
<p>The <code>patent_count()</code> function is based on the the code we developed for applicants. It contains variations to make it work as a function. The function takes four arguments:</p>
<ol style="list-style-type: decimal">
<li>col = the concatenated column that we want to split and gather back in</li>
<li>col_count = a column for generating counts (in this dataset the publication_number)</li>
<li>n_results = the number of results we want to see in the new table (typically 10 or 20 for visualisation). This is equivalent to the number of rows that you want to see.</li>
<li>sep = the separator to use to separate the data in col. With patent data this is almost always “;” (as <code>;space</code>.</li>
</ol>
<p>To generate the <code>ipc_class</code> data we can do the following and then write the file to .csv. Note that we have set the number of results <code>n_results</code> to 10.</p>
<div class="sourceCode" id="cb85"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb85-1" data-line-number="1">pizza_ipc_class &lt;-<span class="st"> </span><span class="kw">patent_count</span>(<span class="dt">data =</span> pizza_<span class="dv">1990</span>_<span class="dv">2012</span>, <span class="dt">col =</span> <span class="st">&quot;ipc_class&quot;</span>, <span class="dt">count_col =</span> <span class="st">&quot;publication_number&quot;</span>, </a>
<a class="sourceLine" id="cb85-2" data-line-number="2">    <span class="dt">n_results =</span> <span class="dv">10</span>, <span class="dt">sep =</span> <span class="st">&quot;;&quot;</span>)</a>
<a class="sourceLine" id="cb85-3" data-line-number="3">pizza_ipc_class</a></code></pre></div>
<pre><code>## # A tibble: 10 × 2
##                                               ipc_class records
##                                                   &lt;chr&gt;   &lt;dbl&gt;
## 1                                           A21: Baking    2218
## 2                                        G06: Computing    1209
## 3                              A23: Foods Or Foodstuffs    1058
## 4                                        B65: Conveying     903
## 5                              A23: Foods Or Foodstuffs     785
## 6                                        A47: Furniture     645
## 7                                        B65: Conveying     480
## 8   H05: Electric Techniques Not Otherwise Provided For     456
## 9                 H04: Electric Communication Technique     427
## 10                H04: Electric Communication Technique     320</code></pre>
<div class="sourceCode" id="cb87"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb87-1" data-line-number="1"><span class="kw">write_csv</span>(pizza_ipc_class, <span class="st">&quot;pizza_ipcclass_1990_2012.csv&quot;</span>)</a></code></pre></div>
<p>Note that this dataset is based on the main <code>pizza_1990_2012</code> dataset (including cases where no applicant name is available). The reason we have not used the applicants dataset is because that dataset will duplicate the IPC field for each split of an applicant name. As a result it will over count the IPCs by the number of applicants on a document name. As this suggests, it is important to be careful when working with data that has been tidied because of the impact on other counts.</p>
<p>This problem does not apply in the case of our Google data because the only applicant listed in that data is Google (excluding co-applicants). We can therefore safely use the Google dataset to identify the IPC codes.</p>
<div class="sourceCode" id="cb88"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb88-1" data-line-number="1">google_ipc_class &lt;-<span class="st"> </span><span class="kw">patent_count</span>(<span class="dt">data =</span> google, <span class="dt">col =</span> <span class="st">&quot;ipc_class&quot;</span>, <span class="dt">count_col =</span> <span class="st">&quot;publication_number&quot;</span>, </a>
<a class="sourceLine" id="cb88-2" data-line-number="2">    <span class="dt">n_results =</span> <span class="dv">10</span>, <span class="dt">sep =</span> <span class="st">&quot;;&quot;</span>)</a>
<a class="sourceLine" id="cb88-3" data-line-number="3">google_ipc_class</a></code></pre></div>
<pre><code>## # A tibble: 10 × 2
##                                 ipc_class records
##                                     &lt;chr&gt;   &lt;dbl&gt;
## 1                          G06: Computing      95
## 2                          G01: Measuring      14
## 3                          G09: Educating      11
## 4                          G06: Computing      10
## 5   H04: Electric Communication Technique      10
## 6   H04: Electric Communication Technique       7
## 7                G10: Musical Instruments       6
## 8                         G08: Signalling       1
## 9                G10: Musical Instruments       1
## 10                            A63: Sports       1</code></pre>
<div class="sourceCode" id="cb90"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb90-1" data-line-number="1"><span class="kw">write_csv</span>(google_ipc_class, <span class="st">&quot;google_ipcclass_1990_2012.csv&quot;</span>)</a></code></pre></div>
<p>There are only 7 classes and as we might expect they are dominated by computing. We might want to dig into this in a little more detail and so let’s also create an IPC subclass field.</p>
<div class="sourceCode" id="cb91"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb91-1" data-line-number="1">google_ipc_subclass &lt;-<span class="st"> </span><span class="kw">patent_count</span>(<span class="dt">data =</span> google, <span class="dt">col =</span> <span class="st">&quot;ipc_subclass_detail&quot;</span>, </a>
<a class="sourceLine" id="cb91-2" data-line-number="2">    <span class="dt">count_col =</span> <span class="st">&quot;publication_number&quot;</span>, <span class="dt">n_results =</span> <span class="dv">10</span>, <span class="dt">sep =</span> <span class="st">&quot;;&quot;</span>)</a>
<a class="sourceLine" id="cb91-3" data-line-number="3">google_ipc_subclass</a></code></pre></div>
<pre><code>## # A tibble: 10 × 2
##                                                            ipc_subclass_detail
##                                                                          &lt;chr&gt;
## 1                                       G06F: Electric Digital Data Processing
## 2                                G01C: Measuring Distances, Levels Or Bearings
## 3   G06Q: Data Processing Systems Or Methods, Specially Adapted For Administra
## 4  G06Q: Data Processing Systems Or Methods, Specially Adapted For Administrat
## 5                                G09B: Educational Or Demonstration Appliances
## 6                                       G06F: Electric Digital Data Processing
## 7                                        H04W: Wireless Communication Networks
## 8                                           G10L: Speech Analysis Or Synthesis
## 9   G09G: Arrangements Or Circuits For Control Of Indicating Devices Using Sta
## 10                                                          H04B: Transmission
## # ... with 1 more variables: records &lt;dbl&gt;</code></pre>
<div class="sourceCode" id="cb93"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb93-1" data-line-number="1"><span class="kw">write_csv</span>(google_ipc_subclass, <span class="st">&quot;google_ipcsubclass_1990_2012.csv&quot;</span>)</a></code></pre></div>
<p>We now have the data on technology areas that we need to understand our data. The next and final step is to generate data from the text fields.</p>
<p>###Phrases Tables</p>
<p>We will be using data from words and phrases in the titles of patent documents for use in a word cloud in our infographic. It is possible to generate this type of data in R directly using the <code>tm</code> and <code>NLP</code> packages. Our pizza dataset already contains a title field broken down into phrases using Vantagepoint software and so we will use that. We will use the field <code>title_nlp_multiword_phrases</code> as phrases are generally more informative than individual words. Once again we will use our general <code>patent_count()</code> function although experimentation may be needed to identify the number of phrases that visualise well in a word cloud.</p>
<div class="sourceCode" id="cb94"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb94-1" data-line-number="1">pizza_phrases &lt;-<span class="st"> </span><span class="kw">patent_count</span>(<span class="dt">data =</span> pizza_<span class="dv">1990</span>_<span class="dv">2012</span>, <span class="dt">col =</span> <span class="st">&quot;title_nlp_multiword_phrases&quot;</span>, </a>
<a class="sourceLine" id="cb94-2" data-line-number="2">    <span class="dt">count_col =</span> <span class="st">&quot;publication_number&quot;</span>, <span class="dt">n_results =</span> <span class="dv">15</span>, <span class="dt">sep =</span> <span class="st">&quot;;&quot;</span>)</a>
<a class="sourceLine" id="cb94-3" data-line-number="3">pizza_phrases</a></code></pre></div>
<pre><code>## # A tibble: 15 × 2
##    title_nlp_multiword_phrases records
##                          &lt;chr&gt;   &lt;dbl&gt;
## 1                 Food Product     135
## 2              Microwave Ovens      99
## 3                 Food Product      44
## 4                  Crust Pizza      41
## 5                conveyor Oven      40
## 6              Microwave Ovens      38
## 7               Bakery Product      34
## 8                  Making Same      33
## 9                Baked Product      33
## 10                   Cook Food      32
## 11                  Pizza Oven      30
## 12                   pizza Box      30
## 13              Related Method      29
## 14           Microwave Cooking      28
## 15           microwave Heating      27</code></pre>
<div class="sourceCode" id="cb96"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb96-1" data-line-number="1"><span class="kw">write_csv</span>(pizza_phrases, <span class="st">&quot;pizza_phrases_1990_2012.csv&quot;</span>)</a></code></pre></div>
<p>Now we do the same with the Google data.</p>
<div class="sourceCode" id="cb97"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb97-1" data-line-number="1">google_phrases &lt;-<span class="st"> </span><span class="kw">patent_count</span>(<span class="dt">data =</span> google, <span class="dt">col =</span> <span class="st">&quot;title_nlp_multiword_phrases&quot;</span>, </a>
<a class="sourceLine" id="cb97-2" data-line-number="2">    <span class="dt">count_col =</span> <span class="st">&quot;publication_number&quot;</span>, <span class="dt">n_results =</span> <span class="dv">15</span>, <span class="dt">sep =</span> <span class="st">&quot;;&quot;</span>)</a>
<a class="sourceLine" id="cb97-3" data-line-number="3">google_phrases</a></code></pre></div>
<pre><code>## # A tibble: 15 × 2
##             title_nlp_multiword_phrases records
##                                   &lt;chr&gt;   &lt;dbl&gt;
## 1                    Digital Map System      10
## 2  conversion Path Performance Measures       9
## 3                        Search Results       7
## 4                         Mobile Device       6
## 5                   Location Prominence       4
## 6                    Processing Queries       4
## 7                Geographical Relevance       4
## 8                  Local Search Results       4
## 9            Network Speech Recognizers       4
## 10                         Search Query       4
## 11                   indexing Documents       3
## 12        providing Profile Information       3
## 13          search Query Categorization       3
## 14                       Search Ranking       3
## 15 aspect-Based Sentiment Summarization       3</code></pre>
<div class="sourceCode" id="cb99"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb99-1" data-line-number="1"><span class="kw">write_csv</span>(google_phrases, <span class="st">&quot;google_phrases_1990_2012.csv&quot;</span>)</a></code></pre></div>
<p>We now have the following .csv files.</p>
<ol style="list-style-type: decimal">
<li><code>pizza_total_1990_2012</code></li>
<li><code>pizza_country_1990_2012</code></li>
<li><code>pizza_applicants_1990_2012</code></li>
<li><code>pizza_ipcclass_1990_2012</code></li>
<li><code>pizza_phrases_1990_2012</code></li>
<li><code>Google_1990_2012</code></li>
<li><code>Google_ipclass_1990_2012</code></li>
<li><code>Google_ipcsubclass_1990_2012</code></li>
<li><code>Google_phrases-1990_2012</code></li>
</ol>
<p>##Creating an infographic in infogr.am</p>
<p>If you are starting this chapter here then download the datasets we will be using as a single zip file from the Manual repository <a href="https://github.com/wipo-analytics/opensource-patent-analytics/blob/master/2_datasets/infographic/infographic.zip?raw=true">here</a> and then unzip the file.</p>
<p>We first need to sign up for a free account with <a href="https://infogr.am/">infogr.am</a></p>
<p><img src="images/infogram/fig1_infogram_front.png" /></p>
<p>We will then see a page with some sample infographics to provide ideas to get you started.</p>
<p><img src="images/infogram/fig2_infogram_login.png" /></p>
<p>Click on one of the infograms with a graph such as Trends in Something and then click inside the graph box itself and select the edit button in the top right.</p>
<p><img src="images/infogram/fig3_infogram_findedit.png" /></p>
<p>This will open up a data panel with the toy data displayed.</p>
<p><img src="images/infogram/fig4_infogram_datapanel.png" /></p>
<p>We want to replace this data by choosing the upload button and selecting our <code>pizza_country_1990_2012.csv</code> file.</p>
<p><img src="images/infogram/fig5_infogram_panelgraph.png" /></p>
<p>We now have a decent looking graph for our country trends data where we can see the number of records per country and year by hovering over the relevant data points. While some of the countries with low frequency data are crunched at the bottom (and would be better displayed in a separate graph), hovering over the data or over a country name will display the relevant country activity. We will therefore live with this.</p>
<p>We now want to start adding story elements by clicking on the edit button in the title. Next we can start adding new boxes using the menu icons on the right. Here we have changed the title, added a simple body text for the data credit and then a quote from someone describing themselves as the Head of Pizza Analytics.</p>
<p><img src="images/infogram/fig6_infogram_paneltext.png" /></p>
<p>Next we need to start digging into the data using our IPC, applicants and phrases data.</p>
<p>To work with our IPC class data we will add a bar chart and load the data. To do this select the graph icon in the right and then Bar. Once again we will choose edit and then load our <code>pizza_ipcclass_1990_2012</code> dataset. Then we can add a descriptive text box. We can then continue to add elements as follows:</p>
<ol style="list-style-type: decimal">
<li>applicants bar chart</li>
<li>pizza phrases by selecting graph and word cloud</li>
<li>Google ipc-subclass</li>
<li>Google word cloud.</li>
</ol>
<p>One useful approach to developing an infographic is to start by adding the images and then add titles and text boxes to raise key points. In infogram new text boxes appear below existing boxes but can be repositioned by dragging and dropping boxes onto each other.</p>
<p>One nice feature of infogram is that it is easy to share the infographic with others through a url, an embed code or on facebook or via twitter.</p>
<p>At the end of the infographic it is a good idea to provide a link where the reader can obtain more information, such as the full report or the underlying data. In this case we will add a link to the Tableau workbook on pizza patent activity that we developed in an earlier <a href="https://public.tableau.com/profile/wipo.open.source.patent.analytics.manual#!/vizhome/pizzapatents/Overview">chapter</a>.</p>
<p>Our final infographic should look something like <a href="https://infogr.am/trends_in_something">this</a>.</p>
<p>###Round Up</p>
<p>In this chapter we have concentrated on using R to tidy patent data in order to create an online infographic using free software. Using our trusty pizza patent data from WIPO Patentscope we walked through the process of wrangling and tidying patent data first using short lines of code that we then combined into a reusable function. As this introduction to tidying data in R has hopefully revealed, R and packages such as <code>dplyr</code>, <code>tidyr</code> and <code>stringr</code> provide very useful tools for working with patent data, and they are free and well supported.</p>
<p>In the final part of the chapter we used the data we had generated in RStudio to create an infographic using infogr.am that we then shared online. Infogram is just one of a number of online infographic services and it is well worth trying other services such as <a href="https://www.easel.ly">easel.ly</a> to find a service that meets your needs.</p>
<p>As regular users of R will already know, it is already possible to produce all of these graphics (such as word clouds) directly in R using tools such as <code>ggplot2</code>, <code>plotly</code> and word clouds using packages such as <code>wordcloud</code>. Some of these topics have been covered in other chapters and for more on text mining and word clouds in R see this recent article on <a href="http://www.r-bloggers.com/building-wordclouds-in-r/">R-bloggers</a>. None of the infographic services we viewed appeared to offer an API that would enable a direct connection with R. There also seems to be a gap in R’s packages where infographics might sit with this <a href="http://www.r-bloggers.com/r-how-to-layout-and-design-an-infographic/">2015 R-bloggers article</a> providing a walk through on how to create a basic infographic.</p>

</div>
</div>
</div>



<p>##Introduction</p>
<p>In this chapter we look at the use of the <a href="https://github.com/ropensci/rplos"><code>rplos</code></a> package from <a href="https://ropensci.org">rOpenSci</a> to access the scientific literature from the <a href="https://www.plos.org">Public Library of Science</a> using the <a href="http://api.plos.org/solr/faq/">PLOS Search API</a>.</p>
<p>The Public Library of Science (PLOS) is the main champion of open access peer reviewed scientific publications and has published somewhere in the region of 140,000 articles. These articles are a fantastic resource. PLOS includes the following titles.</p>
<ul>
<li>PLOS ONE</li>
<li>PLOS Biology</li>
<li>PLOS Medicine</li>
<li>PLOS Computational Biology</li>
<li>PLOS Genetics</li>
<li>PLOS Pathogens</li>
<li>PLOS Neglected Tropical Diseases</li>
<li>PLOS Clinical Trials ()</li>
<li>PLOS Collections (collections of articles)</li>
</ul>
<p>PLOS is important because it provides open access to the full text of peer reviewed research. For researchers interested in working with R, <code>rplos</code> and its bigger sister package, the <a href="https://ropensci.org/tutorials/fulltext_tutorial.html">rOpenSci <code>fulltext</code> package</a> are very important tools for gaining access to research.</p>
<p>This article is part of work in progress for the WIPO Manual on Open Source Patent Analytics. The Manual is intended to introduce open source analytics tools to patent researchers in developing countries and to be of wider use to the science and technology research community. An important part of patent research is being able to access and analyse the scientific literature.</p>
<p>This article makes no assumptions about knowledge of R or programming. <code>rplos</code> is a good place to start with learning how to access scientific literature in R using Application Programming Interfaces (APIs). Because <code>rplos</code> is well organised and the data is very clean it is also a good place to learn some of the basics of working with data in R. This provides a good basis for working with the ROpenSci <a href="https://github.com/ropensci/fulltext">fulltext package</a>. <code>fulltext</code> allows you to retrieve scientific literature from multiple data sources and we will deal with that next.</p>
<p>We will also use this as an opportunity to introduce some of the popular packages for working with data in R, notably the family of packages for tidying and wrangling data developed by Hadley Wickham at RStudio (namely, <code>plyr</code>, <code>dplyr</code>, <code>stringr</code> and <code>tidyr</code>). We will only touch on these but we include then as everyday working packages that you will find useful in learning more about R.</p>
<p>The first step is to make sure that you have R and RStudio.</p>
<p>##Install R and RStudio</p>
<p>To get up and running you need to install a version of R for your operating system. You can do that from <a href="http://cran.rstudio.com/">here</a>. Then download RStudio Desktop for your operating system from <a href="https://www.rstudio.com/products/rstudio/download/">here</a> using the installer for your system. Then open RStudio.</p>
<p>##Create A Project</p>
<p>Projects are probably the best way of organising your work in RStudio. To create a new project select the dropdown menu in to top right where you see the blue R icon. Navigate to where you want to keep your R materials and give your project a name (e.g. rplos). Now you will be able to save you work into an rplos project folder and R will keep everything together when you save the project.</p>
<p>##Install Packages</p>
<p>First we need to install some packages to help us work with the data. This list of packages are common “go to” packages for daily use.</p>
<div class="sourceCode" id="cb100"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb100-1" data-line-number="1"><span class="kw">install.packages</span>(<span class="st">&quot;rplos&quot;</span>)  <span class="co">#the main event</span></a>
<a class="sourceLine" id="cb100-2" data-line-number="2"><span class="kw">install.packages</span>(<span class="st">&quot;readr&quot;</span>)  <span class="co">#for reading data</span></a>
<a class="sourceLine" id="cb100-3" data-line-number="3"><span class="kw">install.packages</span>(<span class="st">&quot;plyr&quot;</span>)  <span class="co">#for wrangling data</span></a>
<a class="sourceLine" id="cb100-4" data-line-number="4"><span class="kw">install.packages</span>(<span class="st">&quot;dplyr&quot;</span>)  <span class="co">#for wrangling data</span></a>
<a class="sourceLine" id="cb100-5" data-line-number="5"><span class="kw">install.packages</span>(<span class="st">&quot;tidyr&quot;</span>)  <span class="co">#for tidying data</span></a>
<a class="sourceLine" id="cb100-6" data-line-number="6"><span class="kw">install.packages</span>(<span class="st">&quot;stringr&quot;</span>)  <span class="co">#for manipulating strings</span></a>
<a class="sourceLine" id="cb100-7" data-line-number="7"><span class="kw">install.packages</span>(<span class="st">&quot;tm&quot;</span>)  <span class="co">#for text mining</span></a>
<a class="sourceLine" id="cb100-8" data-line-number="8"><span class="kw">install.packages</span>(<span class="st">&quot;XML&quot;</span>)  <span class="co">#for dealing with text in xml</span></a></code></pre></div>
<p>Then we load the libraries. Note that <code>rplos</code> will install and load any other packages that it needs (in this case ggplot2 for graphing) so we don’t need to worry about that.</p>
<div class="sourceCode" id="cb101"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb101-1" data-line-number="1"><span class="kw">library</span>(rplos)</a>
<a class="sourceLine" id="cb101-2" data-line-number="2"><span class="kw">library</span>(readr)</a>
<a class="sourceLine" id="cb101-3" data-line-number="3"><span class="kw">library</span>(plyr)  <span class="co"># load before dplyr to avoid errors</span></a>
<a class="sourceLine" id="cb101-4" data-line-number="4"><span class="kw">library</span>(dplyr)</a>
<a class="sourceLine" id="cb101-5" data-line-number="5"><span class="kw">library</span>(tidyr)</a>
<a class="sourceLine" id="cb101-6" data-line-number="6"><span class="kw">library</span>(stringr)</a>
<a class="sourceLine" id="cb101-7" data-line-number="7"><span class="kw">library</span>(tm)</a>
<a class="sourceLine" id="cb101-8" data-line-number="8"><span class="kw">library</span>(XML)</a></code></pre></div>
<p>Next let’s take a look at the wide range of functions that are available for searching using <code>rplos</code> by moving over to the Packages tab in RStudio and clicking on <code>rplos</code>. A very useful tutorial on using <code>rplos</code> can be found <a href="https://ropensci.org/tutorials/rplos_tutorial.html">here</a> and can be cited as “Scott Chamberlain, Carl Boettiger and Karthik Ram (2015). rplos: Interface to PLOS Journals search API. R package version 0.5.0 <a href="https://github.com/ropensci/rplos" class="uri">https://github.com/ropensci/rplos</a>”. If you are already comfortable working in R you might want to head to that introductory tutorial as this article contains a lot more in the way of explanation. However, we will also add some new examples and code for working with the results to add to the resource base for <code>rplos</code>.</p>
<p>##Key functions in rplos</p>
<p>R is an object oriented language meaning that it works on objects such as a vector, table, list, or matrix. These are easy to create. We then apply functions to the data from <code>base R</code> or from packages we have installed for particular tasks.</p>
<ul>
<li><code>searchplos()</code>, the basic function for searching plos</li>
<li><code>plosauthor()</code>, search on author name</li>
<li><code>plostitle()</code>, search the title</li>
<li><code>plosabstract()</code>, search the abstract</li>
<li><code>plossubject()</code>, search by subject</li>
<li><code>citations()</code>, search the <a href="http://api.richcitations.org/">PLOS Rich Citations</a></li>
<li><code>plos_fulltext()</code>, retrieve full text using a DOI</li>
<li><code>highplos()</code>, highlight search terms in the results.</li>
<li><code>highbrow()</code>, browse search terms in a browser with hyperlinks.</li>
</ul>
<p>Functions in R take (accept) arguments which are options for the type of data we want to obtain when using an API or the calculations that we want to run on the data. For <code>rplos</code> we will mainly use arguments setting out our search query, the fields that we want to search, and the number of results.</p>
<p>If you are new to R this will typically takes the form of a short piece of code that is structured like this.</p>
<div class="sourceCode" id="cb102"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb102-1" data-line-number="1">newobject &lt;-<span class="st"> </span><span class="cf">function</span>(yourdata, argument1, argument2, etc)</a></code></pre></div>
<p>A new object is likely to be a table or list containing data. the sign <code>&lt;-</code> gets or passes the results of the function (such as seachplos) to the new object. To specify what we want we first include our data (<code>yourdata</code>) and then one or more arguments which control what we get, such as the number of records or the title etc.</p>
<p>##Data Fields in rplos</p>
<p>There are quite a number of fields that can be searched with <code>rplos</code> or used to refine a search. We will only use a few of them. To see the range of fields type <code>plosfields</code> into the console and press Enter.</p>
<div class="sourceCode" id="cb103"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb103-1" data-line-number="1">plosfields</a></code></pre></div>
<p>For example, if we wanted to search the title, abstract and conclusions we would use these fields in building the query (see below). If we wanted to search everything but those fields we would probably use body. If we wanted to retrieve the references then we would include <code>reference</code> in the fields. In <code>rplos</code> a field is denoted by <code>fl =</code> with the fields in quotes such as <code>fl = &quot;title&quot;</code> and so on as we will see below.</p>
<p>##Basic Searching using <code>searchplos()</code>, Navigating and Exporting Data</p>
<p><code>searchplos()</code> is the basic <code>rplos</code> search function and returns a list of document identifiers (DOIs) or other data fields. The basic search result is a set of DOIs that can be used for further work. To get help for a function, or to find working examples, use <code>?</code> in front of the function in the console:</p>
<div class="sourceCode" id="cb104"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb104-1" data-line-number="1"><span class="st">`</span><span class="dt">?</span><span class="st">`</span>(searchplos)</a></code></pre></div>
<p>This will bring up the help page for that function with a description of the arguments that are available and with examples at the bottom of the page.</p>
<p>The examples are there to help you. In <code>rplos</code> they presently focus on the use of single search terms such as ecology. However, as we will see below, it is possible to use phrases in searching and to use multiple terms. There are quite a number of arguments (options) available for refining the results and we will include some of these in the examples.</p>
<p>The author of this article is a big fan of pizza. So, in the first example we will carry out a simple search for the term pizza and then specify the results we want to see using the argument <code>fl =</code> (for fields) and the number of results that we want to see using <code>limit = 20</code>. In specifying the fields we will use <code>c()</code> to combine them together.</p>
<div class="sourceCode" id="cb105"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb105-1" data-line-number="1">p &lt;-<span class="st"> </span><span class="kw">searchplos</span>(<span class="dt">q =</span> <span class="st">&quot;pizza&quot;</span>, <span class="dt">fl =</span> <span class="kw">c</span>(<span class="st">&quot;id&quot;</span>, </a>
<a class="sourceLine" id="cb105-2" data-line-number="2">    <span class="st">&quot;publication_date&quot;</span>, <span class="st">&quot;title&quot;</span>, <span class="st">&quot;abstract&quot;</span>), </a>
<a class="sourceLine" id="cb105-3" data-line-number="3">    <span class="dt">limit =</span> <span class="dv">20</span>)</a>
<a class="sourceLine" id="cb105-4" data-line-number="4">p</a></code></pre></div>
<p>What <code>searchplos()</code> has done in the background is to send a request to the PLOS API to bring back the id, publication_date, title and abstract for 20 records across the PLOS journals. To see the results type:</p>
<div class="sourceCode" id="cb106"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb106-1" data-line-number="1">p</a></code></pre></div>
<p>Results in R are stored in objects (in this case the object is a list). To see the type of object in R use:</p>
<div class="sourceCode" id="cb107"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb107-1" data-line-number="1"><span class="kw">class</span>(p)</a></code></pre></div>
<p>When working with R it is generally more useful to understand the structure of the data so that you can work out how to access it. That can be done using <code>str()</code> for structure. This is one of the most useful functions in R and well worth writing down.</p>
<div class="sourceCode" id="cb108"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb108-1" data-line-number="1"><span class="kw">str</span>(p)</a></code></pre></div>
<p>The results might seem a little confusing at first but what this is telling us is that we have an R object that is a list consisting of two components. The first is an item called <code>meta</code> that reports the number of records found and the type of object (a data.frame). The second is <code>data</code> which contains the information on the two results in the form of a data frame (basically a table) containing the id, date, title and abstract information that we asked PLOS for.</p>
<p>Note that the list contains a marker <code>$</code> for the beginning of the two lists with the data they contain appearing as <code>..$</code> signifying that they are nested under <code>meta</code> or <code>data</code>. This hierarchy helps us with accessing the data using subsetting in R. For example, if we wanted to access the <code>meta</code> data (and we do) we can use the following:</p>
<div class="sourceCode" id="cb109"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb109-1" data-line-number="1">p<span class="op">$</span>meta</a></code></pre></div>
<p>That will just print the full <code>meta</code> data entries. If we wanted to just access the number of records (num$Found) then we would extend this a little by moving to that position in the hierarchy with:</p>
<div class="sourceCode" id="cb110"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb110-1" data-line-number="1">p<span class="op">$</span>meta<span class="op">$</span>numFound</a></code></pre></div>
<p>That will print out just the number of records returned by our search. An alternative way of subsetting is to use the “[” and “[[” and the numeric position in the list. In <a href="http://www.amazon.com/Hands-On-Programming-Write-Functions-Simulations/dp/1449359019">Hands on Programming with R</a> Garrett Grolemund compares this to a train with numbered carriages where “[]” selects the train carriage e.g. [1] and “[[1]]” selects the contents of carriage number 1. We don’t need to worry about this but it is very helpful as a way of remembering the difference. For example the following selects the contents of the first item in our list (<code>meta</code>):</p>
<div class="sourceCode" id="cb111"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb111-1" data-line-number="1">p[[<span class="dv">1</span>]]</a></code></pre></div>
<p>and is the same as <code>p$meta</code>. While:</p>
<div class="sourceCode" id="cb112"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb112-1" data-line-number="1">p[[<span class="dv">1</span>]][[<span class="dv">1</span>]]</a></code></pre></div>
<p>is the same as <code>p$meta$numFound</code>.</p>
<p>Subsetting the data by its numeric position rather than its name makes life much easier when working with lists with lots of items. As we will see below, when applying a function to a list with multiple items we can also use “[[”, 2. This will retrieve the second item in each of our line of train carriages.</p>
<p>Another useful tip for navigating the data in RStudio is using autocomplete. Try typing the following into the console.</p>
<p>p<span class="math inline">\(meta\)</span> #type me in the console, do not cut and paste</p>
<p>When we type the $ a popup will appear and display two entries as tables for <code>meta</code> and <code>data</code>. Click on meta, then add another $ sign at the end. It will now display three items in purple (for vectors). Select <code>numFound</code> and hey presto! As you work with RStudio you will notice that when you start to type a function name, lists of names will start to pop up. Type <code>search</code> into the console but do not press enter and wait a moment. A list with three items should pop up with search {base}, searchpaths {base}, and searchplos {rplos}. This is really helpful because it saves a lot of typing. As you become more familiar with R it also helpfully displays what a function does and a reminder of its arguments. The soft brackets around {base} indicate the package where the function can be found (this can be useful for discovering functions when you get stuck).</p>
<p>Finally, you can also see the items in your project in the Environment pane. Click on the blue arrow for <code>p</code> in the Environment pane under Values and you will see the structure of the data in <code>p</code> and some of its content.</p>
<p>###Creating a New Object and Writing to File</p>
<p>Ok so we have a list with some results containing <code>meta</code> and <code>data</code>. We now want to export <code>data</code> to a .csv file that we can work with in Excel or another programme.</p>
<p>While we will want to make a note of the total number of results in <code>meta</code>, what we really want will be in <code>data</code>. We can simply create a new object using the code above and assign it to a name using <code>&lt;-</code>. Note that there is no space here and <code>&lt; -</code> will not work.</p>
<div class="sourceCode" id="cb113"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb113-1" data-line-number="1">dat &lt;-<span class="st"> </span>p<span class="op">$</span>data</a></code></pre></div>
<p>If we look at the class of this object (<code>class(dat)</code>) we now have a data.frame (a table) that we can write to a .csv file to use later. We can do this easily using <code>write.csv()</code> and start by naming the object we want to write (<code>dat</code>) and then giving it a file name. Because we created an <code>rplos</code> project in RStudio earlier (didn’t we), the file will be saved into the project folder. If you didn’t create a project or want to check the directory then use:</p>
<div class="sourceCode" id="cb114"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb114-1" data-line-number="1"><span class="kw">getwd</span>()</a></code></pre></div>
<p>This will show your current working directory. If you do not see the name of your <code>rplos</code> project then copy the full file path so that it looks something like this (don’t forget the &quot;&quot; around the path):</p>
<div class="sourceCode" id="cb115"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb115-1" data-line-number="1"><span class="kw">setwd</span>(<span class="st">&quot;/Users/pauloldham/Desktop/open_source_master/rplos&quot;</span>)</a></code></pre></div>
<p>Ok, we now know where we are. So, let’s save the file.</p>
<div class="sourceCode" id="cb116"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb116-1" data-line-number="1"><span class="kw">write.csv</span>(dat, <span class="st">&quot;dat.csv&quot;</span>, <span class="dt">row.names =</span> <span class="ot">FALSE</span>)</a></code></pre></div>
<p>If we open this up in Excel or Open Office Calc then we will see two blank entries in the abstract fields. Blank cells can create calculation problems. Inside R we can handle this by filling in the blanks with NA as follows [2]. In this case we are subsetting into dat and then asking R to identify those cells that exactly match <code>==</code> with <code>&quot;&quot;</code>. We then fill those cells in dat with NA (for Not Available).</p>
<div class="sourceCode" id="cb117"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb117-1" data-line-number="1">dat[dat <span class="op">==</span><span class="st"> &quot;&quot;</span>] &lt;-<span class="st"> </span><span class="ot">NA</span></a>
<a class="sourceLine" id="cb117-2" data-line-number="2">dat</a></code></pre></div>
<p>We can then simply write the file as before. If we wanted to remove the NAs we have just introduced then we could use <code>write.csv(dat, &quot;dat.csv&quot;, row.names = FALSE, na = &quot;&quot;)</code> which will convert them back to blank spaces.</p>
<p>A faster way to deal with writing files is to use the recent <code>readr</code> package as this will not add row numbers to exported files. Here we will use the <code>write_csv()</code> function.</p>
<div class="sourceCode" id="cb118"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb118-1" data-line-number="1"><span class="kw">write_csv</span>(dat, <span class="st">&quot;dat.csv&quot;</span>)</a></code></pre></div>
<p>The advantage of <code>readr</code> is that it is fast and does not require the same number of arguments as the standard <code>write.csv</code> such as specifying row names or with <code>read.csv</code> specifying stringsAsFactors = FALSE.</p>
<p>Finally, if we wanted to write the entire list <code>p</code>, including <code>meta</code> to file then we could use:</p>
<div class="sourceCode" id="cb119"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb119-1" data-line-number="1"><span class="kw">write.csv</span>(p, <span class="st">&quot;p.csv&quot;</span>, <span class="dt">row.names =</span> <span class="ot">FALSE</span>)</a></code></pre></div>
<p>We have now retrieved some data containing pizza through the PLOS API using <code>rplos</code> and we have written the data to a file as a table we can use later. We will now move on to some more sophisticated things we can do with <code>rplos</code>.</p>
<p>##Limit by journal</p>
<p>As we have seen above, PLOS contains 7 journals and in <code>rplos</code> the results for a search can be limited to specific journals such as PLOS ONE or PLOS Biology. Note that the short journal names appear to use the old format for PLOS consisting of mixed upper and lowercase characters (e.g. PLoSONE not PLOSONE). A nice easy way to find the short journal names is to use:</p>
<div class="sourceCode" id="cb120"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb120-1" data-line-number="1"><span class="kw">journalnamekey</span>()</a></code></pre></div>
<p>Here we will limit the search to PLOS ONE by adding <code>fq =</code> to the arguments and then the <code>cross_published_journal_key</code> argument. Note that the <code>fq=</code> argument takes the same options as <code>fl=</code>. But, <code>fq =</code> filters the results returned by PLOS to only those specified in <code>fq =</code>.</p>
<div class="sourceCode" id="cb121"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb121-1" data-line-number="1">pizza &lt;-<span class="st"> </span><span class="kw">searchplos</span>(<span class="dt">q =</span> <span class="st">&quot;pizza&quot;</span>, <span class="dt">fl =</span> <span class="kw">c</span>(<span class="st">&quot;id&quot;</span>, <span class="st">&quot;publication_date&quot;</span>, <span class="st">&quot;title&quot;</span>, <span class="st">&quot;abstract&quot;</span>), <span class="dt">fq =</span> <span class="st">&#39;cross_published_journal_key:PLoSONE&#39;</span>, <span class="dt">start =</span> <span class="dv">0</span>, <span class="dt">limit =</span> <span class="dv">20</span>)</a>
<a class="sourceLine" id="cb121-2" data-line-number="2"><span class="kw">head</span>(pizza<span class="op">$</span>data)</a></code></pre></div>
<p>We have retrieved 20 records here using <code>limit = 20</code> (the default is 10). It is generally a good idea to start with a small number of results to test that we are getting what we expect back rather than lots of irrelevant data. What if we wanted to retrieve all of the results? Here we will need to do a bit more work using the meta field.</p>
<p>##Obtaining the full number of results</p>
<p>One way to do this is to take our original number of results and then subset in to the data and create a new object containing the value for the number of records in <code>numFound</code>. Note that the number of records for a particular query below may well have gone up by the time that you read this article.</p>
<div class="sourceCode" id="cb122"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb122-1" data-line-number="1">r &lt;-<span class="st"> </span>pizza<span class="op">$</span>meta<span class="op">$</span>numFound</a></code></pre></div>
<p>To run a new search we can now insert <code>r</code> into the limit = value. This will be interpreted as the numeric value of <code>r</code> (210).</p>
<div class="sourceCode" id="cb123"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb123-1" data-line-number="1">pizza &lt;-<span class="st"> </span><span class="kw">searchplos</span>(<span class="dt">q =</span> <span class="st">&quot;pizza&quot;</span>, <span class="dt">fl =</span> <span class="kw">c</span>(<span class="st">&quot;id&quot;</span>, <span class="st">&quot;publication_date&quot;</span>, <span class="st">&quot;title&quot;</span>, <span class="st">&quot;abstract&quot;</span>), </a>
<a class="sourceLine" id="cb123-2" data-line-number="2">    <span class="dt">fq =</span> <span class="st">&quot;cross_published_journal_key:PLoSONE&quot;</span>, <span class="dt">start =</span> <span class="dv">0</span>, <span class="dt">limit =</span> r)</a>
<a class="sourceLine" id="cb123-3" data-line-number="3"><span class="kw">head</span>(pizza<span class="op">$</span>data)</a></code></pre></div>
<p>An alternative way of doing this is to make life a bit easier for ourselves by first running our query and setting the limit as <code>limit = 0</code>. This will only return the <code>meta</code> data. We then add the subset for number found to the end of the code as <code>$meta$numFound</code>. That will pull back the value directly.</p>
<div class="sourceCode" id="cb124"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb124-1" data-line-number="1">r &lt;-<span class="st"> </span><span class="kw">searchplos</span>(<span class="dt">q =</span> <span class="st">&quot;pizza&quot;</span>, <span class="dt">fq =</span> <span class="st">&quot;cross_published_journal_key:PLoSONE&quot;</span>, <span class="dt">limit =</span> <span class="dv">0</span>)<span class="op">$</span>meta<span class="op">$</span>numFound</a>
<a class="sourceLine" id="cb124-2" data-line-number="2">r</a></code></pre></div>
<p>We can then run the query again using the value of <code>r</code> in limit = :</p>
<div class="sourceCode" id="cb125"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb125-1" data-line-number="1">pizza &lt;-<span class="st"> </span><span class="kw">searchplos</span>(<span class="dt">q =</span> <span class="st">&quot;pizza&quot;</span>, <span class="dt">fl =</span> <span class="kw">c</span>(<span class="st">&quot;id&quot;</span>, <span class="st">&quot;publication_date&quot;</span>, <span class="st">&quot;title&quot;</span>, <span class="st">&quot;abstract&quot;</span>), <span class="dt">fq =</span> <span class="st">&#39;cross_published_journal_key:PLoSONE&#39;</span>, <span class="dt">start =</span> <span class="dv">0</span>, <span class="dt">limit =</span> r) </a>
<a class="sourceLine" id="cb125-2" data-line-number="2"><span class="kw">head</span>(pizza<span class="op">$</span>data)</a></code></pre></div>
<p>##Obtaining the number of records across PLOS Journals</p>
<p>That has returned the full 210 results for PLOS ONE. We could attempt to make life even easier by first getting the results across all PLOS journals. We do this by removing the <code>fq =</code> argument limiting the data to PLOS ONE and saving the result in and object we will call <code>r1</code>. Note that the number of records will probably have gone up by the time you read this.</p>
<div class="sourceCode" id="cb126"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb126-1" data-line-number="1">r1 &lt;-<span class="st"> </span><span class="kw">searchplos</span>(<span class="st">&quot;pizza&quot;</span>, <span class="dt">limit =</span> <span class="dv">0</span>)<span class="op">$</span>meta<span class="op">$</span>numFound</a>
<a class="sourceLine" id="cb126-2" data-line-number="2">r1</a></code></pre></div>
<p>This produces 298 results at the time of writing. What happens now if we run our original query using the value of <code>r1</code> (298 records) but limiting the results only to PLOS ONE?</p>
<div class="sourceCode" id="cb127"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb127-1" data-line-number="1">pizza &lt;-<span class="st"> </span><span class="kw">searchplos</span>(<span class="dt">q =</span> <span class="st">&quot;pizza&quot;</span>, <span class="dt">fl =</span> <span class="kw">c</span>(<span class="st">&quot;id&quot;</span>, <span class="st">&quot;publication_date&quot;</span>, <span class="st">&quot;title&quot;</span>, <span class="st">&quot;abstract&quot;</span>), <span class="dt">fq =</span> <span class="st">&#39;cross_published_journal_key:PLoSONE&#39;</span>, <span class="dt">start =</span> <span class="dv">0</span>, <span class="dt">limit =</span> r1)</a>
<a class="sourceLine" id="cb127-2" data-line-number="2">pizza<span class="op">$</span>meta<span class="op">$</span>numFound</a></code></pre></div>
<p>The answer is that the 210 results in PLOS ONE are returned from the total of 244 across the PLOS journals. Why? The reason this works is that <code>searchplos()</code> initially pulls back all of the data from the PLOS API and then applies our entry in <code>fq =</code> as a filter. So, in reality the full 244 records are fetched and then filtered down to the 210 from PLOS ONE. In this case, this makes our lives easier because we can use the results across PLOS journals and then restrict the data.</p>
<p>##Writing the results and using a codebook</p>
<p>We now have a total of 210 results for pizza. We can simply write the results to a .csv file.</p>
<div class="sourceCode" id="cb128"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb128-1" data-line-number="1"><span class="kw">write.csv</span>(pizza, <span class="st">&quot;plosone_pizza.csv&quot;</span>, <span class="dt">row.names =</span> <span class="ot">FALSE</span>)</a></code></pre></div>
<p>As this illustrates, it is very easy to use <code>rplos()</code> and rapidly create a file that can be used for other purposes.</p>
<p>When working in R you will often create multiple tables and take multiple steps. To keep track of what you do it is a good idea to create a text file as a codebook. Use the codebook to note down the important steps you take. The idea of a codebook is taken from Jeffrey Leek’s <a href="https://leanpub.com/datastyle">Elements of Data Analytic Sytle</a> which provides a very accessible introduction to staying organised. To create a codebook in RStudio simply use <code>File &gt; New File &gt; Text File</code>. This will open a text file that can be saved with your project. The codebook allows you to recall what actions you performed on the data months or years later. It also allows others to follow and reproduce your results and is important for <a href="https://ropensci.org/blog/2014/06/09/reproducibility/">reproducible research</a>.</p>
<p>##Proximity Searching</p>
<p>We will typically want to carry out a search by first retrieving a rough working set of results to get a feel for the data and then experimenting until we are happy with the data to noise ratio (see this <a href="http://rsta.royalsocietypublishing.org/content/372/2031/20140065">article</a> for an example).</p>
<p>In thinking about ways to refine our search criteria we can also use proximity searching. Proximity searching focuses on the distance between words that we are interested in. To read more about this use <code>?searchplos</code> in the console and scroll down to example seven in the help list. We reproduce that example here using the words synthetic and biology as our terms.</p>
<p>We can set the proximity of terms using tilde <code>~</code> and a value. For example, <code>~15</code> will find instances of the terms synthetic and biology within 15 words of each other in the full texts of PLOS articles.</p>
<div class="sourceCode" id="cb129"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb129-1" data-line-number="1"><span class="kw">searchplos</span>(<span class="dt">q =</span> <span class="st">&quot;everything:</span><span class="ch">\&quot;</span><span class="st">synthetic biology</span><span class="ch">\&quot;</span><span class="st">~15&quot;</span>, <span class="dt">fl =</span> <span class="st">&quot;title&quot;</span>, <span class="dt">fq =</span> <span class="st">&quot;doc_type:full&quot;</span>)</a></code></pre></div>
<p>Note that while synthetic and biology appear inside quotes (suggesting they are a phrase to be searched) in reality the API will treat this as synthetic AND biology. That is, the query will look first for documents that contain the words synthetic AND biology and then for those cases where the words appear within 15 words of each other. In this case we get 1,684 results across PLOS (everything) and full texts (<code>fq = &quot;doc_type:full</code>) as we can see from this code.</p>
<div class="sourceCode" id="cb130"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb130-1" data-line-number="1"><span class="kw">searchplos</span>(<span class="dt">q =</span> <span class="st">&quot;everything:</span><span class="ch">\&quot;</span><span class="st">synthetic biology</span><span class="ch">\&quot;</span><span class="st">~15&quot;</span>, <span class="dt">fl =</span> <span class="st">&quot;title&quot;</span>, <span class="dt">fq =</span> <span class="st">&quot;doc_type:full&quot;</span>)<span class="op">$</span>meta<span class="op">$</span>numFound</a></code></pre></div>
<p>We can narrow the search horizon to ~1 to capture those cases where the terms appear next to each other (within 1 word either to the left or the right) which produces 1001 results.</p>
<div class="sourceCode" id="cb131"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb131-1" data-line-number="1"><span class="kw">searchplos</span>(<span class="dt">q =</span> <span class="st">&quot;everything:</span><span class="ch">\&quot;</span><span class="st">synthetic biology</span><span class="ch">\&quot;</span><span class="st">~1&quot;</span>, <span class="dt">fl =</span> <span class="st">&quot;title&quot;</span>, <span class="dt">fq =</span> <span class="st">&quot;doc_type:full&quot;</span>)<span class="op">$</span>meta<span class="op">$</span>numFound</a></code></pre></div>
<p>This is actually about 10 records higher than the total returned on an exact match for the phrase suggesting that there could be cases of “biology synthetic” or other issues (such as punctuation) or API performance that account for the variance. As noted in the <code>searchplos()</code> documentation:</p>
<p>“Don’t be surprised if queries you perform in a scripting language, like using rplos in R, give different results than when searching for articles on the PLOS website. I am not sure what exact defaults they use on their website.”</p>
<p>As a result, it is a good idea to try different approaches. Even if it is not possible to get to the bottom of any variance it is very useful to note it down in your codebook to highlight the issue to others who may try and repeat your work.</p>
<p>It is also important to emphasise that when using <code>rplos()</code> it is possible to return a fragment of the text with the highlighted terms using <code>highplos()</code> and the <code>hl.fragsize</code> argument to set the horizon for the fragment of text around the search. This is particularly useful for text mining.</p>
<p>In many cases the most useful information comes from searching using phrases and multiple terms. Unlike words, phrases can articulate concepts. This generally makes them more useful than single words for searching for information.</p>
<p>##Searching Using Multiple Phrases</p>
<p>To search by phrases we start by creating an object containing our phrases and put the phrases inside double quotation marks. If we do not use double quotation marks the search will look for documents containing both words rather than the complete phrase (e.g. synthetic AND biology rather than “synthetic biology”). Note that the code below will display &quot;&quot; as &quot;&quot; but you don’t need to enter the <code>\</code>.</p>
<p>We will use the search query developed in this <a href="http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0034368">PLOS ONE article on synthetic biology</a> in this example and retrieve the id, data, author, title and abstract across the PLOS journals.</p>
<p>First we create the search query. Note that we use <code>c()</code>, for combine, to combine the list of terms into a vector inside the object called <code>s</code>.</p>
<div class="sourceCode" id="cb132"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb132-1" data-line-number="1">s &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;</span><span class="ch">\&quot;</span><span class="st">synthetic biology</span><span class="ch">\&quot;</span><span class="st">&quot;</span>, <span class="st">&quot;</span><span class="ch">\&quot;</span><span class="st">synthetic genomics</span><span class="ch">\&quot;</span><span class="st">&quot;</span>, <span class="st">&quot;</span><span class="ch">\&quot;</span><span class="st">synthetic genome</span><span class="ch">\&quot;</span><span class="st">&quot;</span>, </a>
<a class="sourceLine" id="cb132-2" data-line-number="2">    <span class="st">&quot;</span><span class="ch">\&quot;</span><span class="st">synthetic genomes</span><span class="ch">\&quot;</span><span class="st">&quot;</span>)</a>
<a class="sourceLine" id="cb132-3" data-line-number="3">s</a></code></pre></div>
<p>We now want to get the maximum number of results returned by one of the search terms. This is slightly tricky because <code>rplos</code> will return a list containing four list items (one for each of our search terms). Each of those lists will contain <code>meta</code> and <code>data</code> items. What we want to do is find out which of the search terms returns the highest number of results inside <code>meta</code> in <code>numFound</code>. Then we can use that number as our limit.</p>
<p>This involves more than one step.</p>
<ol style="list-style-type: decimal">
<li>First we need to fetch the data.</li>
<li>Then we need to extract <code>meta</code> from each list.</li>
<li>Then we need to select <code>numFound</code> and find and return the maximum value across the lists of results.</li>
</ol>
<p>The easiest way to do this is to create a small function that we will call <code>plos_records</code>. To load the function into your Environment copy it and paste it into your console and press enter. The comments following <code>#</code> explain what is happening will be ignored when the function runs. When you have done this if you move over to Environment you will see <code>plos_records</code> under Functions.</p>
<div class="sourceCode" id="cb133"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb133-1" data-line-number="1">plos_records &lt;-<span class="st"> </span><span class="cf">function</span>(q) {</a>
<a class="sourceLine" id="cb133-2" data-line-number="2">  <span class="kw">library</span>(plyr) <span class="co">#for ldply</span></a>
<a class="sourceLine" id="cb133-3" data-line-number="3">  <span class="kw">library</span>(dplyr) <span class="co">#for pipes, select and filter</span></a>
<a class="sourceLine" id="cb133-4" data-line-number="4">    <span class="kw">lapply</span>(q, <span class="cf">function</span>(x) <span class="kw">searchplos</span>(x, <span class="dt">limit =</span> <span class="dv">0</span>)) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb133-5" data-line-number="5"><span class="st">    </span><span class="kw">ldply</span>(<span class="st">&quot;[[&quot;</span>, <span class="dv">1</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="co">#get meta from the lists</span></a>
<a class="sourceLine" id="cb133-6" data-line-number="6"><span class="st">    </span><span class="kw">select</span>(numFound) <span class="op">%&gt;%</span><span class="st"> </span><span class="co">#select numFound column of meta</span></a>
<a class="sourceLine" id="cb133-7" data-line-number="7"><span class="st">    </span><span class="kw">filter</span>(numFound <span class="op">==</span><span class="st"> </span><span class="kw">max</span>(numFound)) <span class="op">%&gt;%</span><span class="st"> </span><span class="co">#filter on max numFound</span></a>
<a class="sourceLine" id="cb133-8" data-line-number="8"><span class="st">      </span><span class="kw">print</span>() <span class="co">#print max value of numFound</span></a>
<a class="sourceLine" id="cb133-9" data-line-number="9">}</a></code></pre></div>
<p>Now we can run the following code using <code>s</code> as our query (q = s) in the function. If all goes well a result will be printed in the console with the maximum number of results. It can take a few moments for the results to come back from the API.</p>
<div class="sourceCode" id="cb134"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb134-1" data-line-number="1">r2 &lt;-<span class="st"> </span><span class="kw">plos_records</span>(<span class="dt">q =</span> s)</a>
<a class="sourceLine" id="cb134-2" data-line-number="2">r2</a></code></pre></div>
<p>You should now see a number around 1151 (at the time of writing). Yay!</p>
<p>Now we can use <code>r2</code> in the limit to return all of the records. We will write this in the standard way and then display a simpler way using pipes <code>%&gt;%</code> below. Note that we use <code>s</code> as our search terms (see <code>q = s</code>) and we have used <code>r2</code> for the limit (limit = r2). Because we are calling a chunk of data this can take around a minute to run.</p>
<p>Note that at each step in the code below we are creating and then overwriting an object called <code>results</code>. We are also naming <code>results</code> as the first argument in each step. This can take a few moments to run.</p>
<div class="sourceCode" id="cb135"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb135-1" data-line-number="1"><span class="kw">library</span>(plyr)</a>
<a class="sourceLine" id="cb135-2" data-line-number="2">results &lt;-<span class="st"> </span><span class="kw">lapply</span>(s, <span class="cf">function</span>(x) <span class="kw">searchplos</span>(x, <span class="dt">fl =</span> <span class="kw">c</span>(<span class="st">&#39;id&#39;</span>,<span class="st">&#39;author&#39;</span>, <span class="st">&#39;publication_date&#39;</span>, <span class="st">&#39;title&#39;</span>, <span class="st">&#39;abstract&#39;</span>), <span class="dt">limit =</span> r2))</a>
<a class="sourceLine" id="cb135-3" data-line-number="3">results &lt;-<span class="st"> </span><span class="kw">setNames</span>(results, s) <span class="co">#add query terms to the relevant results in the list</span></a>
<a class="sourceLine" id="cb135-4" data-line-number="4">results &lt;-<span class="st"> </span><span class="kw">ldply</span>(results, <span class="st">&quot;[[&quot;</span>, <span class="dv">2</span>) <span class="co">#extract the data into a single data.frame</span></a></code></pre></div>
<p>We can make life simpler by using pipes <code>%&gt;%</code> to simplify the code. The advantage of using pipes is that we do not have to keep creating and overwriting temporary objects (see above for <code>results</code>). The code is also much easier to read and faster. To learn more about using pipes see this article from <a href="http://seananderson.ca/2014/09/13/dplyr-intro.html">Sean Anderson</a>. Again the query might be a bit slow as the data is fetched back.</p>
<div class="sourceCode" id="cb136"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb136-1" data-line-number="1"><span class="kw">library</span>(plyr)</a>
<a class="sourceLine" id="cb136-2" data-line-number="2"><span class="kw">library</span>(dplyr)</a>
<a class="sourceLine" id="cb136-3" data-line-number="3">results &lt;-<span class="st"> </span><span class="kw">lapply</span>(s, <span class="cf">function</span>(x) <span class="kw">searchplos</span>(x, <span class="dt">fl =</span> <span class="kw">c</span>(<span class="st">&#39;id&#39;</span>, <span class="st">&#39;author&#39;</span>, <span class="st">&#39;publication_date&#39;</span>, <span class="st">&#39;title&#39;</span>, <span class="st">&#39;abstract&#39;</span>), <span class="dt">limit =</span> r2)) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb136-4" data-line-number="4"><span class="st">    </span><span class="kw">setNames</span>(s) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb136-5" data-line-number="5"><span class="st">    </span><span class="kw">ldply</span>(<span class="st">&quot;[[&quot;</span>, <span class="dv">2</span>)</a>
<a class="sourceLine" id="cb136-6" data-line-number="6">results</a></code></pre></div>
<p>Pipes are a relatively recent innovation in R (see the <code>magrittr</code>, <code>dplyr</code> and <code>tidyr</code> packages) and most code you will see will be written in the traditional way. However, pipes make R code faster and much easier to follow. While you will need to be familiar with regular R code to follow most existing work, pipes are becoming increasingly popular because the code is simpler and has a clearer logic (e.g. do this then that).</p>
<p>We now have our data consisting of 1,405 records in a single data frame that we can view.</p>
<div class="sourceCode" id="cb137"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb137-1" data-line-number="1"><span class="kw">View</span>(results)</a></code></pre></div>
<p>We could now simply write this to a .csv file. But there are a number of things that we might want to do first. Most of these tasks fall into the category of wrangling and tidying up data so that we can carry on working with it in R or other software such as Excel.</p>
<p>##Tidying and Organising the Data</p>
<p>Many useful data cleaning and organisational tasks can be easily performed using the <code>dplyr()</code> and <code>tidyr()</code> packages developed by Hadley Wickham at RStudio. Other important packages include <code>stringr()</code>(for working with text strings), <code>plyr()</code> and <code>reshape2()</code> (general wrangling) and <code>lubridate()</code> (for working with dates). These packages were developed by Hadley Wickham and colleagues with the specific aim of making it easier to work with data in R in a consistent way. We will mainly use <code>dplyr</code> and <code>tidyr</code> in the examples below and a very useful <a href="https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf">RStudio cheatsheet</a> can help you with working with <code>dplyr</code> and <code>tidyr</code>.</p>
<p>###Renaming a column</p>
<p>First we might want to tidy up by renaming a column. For example we might want to rename <code>.id</code> to something more meaningful. We can use <code>rename()</code> from <code>dplyr()</code> to do that (see <code>?rename</code>).</p>
<div class="sourceCode" id="cb138"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb138-1" data-line-number="1">results &lt;-<span class="st"> </span><span class="kw">rename</span>(results, <span class="dt">search_terms =</span> .id)</a>
<a class="sourceLine" id="cb138-2" data-line-number="2">results</a></code></pre></div>
<p>##Filling Blank Spaces</p>
<p>It is good practice to fill blank cells with NA for “Not Available”&quot; to avoid calculation problems. For example, as in the earlier example, we have some blank cells in the abstract field and there may be others somewhere else. Following this <a href="http://r.789695.n4.nabble.com/How-to-convert-blanks-to-NA-td895155.html">StackOverflow answer</a> we can do this easily.</p>
<div class="sourceCode" id="cb139"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb139-1" data-line-number="1">results[results <span class="op">==</span><span class="st"> &quot;&quot;</span>] &lt;-<span class="st"> </span><span class="ot">NA</span></a></code></pre></div>
<p>If for some reason we wanted to remove the NA values we can handle that at the time of exporting to a file (see above).</p>
<p>##Converting Dates</p>
<p>The <code>publication_date</code> field is a character vector. We can easily turn this into a Date format that can be used in R and drop the T00:00:00 for time information using:</p>
<div class="sourceCode" id="cb140"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb140-1" data-line-number="1">results<span class="op">$</span>publication_date &lt;-<span class="st"> </span><span class="kw">as.Date</span>(results<span class="op">$</span>publication_date)</a>
<a class="sourceLine" id="cb140-2" data-line-number="2"><span class="kw">head</span>(results<span class="op">$</span>publication_date)</a></code></pre></div>
<p>##Adding columns</p>
<p>When dealing with dates we might want to simply split the <code>publication_date</code> field into three columns for year, month and day. We can do that using <code>separate()</code> from <code>tidyr</code>.</p>
<div class="sourceCode" id="cb141"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb141-1" data-line-number="1">results &lt;-<span class="st"> </span><span class="kw">separate</span>(results, publication_date, <span class="kw">c</span>(<span class="st">&quot;year&quot;</span>, <span class="st">&quot;month&quot;</span>, <span class="st">&quot;day&quot;</span>), <span class="dt">sep =</span> <span class="st">&quot;-&quot;</span>, </a>
<a class="sourceLine" id="cb141-2" data-line-number="2">    <span class="dt">remove =</span> <span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb141-3" data-line-number="3"><span class="kw">head</span>(<span class="kw">select</span>(results, year, month, day))</a></code></pre></div>
<p>Here we have specified the data (results), the column we want to separate (results) and then the three new columns that we want to create by closing them in <code>c()</code> and placing them in quotes. This creates three new columns. The <code>remove</code> argument specifies whether we want to remove the original column (the default is TRUE) or keep it.</p>
<p>Because working with dates can be quite awkward (to put it mildly) it makes sense to have a range of options available to you early on in working with your data rather than having to go back to the beginning much later on.</p>
<p>##Add a count</p>
<p>One feature of pulling back literature from an API for scientific literature is that the fields tend to be character fields rather than numeric. Character vectors in R are quoted with &quot;&quot;. This can make life awkward if we want to start counting things later on. To add a count column we can use <code>mutate</code> from the <code>dplyr()</code> package to create a new column <code>number</code>. <code>number</code> is based on assigning the value 1 to the id columns using <code>mutate()</code>. We are avoiding the term count because it is the name of a function <code>count()</code>. There are other ways of doing this but this approach points to the very useful <code>mutate()</code> function in <code>dplyr</code> for adding a new variable.</p>
<div class="sourceCode" id="cb142"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb142-1" data-line-number="1"><span class="kw">library</span>(dplyr)</a>
<a class="sourceLine" id="cb142-2" data-line-number="2">results &lt;-<span class="st"> </span><span class="kw">mutate</span>(results, <span class="dt">number =</span> <span class="kw">sum</span>(<span class="dt">id =</span> <span class="dv">1</span>))</a>
<a class="sourceLine" id="cb142-3" data-line-number="3"><span class="kw">head</span>(<span class="kw">select</span>(results, title, number))</a></code></pre></div>
<p>When we view results we will now see a new column number that contains the value 1 for each entry.</p>
<p>##Remove a column</p>
<p>We will often end up with more data than we want, or create more columns than we need. The standard way to remove a column is to use the trusty <code>$</code> to select the column and assign it to NULL.</p>
<div class="sourceCode" id="cb143"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb143-1" data-line-number="1">results<span class="op">$</span>columnname &lt;-<span class="st"> </span><span class="ot">NULL</span>  <span class="co">#dummy example</span></a></code></pre></div>
<p>Another way of doing this, which can be used for multiple columns, is to use <code>select()</code> from <code>dplyr</code> (see <code>?select()</code>). Select will only keep the columns that we name. We can do this using the column names or position. For example the following will keep the first 8 columns (1:8) but will drop the unnamed 9th column because the default is to drop columns that are not named. We could also write out the column names but using the position numbers is faster in this case.</p>
<div class="sourceCode" id="cb144"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb144-1" data-line-number="1">test &lt;-<span class="st"> </span><span class="kw">select</span>(results, <span class="dv">1</span><span class="op">:</span><span class="dv">8</span>)</a>
<a class="sourceLine" id="cb144-2" data-line-number="2"><span class="kw">length</span>(test)</a></code></pre></div>
<p>We could also drop columns by position using the following (to remove column 5 and 6). This approach is useful when there are lots of columns to deal with.</p>
<div class="sourceCode" id="cb145"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb145-1" data-line-number="1">test &lt;-<span class="st"> </span><span class="kw">select</span>(results, <span class="dv">1</span><span class="op">:</span><span class="dv">4</span>, <span class="dv">7</span><span class="op">:</span><span class="dv">9</span>)</a></code></pre></div>
<p>An easier approach in this case is to explicitly drop columns using <code>-</code> and keep the others.</p>
<div class="sourceCode" id="cb146"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb146-1" data-line-number="1">test &lt;-<span class="st"> </span><span class="kw">select</span>(results, <span class="op">-</span>month, <span class="op">-</span>day)</a></code></pre></div>
<p>Select is also very useful for reordering columns. Let’s imagine that we wanted to move the <code>id</code> column to the first column. We can simply put <code>id</code> as the first entry in <code>select()</code> and then the total columns to reorder.</p>
<div class="sourceCode" id="cb147"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb147-1" data-line-number="1">test &lt;-<span class="st"> </span><span class="kw">select</span>(results, id, <span class="dv">1</span><span class="op">:</span><span class="dv">9</span>)</a></code></pre></div>
<p>The select function is incredibly useful for rapidly organising data as we will see below.</p>
<p>##Arranging the Data</p>
<p>We might want to arrange our rows (which can be quite difficult to do in base R). The <code>arrange()</code> function in <code>dplyr</code> makes this easy and arranges a column’s values in ascending order by default. Here we will specify descending <code>desc()</code> because we want to see the most recent publications that mention our search terms at the top.</p>
<div class="sourceCode" id="cb148"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb148-1" data-line-number="1">results &lt;-<span class="st"> </span><span class="kw">arrange</span>(results, <span class="kw">desc</span>(publication_date))</a>
<a class="sourceLine" id="cb148-2" data-line-number="2"><span class="kw">head</span>(results<span class="op">$</span>publication_date)</a></code></pre></div>
<p>When we use <code>View(results)</code> we will see that the most recent data is at the top. We will also see that some of the titles towards the top are duplicates of the same article because they include all the terms in our search. So, the next thing we will want to do is to address duplicates.</p>
<p>##Dealing with Duplicates</p>
<p>How you deal with duplicates depends on what you are trying to achieve. If you are attempting to develop data on trends then duplicates will result in overcounting unless you take steps to count only distinct records. Duplicates of the same data will also distort text mining of the frequencies of terms. So, from that perspective duplicates are bad. On the other hand. If we are interested in the use of terms over time within an emerging area of science and technology, then we might well want to look in detail at the use of particular terms. For example, synthetic genomics is an alternative term for synthetic biology favoured by the J. Craig Venter group. We could look at whether this term is more widely used. Do synthetic biologists also use terms such as engineering biology, genome engineering or the fashionable new genome editing technique? In these cases duplicate records using terms are good because shifts in language can be mapped over time. This suggests a need for a strategy that uses different data tables to answer different questions.</p>
<p>As we have already seen, it is very easy in R to create new objects (typically data.frames), take some kind of action, and write the data to a file. In thinking about duplicates we would probably first want to find out what we are dealing with by identifying unique records. There are multiple ways to do this, here are two:</p>
<div class="sourceCode" id="cb149"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb149-1" data-line-number="1"><span class="kw">unique</span>(results<span class="op">$</span>id)  <span class="co">#displays unique DOIs (base R)</span></a></code></pre></div>
<div class="sourceCode" id="cb150"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb150-1" data-line-number="1"><span class="kw">n_distinct</span>(results<span class="op">$</span>id)  <span class="co">#displays the count of distinct DOIs (dplyr)</span></a></code></pre></div>
<p>This tells us there are 1,098 unique DOIs meaning there were 307 duplicates at the time of writing.</p>
<p>Next we have two main options.</p>
<ol style="list-style-type: decimal">
<li>We can spread the duplicate results across the table</li>
<li>We can identify and delete the duplicates.</li>
</ol>
<p>###Spreading data using <code>spread()</code> from <code>tidyr</code></p>
<p>Rather than simply deleting our duplicate DOIs, we could create new columns for each search term and its associated DOI. This will be useful because it will tell us which terms are associated with which records over time. This is easy to do with <code>spread()</code> by providing a <code>key</code> and a <code>value</code> in the arguments. In this case, we want to use <code>search_terms</code> as the <code>key</code> (column names) to spread across the table and the DOIs in the <code>id</code> column as the <code>value</code> for the rows.</p>
<div class="sourceCode" id="cb151"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb151-1" data-line-number="1">spread_results &lt;-<span class="st"> </span><span class="kw">spread</span>(results, search_terms, id)</a></code></pre></div>
<p>This creates a column for each search term with the relevant DOIs as the values. Note that the default is to drop the original column (in this case <code>search_terms</code>) when creating the new columns. Things will go badly wrong if you try to keep the existing column because R will be simultaneously trying to spread the data, thus reducing the size of the table, and keep the table in the same size. So, we will leave the default to drop the column as is.</p>
<p>We now have a data.frame with 1098 rows and the search terms identified in each column. If we briefly inspect <code>spread_results</code> on the terms at the end we can detect a potentially interesting pattern where some documents are only using terms such as synthetic genome or synthetic genomics while others are using only synthetic biology or a mix of terms.</p>
<p>We have now reduced our data to unique records while preserving our search terms as reference points. The limitation of this approach is that by spreading the DOIs across 4 columns we no longer have a tidy single column of DOIs.</p>
<p>###Deleting Duplicates</p>
<p>As an alternative, or complement, to spread we can use a logical TRUE/FALSE test to filter our dataset. There are a number of functions that perform logical tests in R (see also <code>which()</code>, <code>%in%</code>, <code>within()</code>). In this case the most appropriate choice is probably <code>duplicated()</code>. <code>duplicated()</code> will mark duplicate records as TRUE and non-duplicated records as FALSE. We will add a column to our data using the trusty <code>$</code> when creating the new column.</p>
<div class="sourceCode" id="cb152"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb152-1" data-line-number="1">results<span class="op">$</span>duplicate &lt;-<span class="st"> </span><span class="kw">duplicated</span>(results<span class="op">$</span>id)</a></code></pre></div>
<p>If we use View(results) a new column will have been added to results. Records that are not duplicates are marked FALSE while records that are duplicates are marked TRUE. We now want to filter that table down to the results that are not duplicated (are FALSE) from our logical test. We will use <code>filter()</code> from <code>dplyr</code> (see above). While <code>select()</code> works exclusively with columns <code>filter()</code> works with rows and allows us to easily filter the data on the values contained in a row.</p>
<div class="sourceCode" id="cb153"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb153-1" data-line-number="1">unique_results &lt;-<span class="st"> </span><span class="kw">filter</span>(results, duplicate <span class="op">==</span><span class="st"> </span><span class="ot">FALSE</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb153-2" data-line-number="2"><span class="st">  </span><span class="kw">select</span>(<span class="op">-</span><span class="st"> </span>search_terms) </a>
<a class="sourceLine" id="cb153-3" data-line-number="3"><span class="co">#drop search_terms column</span></a></code></pre></div>
<p>Here we have asked <code>filter()</code> to show us only those values in the duplicate column that exactly match with FALSE. We now have a data from with 1097 unique results with the DOIs in one column.</p>
<p>The creation of logical TRUE/FALSE vectors is very useful in creating conditions to filter data. In this case however, in the process note that we will lose information from the <code>search_terms</code> column which will become incomplete. To avoid potential confusion later on we drop the <code>search_terms</code> column using <code>select(- search_terms)</code> in the code above. If we wanted to keep the terms we would use the spread method above.</p>
<p>We now have three data.frames, <code>results</code>, <code>spread_results</code>, and <code>unique_results</code>.</p>
<p><code>results</code> is our core or reference set. If we planned to do a significant amount of work with this data we would save a copy of <code>results</code> to .csv and label it as <code>raw</code> with notes in our codebook on its origins and the actions taken to generate it. It can be a good idea to <code>.zip</code> a raw file so that it is more difficult to access by accident.</p>
<p>Going forward we would use the <code>spread_results</code> and <code>unique_results</code> for further work.</p>
<p>As we did earlier, use either write.csv(x, “x.csv”, row.names = FALSE) or the simpler and faster <code>write_csv()</code>. R can write multiple files in a blink. This will write all three files to the rplos project folder (use <code>getwd()</code> and <code>setwd()</code> if you want to do something different).</p>
<div class="sourceCode" id="cb154"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb154-1" data-line-number="1"><span class="kw">write_csv</span>(results, <span class="st">&quot;results.csv&quot;</span>)</a>
<a class="sourceLine" id="cb154-2" data-line-number="2"><span class="kw">write_csv</span>(spread_results, <span class="st">&quot;spread_results.csv&quot;</span>)</a>
<a class="sourceLine" id="cb154-3" data-line-number="3"><span class="kw">write_csv</span>(unique_results, <span class="st">&quot;unique_results.csv&quot;</span>)</a></code></pre></div>
<p>Ok, so we have now a dataset containing the records for a range of terms and we have come a long way. Quite a lot of this has been about what to do with PLOS data once we have accessed it in terms of turning it into tables that we can work with. In the next section we will look at how to restrict searches by section.</p>
<p>##Restricting searches by section</p>
<p>The default for searching with <code>rplos</code> is to search everything. This can produce many passing results and be overwhelming. There are quite a number of options for restricting searches in <code>rplos</code>.</p>
<p>##By author</p>
<p>In creating the results dataset above we included the <code>author</code> field. However, there are some complexities to searching with author names and working with author data that it is important to understand. We will start by searching on author names and then look at how to process the data.</p>
<p>To restrict a search by author name we can use either the full name or the surname:</p>
<div class="sourceCode" id="cb155"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb155-1" data-line-number="1"><span class="kw">plosauthor</span>(<span class="dt">q =</span> <span class="st">&quot;Paul Oldham&quot;</span>, <span class="dt">fl =</span> <span class="kw">c</span>(<span class="st">&quot;author&quot;</span>, <span class="st">&quot;id&quot;</span>), <span class="dt">fq =</span> <span class="st">&quot;doc_type:full&quot;</span>, </a>
<a class="sourceLine" id="cb155-2" data-line-number="2">    <span class="dt">limit =</span> <span class="dv">20</span>)</a></code></pre></div>
<p>In this example we have specified <code>doc_type:full</code> to return only the results for full articles. If you do not use this then the search will return a large number of repeated results based on article sections. So, in this case, Paul Oldham - the author of this article on <code>rplos</code> - has published two articles in PLOS ONE. If <code>doc_type:full</code> isn’t specified more than 20 results are returned that display different sections of the two articles. This will create a duplication issue later on, so a sensible default approach is to use <code>doc_type:full</code>.</p>
<p>As a general observation, considerable caution should be exercised when working with author names because of problems with the lumping of names and splitting of names as described in this <a href="http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0070299">PLOS ONE article</a>. If a large number of results are encountered on a single author name consider using match criteria from other available data fields to ensure that separate persons are not being lumped together by name. Above all, do not assume that simply because a name is the same, or very similar to the target name, that the name designates the same person.</p>
<p>The next issue we need to address is what to do with the author data when we have retrieved it. The reason for this is that the author field in the results is generally a concatenated field containing the names of the authors of a particular article. We will start with the <code>oldham</code> results set.</p>
<p>In this case we will make the call to <code>plosauthor()</code> and then use <code>ldply()</code> from <code>plyr</code> to return a data frame containing <code>meta</code> and <code>data</code>. Then we will use <code>fill</code> from <code>tidyr</code> to take the <code>numFound</code> and fill down that column. We will remove the start column using <code>select()</code> and finally <code>filter()</code> to limit the table to data.</p>
<div class="sourceCode" id="cb156"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb156-1" data-line-number="1">oldham &lt;-<span class="st"> </span><span class="kw">plosauthor</span>(<span class="dt">q =</span> <span class="st">&quot;Paul Oldham&quot;</span>, <span class="dt">fl =</span> <span class="kw">c</span>(<span class="st">&quot;author&quot;</span>, <span class="st">&quot;id&quot;</span>), <span class="dt">fq =</span> <span class="st">&quot;doc_type:full&quot;</span>, <span class="dt">limit =</span> <span class="dv">20</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb156-2" data-line-number="2"><span class="st">  </span><span class="kw">ldply</span>(<span class="st">&quot;[&quot;</span>, <span class="dv">1</span><span class="op">:</span><span class="dv">2</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb156-3" data-line-number="3"><span class="st">  </span><span class="kw">fill</span>(numFound, start) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb156-4" data-line-number="4"><span class="st">  </span><span class="kw">select</span>(<span class="op">-</span><span class="st"> </span>start) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb156-5" data-line-number="5"><span class="st">  </span><span class="kw">filter</span>(.id <span class="op">==</span><span class="st"> &quot;data&quot;</span>)</a></code></pre></div>
<p>We now have a two records with the author and id (DOI) data. The next thing we want to do is to separate the author names out. We can do this using <code>separate()</code>. Note that <code>separate()</code> will need to know the number of names involved before hand. In the oldham data case there are three authors of each article. We will deal with how to calculate the number of author names shortly.</p>
<div class="sourceCode" id="cb157"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb157-1" data-line-number="1">oldham &lt;-<span class="st"> </span><span class="kw">separate</span>(oldham, author, <span class="dv">1</span><span class="op">:</span><span class="dv">3</span>, <span class="dt">sep =</span> <span class="st">&quot;;&quot;</span>, <span class="dt">remove =</span> <span class="ot">FALSE</span>)</a></code></pre></div>
<p>We now have some other choices. We could simply keep only the first author name. To do that, in this particular case, we could use <code>select()</code> and the numeric position of the columns that we want to remove.</p>
<div class="sourceCode" id="cb158"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb158-1" data-line-number="1">first_author &lt;-<span class="st"> </span><span class="kw">select</span>(oldham, <span class="dv">-7</span>, <span class="dv">-8</span>)</a></code></pre></div>
<p>As an alternative, we could place each author name on its own row so that we can focus in on a specific author later. For that we can use <code>gather()</code> from <code>tidyr</code> and the column position numbers (not their names in this case) of the columns we want to gather.</p>
<div class="sourceCode" id="cb159"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb159-1" data-line-number="1">authors &lt;-<span class="st"> </span><span class="kw">gather</span>(oldham, number, authors, <span class="dv">5</span><span class="op">:</span><span class="dv">7</span>)</a></code></pre></div>
<p>As above <code>gather()</code> requires a key and value field. In this case we have used the number as our key and authors as our value. We have then specified that we want to gather columns 6 to 8 into the new column authors.</p>
<p>That was easy because we are dealing with a small number of results with a uniform number of authors. However, our <code>results</code> data is more complicated than this because we have multiple author names for each article and the number of authors for the articles could vary considerably.</p>
<p>We will need to organise the data and to run some simple calculations to make this work. This will take six steps. The full working code is below.</p>
<ol style="list-style-type: decimal">
<li>We calculate the number of columns in our dataset. We do this because the number may vary depending on what fields we retrieve from <code>rplos</code>. We will use <code>ncols()</code> to make the calculation.</li>
<li>We use a short function from <code>stringr</code> to calculate the number of authors based on the author name separator “;” (+1 to capture the final names in the sequence). This gives us the maximum number of authors across the dataset that we need to split the data into (in this case 83 as the value of n). Copy and paste the function below into the console to access it.</li>
</ol>
<div class="sourceCode" id="cb160"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb160-1" data-line-number="1">author_count &lt;-<span class="st"> </span><span class="cf">function</span>(data, <span class="dt">col =</span> <span class="st">&quot;&quot;</span>, <span class="dt">sep =</span> <span class="st">&quot;[^[:alnum:]]+&quot;</span>) {</a>
<a class="sourceLine" id="cb160-2" data-line-number="2">    <span class="kw">library</span>(stringr)</a>
<a class="sourceLine" id="cb160-3" data-line-number="3">    authcount &lt;-<span class="st"> </span><span class="kw">str_count</span>(data[[col]], <span class="dt">pattern =</span> sep)</a>
<a class="sourceLine" id="cb160-4" data-line-number="4">    n &lt;-<span class="st"> </span><span class="kw">as.integer</span>(<span class="kw">max</span>(authcount) <span class="op">+</span><span class="st"> </span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb160-5" data-line-number="5">    <span class="kw">print</span>(n)</a>
<a class="sourceLine" id="cb160-6" data-line-number="6">}</a></code></pre></div>
<ol start="3" style="list-style-type: decimal">
<li>We use <code>select()</code> from <code>dplyr</code> to move our target column to the first column. This simply makes it easier to specify column positions in <code>separate()</code> and <code>gather()</code> later on.</li>
<li>We use the value of <code>n</code> to separate the author names into multiple columns</li>
<li>We then gather them back in using the value of <code>n</code>.</li>
<li>Splitting on a separator such as <code>;</code> normally generates invisible leading and trailing white space. This will prevent author names from ranking correctly (e.g. in Excel or Tableau). The <code>str_trim()</code> function from <code>stringr</code> provides an easy way of removing the white space (specify side as right, left or both).</li>
</ol>
<p>Copy and paste the code below and then hit Enter.</p>
<div class="sourceCode" id="cb161"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb161-1" data-line-number="1"><span class="co">#---calculations---</span></a>
<a class="sourceLine" id="cb161-2" data-line-number="2">colno &lt;-<span class="st"> </span><span class="kw">ncol</span>(unique_results)  <span class="co">#calculate number of columns</span></a>
<a class="sourceLine" id="cb161-3" data-line-number="3">n &lt;-<span class="st"> </span><span class="kw">author_count</span>(unique_results, <span class="st">&quot;author&quot;</span>, <span class="st">&quot;;&quot;</span>)  <span class="co"># See function above. Calculate n as an integer to meet requirement for separate()</span></a>
<a class="sourceLine" id="cb161-4" data-line-number="4"><span class="co">#---select, separate and gather---</span></a>
<a class="sourceLine" id="cb161-5" data-line-number="5">full_authors &lt;-<span class="st"> </span><span class="kw">select</span>(unique_results, author, <span class="dv">1</span><span class="op">:</span>colno)  <span class="co">#bring author to the front</span></a>
<a class="sourceLine" id="cb161-6" data-line-number="6">full_authors &lt;-<span class="st"> </span><span class="kw">separate</span>(full_authors, author, <span class="dv">1</span><span class="op">:</span>n, <span class="dt">sep =</span> <span class="st">&quot;;&quot;</span>, <span class="dt">remove =</span> <span class="ot">TRUE</span>, </a>
<a class="sourceLine" id="cb161-7" data-line-number="7">    <span class="dt">convert =</span> <span class="ot">FALSE</span>, <span class="dt">extra =</span> <span class="st">&quot;merge&quot;</span>, <span class="dt">fill =</span> <span class="st">&quot;right&quot;</span>)  <span class="co">#separate</span></a>
<a class="sourceLine" id="cb161-8" data-line-number="8">full_authors &lt;-<span class="st"> </span><span class="kw">gather</span>(full_authors, value, authors, <span class="dv">1</span><span class="op">:</span>n, <span class="dt">na.rm =</span> <span class="ot">TRUE</span>)  <span class="co">#gather</span></a>
<a class="sourceLine" id="cb161-9" data-line-number="9"><span class="co">#---trim authors----</span></a>
<a class="sourceLine" id="cb161-10" data-line-number="10">full_authors<span class="op">$</span>authors &lt;-<span class="st"> </span><span class="kw">str_trim</span>(full_authors<span class="op">$</span>authors, <span class="dt">side =</span> <span class="st">&quot;both&quot;</span>)  <span class="co">#trim leading and trailing whitespace</span></a></code></pre></div>
<p>We can simplify this with pipes to bring together the actions on the new full_authors object.</p>
<div class="sourceCode" id="cb162"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb162-1" data-line-number="1"><span class="co">#---calculations---</span></a>
<a class="sourceLine" id="cb162-2" data-line-number="2">colno &lt;-<span class="st"> </span><span class="kw">ncol</span>(unique_results)</a>
<a class="sourceLine" id="cb162-3" data-line-number="3">n &lt;-<span class="st"> </span><span class="kw">author_count</span>(unique_results, <span class="st">&quot;author&quot;</span>, <span class="st">&quot;;&quot;</span>)</a>
<a class="sourceLine" id="cb162-4" data-line-number="4"><span class="co">#---select, separate, gather---</span></a>
<a class="sourceLine" id="cb162-5" data-line-number="5">full_authors &lt;-<span class="st"> </span><span class="kw">select</span>(unique_results, author, <span class="dv">1</span><span class="op">:</span>colno) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">separate</span>(author, </a>
<a class="sourceLine" id="cb162-6" data-line-number="6">    <span class="dv">1</span><span class="op">:</span>n, <span class="dt">sep =</span> <span class="st">&quot;;&quot;</span>, <span class="dt">remove =</span> <span class="ot">TRUE</span>, <span class="dt">convert =</span> <span class="ot">FALSE</span>, <span class="dt">extra =</span> <span class="st">&quot;merge&quot;</span>, <span class="dt">fill =</span> <span class="st">&quot;right&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb162-7" data-line-number="7"><span class="st">    </span><span class="kw">gather</span>(value, authors, <span class="dv">1</span><span class="op">:</span>n, <span class="dt">na.rm =</span> <span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb162-8" data-line-number="8"><span class="co">#---trim authors----</span></a>
<a class="sourceLine" id="cb162-9" data-line-number="9">full_authors<span class="op">$</span>authors &lt;-<span class="st"> </span><span class="kw">str_trim</span>(full_authors<span class="op">$</span>authors, <span class="dt">side =</span> <span class="st">&quot;both&quot;</span>)</a></code></pre></div>
<p>In running this code we will remove the original author column (column 1) by specifying <code>remove = TRUE</code> in <code>separate()</code>. <code>gather()</code> will place the new <code>authors</code> column at the end. So, make sure you scroll to the final column when viewing the results. We could also drop unwanted columns.</p>
<p>We now have a complete list of individual author names that could be used to look up individual authors, to clean up author names for statistical use and for author network mapping. As a brief example, if we wanted to look up contributions by Jean Peccoud who leads the <a href="http://blogs.plos.org/synbio/">PLOS SynBio blog</a> we might use the following based on this useful <a href="http://stackoverflow.com/questions/22850026/filtering-row-which-contains-a-certain-string-using-dplyr">Stack Overflow answer</a>. See ?grepl for more info.</p>
<div class="sourceCode" id="cb163"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb163-1" data-line-number="1">Peccoud &lt;-<span class="st"> </span><span class="kw">filter</span>(full_authors, <span class="kw">grepl</span>(<span class="st">&quot;Peccoud&quot;</span>, authors))</a></code></pre></div>
<p>We will not go into depth on these topics, but generating this type of author list is an important step in enabling wider analytics and visualisation. While the code used to get to this list of authors may appear quite involved, once the basics are understood it can be used over and over again.</p>
<p>Let’s write that data to a .csv file to explore later.</p>
<div class="sourceCode" id="cb164"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb164-1" data-line-number="1"><span class="kw">write_csv</span>(full_authors, <span class="st">&quot;full_authors.csv&quot;</span>)</a></code></pre></div>
<p>##Title search using <code>plostitle()</code></p>
<p>For a title search we can use <code>plostitle()</code>. As above you may want to count the number of records first using:</p>
<div class="sourceCode" id="cb165"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb165-1" data-line-number="1">t &lt;-<span class="st"> </span><span class="kw">plostitle</span>(<span class="dt">q =</span> <span class="st">&quot;synthetic biology&quot;</span>, <span class="dt">limit =</span> <span class="dv">0</span>)<span class="op">$</span>meta<span class="op">$</span>numFound</a></code></pre></div>
<p>Then we run the search to return the number of results we would like. Here we have set it to the value of t above (11). We have limited the results to the data field by subsetting with $data.</p>
<div class="sourceCode" id="cb166"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb166-1" data-line-number="1">title &lt;-<span class="st"> </span><span class="kw">plostitle</span>(<span class="dt">q =</span> <span class="st">&quot;synthetic biology&quot;</span>, <span class="dt">fl =</span> <span class="st">&quot;title&quot;</span>, <span class="dt">limit =</span> t)<span class="op">$</span>data</a></code></pre></div>
<p>##Abstract search using <code>plosabstract()</code></p>
<p>For confining the searches to abstracts we can use <code>plosabstract()</code>. We will start with a quick count of records.</p>
<div class="sourceCode" id="cb167"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb167-1" data-line-number="1">a &lt;-<span class="st"> </span><span class="kw">plosabstract</span>(<span class="dt">q =</span> <span class="st">&quot;synthetic biology&quot;</span>, <span class="dt">limit =</span> <span class="dv">0</span>)<span class="op">$</span>meta<span class="op">$</span>numFound</a></code></pre></div>
<p>To retrieve the results we could use the value of <code>a</code>. As an alternative we could set it arbitrarily high and the correct results will be returned. Of course if we don’t know what the total number of results are then we will be unsure whether we have captured the universe. But, an arbitrary number can be useful for exploration.</p>
<div class="sourceCode" id="cb168"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb168-1" data-line-number="1">abstract &lt;-<span class="st"> </span><span class="kw">plosabstract</span>(<span class="dt">q =</span> <span class="st">&quot;synthetic biology&quot;</span>, <span class="dt">fl =</span> <span class="st">&quot;id, title, abstract&quot;</span>, </a>
<a class="sourceLine" id="cb168-2" data-line-number="2">    <span class="dt">limit =</span> <span class="dv">200</span>)</a>
<a class="sourceLine" id="cb168-3" data-line-number="3">abstract<span class="op">$</span>data</a></code></pre></div>
<p>As before, we can easily create a new object containing the data.frame. In this case we will also include the meta data and then use <code>fill()</code> from <code>tidyr()</code> to fill down the <code>numFound</code> field and the start with 0. Note that <code>meta</code> will appear at the top of the list and will create a largely blank row. To avoid this, while keeping number of records for reference, we will use filter from <code>tidyr()</code>. This short code will do that.</p>
<div class="sourceCode" id="cb169"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb169-1" data-line-number="1">abstract_df &lt;-<span class="st"> </span><span class="kw">ldply</span>(abstract, <span class="st">&quot;[&quot;</span>, <span class="dv">1</span><span class="op">:</span><span class="dv">2</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">fill</span>(numFound, start) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(.id <span class="op">==</span><span class="st"> </span></a>
<a class="sourceLine" id="cb169-2" data-line-number="2"><span class="st">    &quot;data&quot;</span>)</a></code></pre></div>
<p>##Subject Area using <code>plossubject()</code></p>
<p>To search by subject area use <code>plossubject</code>. The default return is 10 results of the total results. So, try starting with a search such as this to get an idea of how many results there are. In this case the query has been limited to PLOS ONE and full text articles.</p>
<div class="sourceCode" id="cb170"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb170-1" data-line-number="1">sa &lt;-<span class="st"> </span><span class="kw">plossubject</span>(<span class="dt">q =</span> <span class="st">&quot;</span><span class="ch">\&quot;</span><span class="st">synthetic+biology</span><span class="ch">\&quot;</span><span class="st">&quot;</span>, <span class="dt">fq =</span> <span class="kw">list</span>(<span class="st">&quot;cross_published_journal_key:PLoSONE&quot;</span>, </a>
<a class="sourceLine" id="cb170-2" data-line-number="2">    <span class="st">&quot;doc_type:full&quot;</span>))<span class="op">$</span>meta<span class="op">$</span>numFound</a></code></pre></div>
<p>At the time of writing this returns 739 results. We will simply pull back 10 results. To pull back all of the results replace 10 with <code>sa</code> above or type the number into <code>limit =</code>.</p>
<div class="sourceCode" id="cb171"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb171-1" data-line-number="1"><span class="kw">plossubject</span>(<span class="dt">q =</span> <span class="st">&quot;</span><span class="ch">\&quot;</span><span class="st">synthetic+biology</span><span class="ch">\&quot;</span><span class="st">&quot;</span>, <span class="dt">fl =</span> <span class="st">&quot;id&quot;</span>, <span class="dt">fq =</span> <span class="kw">list</span>(<span class="st">&quot;cross_published_journal_key:PLoSONE&quot;</span>, </a>
<a class="sourceLine" id="cb171-2" data-line-number="2">    <span class="st">&quot;doc_type:full&quot;</span>), <span class="dt">limit =</span> <span class="dv">10</span>)</a></code></pre></div>
<p>As noted in the documentation, the results we return from the API and the results on the website are not necessarily the same because the settings used by PLOS on the website are not clear.</p>
<p>In this case we return 740 results while, at the time of writing, PLOS ONE lists 417 articles in the <a href="http://www.plosone.org/browse/Synthetic+biology?startPage=0&amp;filterAuthors=&amp;filterSubjectsDisjunction=&amp;filterArticleTypes=&amp;pageSize=13&amp;filterKeyword=&amp;filterJournals=PLoSONE&amp;query=&amp;ELocationId=&amp;id=&amp;resultView=&amp;sortValue=&amp;unformattedQuery=*%3A*&amp;sortKey=Most+views%2C+all+time&amp;filterSubjects=Synthetic%20biology&amp;volume=&amp;">Synthetic Biology subject area</a>. This will merit clarification of the criteria for counts used on the PLOS website and the API returns.</p>
<p>##Highlighting terms and text fragments with highplos()</p>
<p><code>highplos()</code> is a great function for research in PLOS, particularly when combined with opening results in a browser using <code>highbrow()</code>.</p>
<p>Highlighting will pull back a chunk of text with the search term highlighted with the emphasis <em> </em> tag enclosing the individual words in a search phrase. It is possible that an entire phrase can be highlighted (see hl.usePhraseHighlighter) but this requires further exploration.</p>
<p>In this example we will simply use the term synthetic biology and then highlight the terms in the abstract <code>hl.fl =</code> and limit this to 10 rows of results. We will also add the function <code>highbrow()</code> (for highlight browse) at the end. This will open the results in our browser. In the examples we use a pipe (%&gt;%) meaning <code>this %then% that</code>. This means that we do not have to enter the name snippet into the highbrow function and simplifies the code.</p>
<p>When reviewing the results in a browser note that we can click on the DOI to see the full article. This is a really useful tool for assessing which articles we might want to take a closer look at.</p>
<div class="sourceCode" id="cb172"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb172-1" data-line-number="1"><span class="kw">highplos</span>(<span class="dt">q =</span> <span class="st">&#39;&quot;synthetic biology&quot;&#39;</span>, <span class="dt">hl.fl =</span> <span class="st">&#39;abstract&#39;</span>, <span class="dt">fq =</span> <span class="st">&quot;doc_type:full&quot;</span>, <span class="dt">rows =</span> <span class="dv">10</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb172-2" data-line-number="2"><span class="st">  </span><span class="kw">highbrow</span>() <span class="co">#launches the browser</span></a></code></pre></div>
<p>Note that in some cases, even though we are restricting to <code>doc-type:full</code>, we retrieve entries with no data. In one case this is because we are highlighting terms in the abstract when the term appears in the full text. In a second case we have picked up a correction where one of the authors is at a synthetic biology centre but neither the abstract or text mention synthetic biology. So, bear in mind that some further exploration may be required to understand why particular results are being returned. These issues are minor and this is a great tool.</p>
<p>There are two additional options (arguments) for <code>highplos()</code> that we can use. The first of these is snippets using <code>hl.snippets =</code> and the second is <code>hl.fragsize =</code>. Both can be used in conjunction with <code>highbrow()</code>.</p>
<p>###Snippets using hl.snippets</p>
<div class="sourceCode" id="cb173"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb173-1" data-line-number="1">snippet &lt;-<span class="st"> </span><span class="kw">highplos</span>(<span class="dt">q =</span> <span class="st">&#39;&quot;synthetic biology&quot;&#39;</span>, <span class="dt">hl.fl =</span> <span class="kw">list</span>(<span class="st">&quot;title&quot;</span>, <span class="st">&quot;abstract&quot;</span>), <span class="dt">hl.snippets =</span> <span class="dv">10</span>, <span class="dt">rows =</span> <span class="dv">100</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb173-2" data-line-number="2"><span class="st">  </span><span class="kw">highbrow</span>()</a></code></pre></div>
<p>The snippets argument is handy (the default value for a snippet is 1 but goes up to as many as you like). It become very interesting when we add <code>hl.mergeContiguous = 'true'</code>. This will display the entries captured in the order of the articles to provide a sense of its uses by the author(s).</p>
<div class="sourceCode" id="cb174"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb174-1" data-line-number="1"><span class="kw">highplos</span>(<span class="dt">q=</span><span class="st">&#39;&quot;synthetic biology&quot;&#39;</span>, <span class="dt">hl.fl =</span> <span class="st">&quot;abstract&quot;</span>, <span class="dt">hl.snippets =</span> <span class="dv">10</span>, <span class="dt">hl.mergeContiguous =</span> <span class="st">&#39;true&#39;</span>, <span class="dt">rows =</span> <span class="dv">10</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb174-2" data-line-number="2"><span class="st">  </span><span class="kw">highbrow</span>()</a></code></pre></div>
<p>###fragment size using hl.fragsize</p>
<p>Greater control over what we are seeing is provided using the <code>hl.fragsize</code> option. This allows us to specify the number of characters (including spaces) that we want to see in relation to our target terms.</p>
<p>In the first example we will highlight the phrase synthetic biology in the titles and abstracts and set the fragment size (using hl.fragsize ) to a high 500. This will return the first 500 characters including spaces rather than words. We will set the number of rows to a somewhat arbitrary 200. This can easily be pushed a lot higher but expect to wait for a few moments if you move this to 1000 rows.</p>
<div class="sourceCode" id="cb175"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb175-1" data-line-number="1"><span class="kw">highplos</span>(<span class="dt">q =</span> <span class="st">&#39;&quot;synthetic biology&quot;&#39;</span>, <span class="dt">hl.fl =</span> <span class="kw">list</span>(<span class="st">&quot;title&quot;</span>, <span class="st">&quot;abstract&quot;</span>), <span class="dt">hl.fragsize =</span> <span class="dv">500</span>, <span class="dt">rows =</span> <span class="dv">200</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb175-2" data-line-number="2"><span class="st">  </span><span class="kw">highbrow</span>()</a></code></pre></div>
<p>We can also do the reverse of a larger search by reducing the fragment size to say up to 100 characters. At the moment it is unclear whether it is possible to control whether characters are selected to the right or the left of our target terms. Note that results will display up to 100 characters where they are available (short results will be for sentences such as titles that are less than 100 characters)</p>
<div class="sourceCode" id="cb176"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb176-1" data-line-number="1"><span class="kw">highplos</span>(<span class="dt">q =</span> <span class="st">&#39;&quot;synthetic biology&quot;&#39;</span>, <span class="dt">hl.fl =</span> <span class="kw">list</span>(<span class="st">&quot;title&quot;</span>, <span class="st">&quot;abstract&quot;</span>), <span class="dt">hl.fragsize =</span> <span class="dv">100</span>, <span class="dt">rows =</span> <span class="dv">200</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb176-2" data-line-number="2"><span class="st">  </span><span class="kw">highbrow</span>()</a></code></pre></div>
<p>What is great about this is that we can easily control the amount of text that we are seeing and then select articles of interest to read straight from the browser. We can also start to think about ways to use this information for text mining to identify terms used in conjunction with synthetic biology or types of synthetic biology.</p>
<p>##Get the full text of one or more articles</p>
<p>We will finish this article by briefly demonstrating how to retrieve and save the full text of one or more articles. <code>rplos</code> uses a combination of the <code>XML</code> and the <code>tm</code> (for text mining) package.</p>
<p>Retrieving full text should initially be used rather sparingly because you could pull back a lot of data in XML format that you may then struggle to process. So, it is probably best to start small.</p>
<p>Using the unique_results data that we created above we have a list of DOIs in the id field. We can create a vector of these using the following:</p>
<div class="sourceCode" id="cb177"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb177-1" data-line-number="1">doi &lt;-<span class="st"> </span>unique_results<span class="op">$</span>id</a></code></pre></div>
<p>That has created a vector of 1097 dois. To limit those results, let’s create a shorter version where we select five rows.</p>
<div class="sourceCode" id="cb178"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb178-1" data-line-number="1">short_doi &lt;-<span class="st"> </span>doi[<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>]</a></code></pre></div>
<p>Now we can use <code>plos_fulltext()</code> to retrieve the full text.</p>
<div class="sourceCode" id="cb179"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb179-1" data-line-number="1">ft &lt;-<span class="st"> </span><span class="kw">plos_fulltext</span>(short_doi)</a></code></pre></div>
<p>When we pull back the two articles an object is created of class <code>plosft</code>. To see the full text of one of the individual articles we use the trusty <code>$</code> and then select a doi.</p>
<div class="sourceCode" id="cb180"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb180-1" data-line-number="1">ft<span class="op">$</span><span class="st">`</span><span class="dt">10.1371/journal.pone.0140969</span><span class="st">`</span></a></code></pre></div>
<p>This displays a lot of the XML tags inside the text. We would now like to extract the text without the XML tags. The <code>rplos</code> documentation for <code>plos_fulltext()</code> helps us to do this using the following code. The first part of the code uses the XML package to parse the results removing the xml tags in the process.</p>
<div class="sourceCode" id="cb181"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb181-1" data-line-number="1"><span class="kw">library</span>(tm)</a>
<a class="sourceLine" id="cb181-2" data-line-number="2"><span class="kw">library</span>(XML)</a>
<a class="sourceLine" id="cb181-3" data-line-number="3">ft_parsed &lt;-<span class="st"> </span><span class="kw">lapply</span>(ft, <span class="cf">function</span>(x) {</a>
<a class="sourceLine" id="cb181-4" data-line-number="4">    <span class="kw">xpathApply</span>(<span class="kw">xmlParse</span>(x), <span class="st">&quot;//body&quot;</span>, xmlValue)</a>
<a class="sourceLine" id="cb181-5" data-line-number="5">})</a></code></pre></div>
<p>If we type <code>ft_parsed</code> we will now see the text (the body without title and abstract) fly by without all of the tags.</p>
<div class="sourceCode" id="cb182"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb182-1" data-line-number="1">ft_parsed</a></code></pre></div>
<p>The object returned by this is a list (use <code>class(ft_parsed)</code>). Next, we can transform this into a corpus (a text or collection of texts) that we can save to disk using the following code from the <code>rplos</code> <code>plos_fulltext()</code> example.</p>
<div class="sourceCode" id="cb183"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb183-1" data-line-number="1">tmcorpus &lt;-<span class="st"> </span><span class="kw">Corpus</span>(<span class="kw">VectorSource</span>(ft_parsed))</a></code></pre></div>
<p>If we type tmcorpus$ into the console then we will see 1 to 5 pop up, but this will return NULL if selected. The data is there but we need to use <code>str(tmcorpus</code>) to see the structure of the corpus. If we want to view a text within the corpus we can use <code>writeLines()</code></p>
<div class="sourceCode" id="cb184"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb184-1" data-line-number="1"><span class="kw">writeLines</span>(<span class="kw">as.character</span>(tmcorpus[[<span class="dv">2</span>]]))</a></code></pre></div>
<p>We can also view the five texts in our corpus (be prepared for a lot of scrolling) by using lapply to read over the two texts as character.</p>
<div class="sourceCode" id="cb185"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb185-1" data-line-number="1"><span class="kw">lapply</span>(tmcorpus[<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>], as.character)</a></code></pre></div>
<p>For more information see the <a href="https://cran.r-project.org/web/packages/tm/vignettes/tm.pdf">Ingo Feinerer (2015) Introduction to the tm package</a> (also available in the tm documentation) from which the above is drawn.</p>
<p>##Writing a corpus to disk</p>
<p>To write a corpus we first need to create a folder where the files will be housed (otherwise they will simply be written into your project folder with everything else).</p>
<p>The easiest way to create a new folder is to head over to the Files Tab in RStudio (normally in the bottom right pane) and choose <code>New Folder</code>. We will call it <code>tm</code>.</p>
<p>Now use <code>getwd()</code> and copy the file path into the following function, from the writeCorpus examples, adding <code>/tm</code> at the end. It will look something like this but replace the path with your own, not forgetting the <code>/tm</code>. Then press Enter.</p>
<div class="sourceCode" id="cb186"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb186-1" data-line-number="1"><span class="kw">writeCorpus</span>(tmcorpus, <span class="dt">path =</span> <span class="st">&quot;/Users/paul/Desktop/open_source_master/rplos/tm&quot;</span>)</a></code></pre></div>
<p>When you look in the tm folder inside rplos (use the Files tab in RStudio) you will now see five texts with the names 1 to 5. For more details, such as naming files and specifying file types, see <code>?writeCorpus</code> and the tm package documentation.</p>
<p>##Round Up</p>
<p>In this chapter we have focused on using the <code>rplos</code> package to access scientific articles from the Public Library of Science (PLOS). As we have seen, with short pieces of code it is easy to search and retrieve data from PLOS on a whole range of subjects whether it be pizza or synthetic biology.</p>
<p>One of the most powerful features of R is that it is quite easy to access free online data using APIs. <code>rplos</code> is a very good starting point for learning how to retrieve data using an API because it is well written and the data that comes back is remarkably clean.</p>
<p>Perhaps the biggest challenge facing new users of R is what to do with data once you have retrieved it. This can result in many hours of frustration staring at a list or object with the data you need without the tools to access it and transform it into the format you need. In this article we have focused on using the <code>plyr</code>, <code>dplyr</code>, <code>tidyr</code> and <code>stringr</code> suite of packages to turn <code>rplos</code> data into something you can use. These packages are rightly very popular for everyday work in R and becoming more familiar with them will reap rewards in learning R for practical work. At the close of the article we used the <code>tm</code> (text mining) package to save the full text of articles. This is only a very small part of this package and <code>rplos</code> provides some useful examples to begin text mining using <code>tm</code> (see the <code>plos_fulltext()</code> examples). R now has a rich range of text mining packages and we will address this in a future article.</p>
<p>In the meantime, if you would like to learn more about R try the resources below. If you would like to learn R inside R then try the very useful Swirl package (details below).</p>
<p>##Resources</p>
<ol style="list-style-type: decimal">
<li><a href="https://ropensci.org/">rOpenSci</a></li>
<li><a href="http://www.cookbook-r.com/">Winston Chang’s R Cookbook</a></li>
<li><a href="https://www.rstudio.com/resources/training/online-learning/">RStudio Online Learning</a></li>
<li><a href="http://www.r-bloggers.com/">r-bloggers.com</a></li>
<li><a href="https://www.datacamp.com/">Datacamp</a></li>
<li>Swirl (developed by the <a href="https://www.coursera.org/course/rprog">free Coursea R Programming course</a> team at John Hopkins University. If you would like to get started with Swirl run the code chunk below to install the package and load the library.</li>
</ol>
<div class="sourceCode" id="cb187"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb187-1" data-line-number="1"><span class="kw">install.packages</span>(<span class="st">&quot;swirl&quot;</span>)</a>
<a class="sourceLine" id="cb187-2" data-line-number="2"><span class="kw">library</span>(swirl)</a></code></pre></div>

</div>






<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p><a href="http://stackoverflow.com/questions/25975827/how-to-feed-the-result-of-a-pipe-chain-magrittr-to-an-object" class="uri">http://stackoverflow.com/questions/25975827/how-to-feed-the-result-of-a-pipe-chain-magrittr-to-an-object</a><a href="databases.html#fnref1" class="footnote-back">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="datasets.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
